---
title: "Classification"
subtitle: "Logistic Regression"
format:
  revealjs:
    scrollable: true
    navigation-mode: vertical
    controls-layout: bottom-right
    controls-tutorial: true
    incremental: false 
    chalkboard:
      src: chalkboard.json
      storage: chalkboard_pres
      theme: whiteboard
      chalk-width: 4
knitr:
  opts_chunk: 
    echo: false
    eval: true
    message: false
    warnings: false
    comment: "#>" 
    
revealjs-plugins:
  - pointer
  - verticator
  
filters: 
  - reveal-header
  - code-fullscreen

editor: source
---

## Learning Outcomes

-   Logistic Regression

-   Multinomial Regression

## Classification

The practice of classifying data points into different categories.

## Motivation

```{r}
#| echo: true
library(palmerpenguins)
library(tidyverse)
penguins |> ggplot(aes(body_mass_g, flipper_length_mm, color = species)) +
  geom_point()
```

## For Now ...

```{r}
#| echo: true
penguins <- penguins |> drop_na() |>  
  mutate(gentoo = ifelse(species == "Gentoo", "Gentoo", "Other"))
penguins |> ggplot(aes(body_mass_g, flipper_length_mm, color = gentoo)) +
  geom_point()
```

## Potential Model

$$
\left(\begin{array}{c}
Gentoo \\
Other
\end{array}\right) = \boldsymbol X^\mathrm T \boldsymbol \beta
$$

# Logistic Regression

## Logistic Regression

Logistic Regression is used to model the association between a set of predictors and a binary outcome.

## Construct Model

$$
\left(\begin{array}{c}
Gentoo \\
Other
\end{array}\right) = \boldsymbol X^\mathrm T \boldsymbol \beta
$$

## Let ...

$$
Y = \left\{\begin{array}{cc}
1 & Gentoo \\
0 & Other
\end{array}\right.
$$

## Construct a Model

$$
P\left(Y = 1\right) = \boldsymbol X^\mathrm T \boldsymbol \beta
$$

## Construct a Model

$$
P\left(Y = 1\right) = \frac{\exp(\boldsymbol X^\mathrm T \boldsymbol \beta)}{1 + \exp(\boldsymbol X^\mathrm T \boldsymbol \beta)}
$$

## Construct a Model

$$
\frac{P(Y = 1)}{1-P(Y = 1)} = \exp(\boldsymbol X^\mathrm T \boldsymbol \beta)
$$

## The Logistic Model

$$
\log\left\{\frac{P(Y = 1)}{1-P(Y = 1)}\right\} = \boldsymbol X^\mathrm T \boldsymbol \beta
$$

## Odds and Log-Odds

$$
\frac{P(Y = 1)}{1-P(Y = 1)}
$$ 

$$
\log\left\{\frac{P(Y = 1)}{1-P(Y = 1)}\right\}
$$

## Estimation

Estimation is done by finding the values of $\boldsymbol \beta$ that maximizes the likelihood function given the data pair $(\boldsymbol X_i, Y_i)$

$$
L(\boldsymbol \beta) = \prod_{i=1}^n P(Y_i=1)^{Y_i}\left\{1-P(Y_i=1)\right\}^{1-Y_i}
$$

## Predicting Category

Once the estimates $\hat \beta$ are obtained, compute:

$$ 
P\left(Y_i = 1\right) = \frac{\exp(\boldsymbol X_i^\mathrm T \boldsymbol{\hat\beta})}{1 + \exp(\boldsymbol X^\mathrm T \boldsymbol{\hat\beta)}}
$$

$$
Y_i = \left\{\begin{array}{cc}
1 & P(Y_i =1) \geq 0.5 \\
0 & Otherwise
\end{array}\right.
$$

# Ordinal Regression

## Ordinal Regression

Ordinal regression extends the logistic regression to more than one category ($J$ Categories) that has a natural order.

An example can be thought of as Grade Levels: A, B, C, D, F

## Modeling Ordinal Responses

We can model Ordinal responses using the the proportional odds model and a logit formula:

$$
\mathrm{logit}\{P(Y\leq j|X)\} = \boldsymbol X_{(j)} ^\mathrm T \boldsymbol \beta _{(j)}
$$

## Linear Model

$$
\boldsymbol X_{(j)}^\mathrm T \boldsymbol \beta_{(j)} = \beta_{0(j)} + \sum^p_{i=1}X_{i}\beta_i
$$

# Multinomial Regression

## Multinomial Regression

## Model

$$
\log\left\{\frac{P(Y = k)}{P(Y = \mathrm{REF})}\right\} = \boldsymbol X^\mathrm T \boldsymbol \beta_k
$$

$\mathrm{REF}$ is a reference value to be modeled.

# R Examples

## R Preparation

```{r}
#| echo: true
## Data
library(GLMsData)
library(carData)
library(palmerpenguins)

## Ordinal and Multinomial Models
library(VGAM)
```

## Logistic Regression

```{r}
#| echo: true
#| eval: false

glm(y ~ x,
    data,
    family = binomial())

```

## Logistic Regression

```{r}
#| echo: true
penguins <- penguins |> mutate(gentoo = ifelse(gentoo == "Gentoo", 1, 0))
res <- penguins |> glm(gentoo ~ flipper_length_mm + body_mass_g, 
                       data = _,
                       family = binomial())
summary(res)
```

## Prediction Logistic Regression

```{r}
#| echo: true
predict(res, 
        newdata = data.frame(flipper_length_mm = 200, body_mass_g = 4005),
        type = "response")
predict(res, 
        newdata = data.frame(flipper_length_mm = 220, body_mass_g = 4775),
        type = "response")
```


## Ordinal Regression

```{r}
#| echo: true
#| eval: false

vglm(y ~ x,
    data,
    family = propodds())

```

## Ordinal Regression

```{r}
#| echo: true
res <- vglm(poverty~religion+degree+country+age+gender,
            family = propodds(),
            data = WVS)
```

## Summary

```{r}
#| echo: true
summary(res)
```

## Multinomial Regression

```{r}
#| echo: true
#| eval: false

vglm(y ~ x,
    data,
    family = multinomial())

```

## Multinomial Regression

```{r}
#| echo: true
res <- penguins |> 
  vglm(species ~ body_mass_g + flipper_length_mm,
       family = multinomial(),
       data = _)
```

## Summary

```{r}
#| echo: true
summary(res)
```
