---
title: "Model Building"
format:
  revealjs:
    scrollable: true
    navigation-mode: vertical
    controls-layout: bottom-right
    controls-tutorial: true
    incremental: false 
    chalkboard:
      src: chalkboard.json
      storage: chalkboard_pres
      theme: whiteboard
      chalk-width: 4
knitr:
  opts_chunk: 
    echo: false
    eval: false
    message: false
    warnings: false
    comment: "#>" 

editor: visual
---

```{r}
#| include: false
#| eval: true
#| label: setup

library(DT)

library(tidyverse)

library(GLMsData)

library(broom)

library(palmerpenguins)

library(magrittr)

library(leaps)

library(glmnet)
```

## Learning Objective

-   Fixing Assumptions

-   Model Building

# Fixing Assumptions

## Fixing Assumptions

-   Linearity

-   Collinearity

-   Unequal Variances

-   Influential observations and Outliers

## Residual Function

```{r}
#| eval: true
#| echo: true

df_resid <- function(x){
  res <- data.frame(obs = 1:nrow(x$model),
                    x$model, 
                    resid = resid(x),
                    fitted = fitted(x),
                    sresid = rstandard(x),
                    hatvals = hatvalues(x),
                    jackknife =  rstudent(x),
                    cooks = cooks.distance(x)
                    )
  return(res)
}
```

## Linearity $\log(x)$

```{r}
#| eval: true
#| echo: true

x <- rnorm(50, 6, 2)
y <- 3 - 2 * log(x) + rnorm(50, sd = 0.25)
x_lm <- lm(y ~ x)
resid_df <- df_resid(x_lm)
ggplot(resid_df, aes(x, resid)) + geom_point() +
  stat_smooth(se = F) +
  geom_hline(yintercept = 0)+
  theme_bw()
```

```{r}
#| eval: true
#| echo: true

x_lm <- lm(y ~ log(x))
resid_df <- df_resid(x_lm)
ggplot(resid_df, aes(x, resid)) + geom_point() +
  stat_smooth(se = F) +
  geom_hline(yintercept = 0)+
  theme_bw()
```

## Linearity $1/x$

```{r}
#| eval: true
#| echo: true

x <- rnorm(50, 6, 1.5)
y <- 3 - 2 / x + rnorm(50, sd = 0.2)
x_lm <- lm(y ~ x)
resid_df <- df_resid(x_lm)
ggplot(resid_df, aes(x, resid)) + geom_point() +
  stat_smooth(se = F) +
  geom_hline(yintercept = 0)+
  theme_bw()
```

```{r}
#| echo: true
#| eval: true
#| 
x_lm <- lm(y ~ I(1/x))
resid_df <- df_resid(x_lm)
ggplot(resid_df, aes(x, resid)) + geom_point() +
  stat_smooth(se = F) +
  geom_hline(yintercept = 0)+
  theme_bw()
```

## Other Common Covariate Function Transformations

-   $\log_{10}(x)$

-   $x^2$

-   $\sqrt x$

-   $x^3$

## Unequal Variance

There are a couple of methods to adjust for unequal variances

-   Generalized Least Squares

-   Mixed-Effects Models

## Collinearity

There are couple of methods to fix collinearity:

-   Principal Components Analysis

-   Ridge Regression

-   Lasso Regression

# Model Building

## Model Building

When given a set of predictors, we want to build a model that only contains predictors that best fits the data, without overfitting.

Ideally, we always want to choose a parsimonious model that best describes the outcome variable. The more predictors into the model, the less parsimonious and less powerful.

Choosing the best model can be done based on selection criteria such as Mallow's $C_p$, AIC, AICc, BIC, and adjusted $R^2$.

## Model Building

-   Best Subset Model

    -   Fit all models and select the best model based criteria

-   Forward Stepwise Model Building

    -   Begin with the null model ($Y\sim 1$) and add variables until a final model is chosen.

-   Backward Stepwise Model Building

    -   Begin with the full model, and remove variable until the final model is chosen.

-   Hybrid Stepwise Regression

    -   A hybrid approach between the forward and backward building approach.

## Best Subset Model Building

1.  Begin with the null model, no predictors
2.  For $k=1,\ldots, p$ (number of predictors):
    1.  Fit all $\left(^p_k\right)$ models that contain $k$ predictors

    2.  Define $M_k$ as the model with the largest $R²$
3.  The final model is the model $M_k$ based on selection criteria

## Forward Stepwise Model Building

1.  Begin with the null model, no predictors
2.  For $k=0,\ldots, p-1$ (number of predictors):
    1.  Fit all $p-k$ models that adds one new predictor to the orginal model containing $k$ predictors

    2.  Define $M_{k+1}$ as the model with the largest $R²$ among $p-k$ models
3.  The final model is the model $M_(k+1)$ based on selection criteria

## Backward Stepwise Model Building

1.  Begin with the full model $M_p$, with all predictors
2.  For $k=p,p-1, \ldots, 1$ (number of predictors):
    1.  Fit all models that contain $k-1$ predictors

    2.  Define $M_{k-1}$ as the model with the largest $R²$
3.  The final model is the model $M_k$ based on selection criteria

# Penalty Term Techniques

## Ridge Regression

Minimizes the following function

$$
\sum^n_{i=1}(y_i-\boldsymbol X_i^\mathrm T \boldsymbol \beta)^2 +\lambda \boldsymbol \beta ^\mathrm T\boldsymbol \beta
$$

-   $\boldsymbol X_i$: Design matrix for $i$th observation

-   $\boldsymbol \beta$: regression coefficients

-   $\lambda > 0$: tuning parameter

## Lasso Regression

Minimize the following function:

$$
\sum^n_{i=1}(y_i-\boldsymbol X_i^\mathrm T \boldsymbol \beta)^2 +\lambda \sum^p_{j=0}|\beta_j|
$$

## Selecting the Ideal penalty term

Selecting the correct penalty term is essential to ensure an ideal bias-variance trade-off. The best approach is to use a cross-validation approach, more specifically, the LOOCV (leave-one-out cross-validation).

## Cross-Validation

-   Choose a set of tuning parameters to test.

-   For each $k$th turning parameter Calculate the tuning parameter error for each value

    -   Utilize the leave-one-out approach

        -   For each observation fit and compute:

            $$
            MSE_i = (y_i - \hat y_{i(i)})^2
            $$

        -   Compute the following error:

            $$
            CVE_k = \frac{1}{n}\sum^n_{i=1}MSE_i 
            $$

-   Identify the $k$th tuning parameter with the lowest $CVE_k$

# Selection Criteria

## Mallow's $C_p$

$$
C_p = \frac{1}{n}(RSS + 2 d \hat \sigma^2) 
$$

-   $RSS$: Residual Sum of Squares

-   $\hat \sigma^2$: Mean Square Error

-   $d$: number of predictors

-   Lower is better

## Aikaike Information Criteria (AIC)

$$
\frac{1}{n\hat\sigma^2}(RSS+2d\hat\sigma^2)
$$

-   Lower is Better

## Bayesian Information Criteria (BIC)

$$
\frac{1}{n\hat\sigma^2}\{RSS+\log(n)d\hat\sigma^2\}
$$

-   Lower is better

## $R^2$

$$
1-\frac{RSS}{TSS}
$$

-   $RSS=\sum^n_{i=1}(y_i-\hat y_i)^2$

-   $TSS=\sum^n_{i=1}(y_i-\bar y)^2$

-   Higher is Better

## Adjusted $R^2$

$$
1-\frac{RSS/(n-d-1)}{
TSS/(n-1)}
$$

-   Higher is Better

# Model Building R Code

## R Code

```{r}
#| echo: true
#| eval: false

library(leaps)
regsubsets(y ~ ., data)

```

## Full Subset

::: panel-tabset
## Code

```{r}
#| echo: true
#| eval: true
lm_full <- regsubsets(mpg ~ ., data = mtcars)

```

## Summary

```{r}
#| echo: true
#| eval: true
sum_lm_full <- summary(lm_full)
print(sum_lm_full)
```

## Plot

```{r}
#| echo: true
#| eval: true
plot(sum_lm_full$cp)
```

## Model

```{r}
#| echo: true
#| eval: true
coef(lm_full,3)
```
:::

## Forward Model Building

::: panel-tabset
## Code

```{r}
#| echo: true
#| eval: true
lm_full <- regsubsets(mpg ~ ., data = mtcars,
                      method =  "forward")

```

## Summary

```{r}
#| echo: true
#| eval: true
sum_lm_full <- summary(lm_full)
print(sum_lm_full)
```

## Plot

```{r}
#| echo: true
#| eval: true
plot(sum_lm_full$cp)
```

## Model

```{r}
#| echo: true
#| eval: true
coef(lm_full,3)
```
:::

## Backward Model Building

::: panel-tabset
## Code

```{r}
#| echo: true
#| eval: true
lm_full <- regsubsets(mpg ~ ., data = mtcars,
                      method =  "backward")

```

## Summary

```{r}
#| echo: true
#| eval: true
sum_lm_full <- summary(lm_full)
print(sum_lm_full)
```

## Plot

```{r}
#| echo: true
#| eval: true
plot(sum_lm_full$cp)
```

## Model

```{r}
#| echo: true
#| eval: true
coef(lm_full,3)
```
:::

## Ridge and Lasso Regression

```{r}
#| echo: true

library(glmnet)
# Ridge Regression
glmnet(x, y, 
       alpha = 0, 
       lambda = grid)
# Lasso Regression
glmnet(x, y, 
       alpha = 1, 
       lambda = grid)
```

## Ridge Regression

::: panel-tabset
## Prep

```{r}
#| echo: true
#| eval: true
x <- model.matrix(mpg ~ ., data = mtcars)[,-1]
y <- mtcars$mpg

grid_lambda <- seq(0, 100, by = 0.1)

```

## CV

```{r}
#| echo: true
#| eval: true
ridge_reg_cv <- cv.glmnet(x, y,
                    alpha = 0,
                    lambda = grid_lambda)
ridge_reg_cv

```

## Model

```{r}
#| echo: true
#| eval: true
ridge_reg <- glmnet(x, y,
                    alpha = 0,
                    lambda = grid_lambda)

coef(ridge_reg)[,974]
```
:::

## Lasso Regression

::: panel-tabset
## Prep

```{r}
#| echo: true
#| eval: true
x <- model.matrix(mpg ~ ., data = mtcars)[,-1]
y <- mtcars$mpg

grid_lambda <- seq(0, 100, by = 0.1)

```

## CV

```{r}
#| echo: true
#| eval: true
lasso_reg_cv <- cv.glmnet(x, y,
                    alpha = 1,
                    lambda = grid_lambda)
lasso_reg_cv

```

## Model

```{r}
#| echo: true
#| eval: true

lasso_reg <- glmnet(x, y,
                    alpha = 1,
                    lambda = grid_lambda)

coef(lasso_reg)[,993]
```
:::
