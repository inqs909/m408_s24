---
title: "Deep Learning"
format:
  revealjs:
    scrollable: true
    navigation-mode: vertical
    controls-layout: bottom-right
    controls-tutorial: true
    incremental: false 
    chalkboard:
      src: chalkboard.json
      storage: chalkboard_pres
      theme: whiteboard
      chalk-width: 4
knitr:
  opts_chunk: 
    echo: true
    eval: true
    message: false
    warnings: false
    comment: "#>" 
    
revealjs-plugins:
  - pointer
  - verticator
  
filters: 
  - reveal-header
  - code-fullscreen

editor: source
---


# Neural Networks

## Neural Networks

Neural networks are a type of machine learning algorithm that are designed to mimic the function of the human brain. They consist of interconnected nodes or "neurons" that process information and generate outputs based on the inputs they receive.

## Uses

Neural networks are typically used for tasks such as image recognition, natural language processing, and prediction. They are capable of learning from data and improving their performance over time, which makes them well-suited for complex and dynamic problems.

## Single Layer Neural Networks

A single layer neural networks can be formulated as linear function:

$$
f(X) = \beta_0 + \sum^K_{k=1}\beta_kh_k(X)
$$

Where $X$ is a vector of inputs of length $p$ and $K$ is the number of activations, $\beta_j$ are the regression coefficients and

$$
h_k(X) = A_k = g(w_{k0} + \sum^p_{l1}w_{kl}X_{l})
$$

with $g(\cdot)$ being a nonlinear function and $w_{kl}$ are the weights.

## Nonlinear Function $g(\cdot)$

-   $g(z) = \frac{e^z}{1+e^z}$

-   $g(z) = (z)_+ = zI(z\geq0)$

## Single Layer Neural Network

![](https://www.oreilly.com/api/v2/epubs/9781789808452/files/assets/290136cc-48f2-47b1-bb95-ffdb625b987d.png){fig-align="center"}

## Multilayer Neural Network

Multilayer Neural Networks create multiple hidden layers where each layer feeds into each other which will create a final outcome.

## Multilayer Neural Network

![](https://www.oreilly.com/api/v2/epubs/9781838642709/files/assets/61bc8450-f3ac-4d81-b405-3d748e30d04a.png){fig-align="center"}


# TensorFlow

## TensorFlow

Tensorflow is an open-source machine learning platform developed by Google. Tensorflow is capable of completing the following tasks:

-   Image Classification

-   Text Classification

-   Regression

-   Time-Series

## Keras

Keras is the API that will talk to Tensorflow via different platforms.

## More Information

[TensorFlow for R](https://tensorflow.rstudio.com/)

# Torch

## Torch

Torch is a scientific computing framework designed to support machine learning in CPU/GPU computing. Torch is capable of computing:

-   Matrix Operations

-   Linear Algebra

-   Neural Networks

-   Numerical Optimization

-   and so much more!

## Torch

Torch can be accessed in both:

-   Pytorch

-   R Torch

## R Torch

R Torch is capable of handling:

-   Image Recognition

-   Tabular Data

-   Time Series Forecasting

-   Audio Processing

## More Information

[![](https://torch.mlverse.org/images/cover.jpg){fig-align="center"}](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/)

# R Code

## MNIST

This is a database of handwritten digits.

We will use to construct neural networks that will classify images.

## MNIST

## Installation of Torch

```{r}
#| echo: true
#| eval: false
install.packages("torch")
install.packages("luz")
install.packages("torchvision")
install.packages("torchdatasets")
install.packages("zeallot")

```

## Torch in R

```{r}
#| echo: true
#| eval: false
# 
library(torch)
library(luz) # high-level interface for torch
library(torchvision) # for datasets and image transformation
library(torchdatasets) # for datasets we are going to use
library(zeallot)
torch_manual_seed(13)

###
train_ds <- mnist_dataset(root = ".", train = TRUE, download = TRUE)
test_ds <- mnist_dataset(root = ".", train = FALSE, download = TRUE)

train_ds[1]
str(test_ds[2])

length(train_ds)
length(test_ds)
###

# 
# ###
# transform <- function(x) {
#   x |> 
#     torch_tensor()  |> 
#     torch_flatten() |> 
#     torch_div(255)
# }
# train_ds <- mnist_dataset(
#   root = ".",
#   train = TRUE,
#   download = TRUE,
#   transform = transform
# )
# test_ds <- mnist_dataset(
#   root = ".",
#   train = FALSE,
#   download = TRUE,
#   transform = transform
# )
# ###
# 
# 
# ###
# modelnn <- nn_module(
#   initialize = function() {
#     self$linear1 <- nn_linear(in_features = 28*28, out_features = 256)
#     self$linear2 <- nn_linear(in_features = 256, out_features = 128)
#     self$linear3 <- nn_linear(in_features = 128, out_features = 10)
# 
#     self$drop1 <- nn_dropout(p = 0.4)
#     self$drop2 <- nn_dropout(p = 0.3)
# 
#     self$activation <- nn_relu()
#   },
#   forward = function(x) {
#     x |> 
#       self$linear1() |> 
#       self$activation() |> 
#       self$drop1() |> 
# 
#       self$linear2() |> 
#       self$activation() |> 
#       self$drop2() |> 
#       self$linear3()
#   }
# )
# ###
# 
# 
# ###
# print(modelnn())
# ###
# 
# 
# ###
# modelnn <- modelnn |> 
#   setup(
#     loss = nn_cross_entropy_loss(),
#     optimizer = optim_rmsprop,
#     metrics = list(luz_metric_accuracy())
#   )
# ###
# 
# ###
# system.time(
#    fitted <- modelnn |> 
#       fit(
#         data = train_ds,
#         epochs = 5,
#         valid_data = 0.2,
#         dataloader_options = list(batch_size = 256),
#         verbose = FALSE
#       )
#  )
# plot(fitted)
# ###
# 
# ###
# accuracy <- function(pred, truth) {
#    mean(pred == truth) }
# 
# # gets the true classes from all observations in test_ds.
# truth <- sapply(seq_along(test_ds), function(x) test_ds[x][[2]])
# 
# fitted |> 
#   predict(test_ds) |> 
#   torch_argmax(dim = 2) |> # the predicted class is the one with higher 'logit'.
#   as_array() |>  # we convert to an R object
#   accuracy(truth)
# ###
# 
# ###
# modellr <- nn_module(
#   initialize = function() {
#     self$linear <- nn_linear(784, 10)
#   },
#   forward = function(x) {
#     self$linear(x)
#   }
# )
# print(modellr())
# ###
# 
# ###
# fit_modellr <- modellr |> 
#   setup(
#     loss = nn_cross_entropy_loss(),
#     optimizer = optim_rmsprop,
#     metrics = list(luz_metric_accuracy())
#   ) |> 
#   fit(
#     data = train_ds,
#     epochs = 5,
#     valid_data = 0.2,
#     dataloader_options = list(batch_size = 128)
#   )
# 
# fit_modellr |> 
#   predict(test_ds) |> 
#   torch_argmax(dim = 2) |>   # the predicted class is the one with higher 'logit'.
#   as_array() |>  # we convert to an R object
#   accuracy(truth)
# 
# 
# # alternatively one can use the `evaluate` function to get the results
# # on the test_ds
# evaluate(fit_modellr, test_ds)
# ###
```