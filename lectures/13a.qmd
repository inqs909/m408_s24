---
title: "Tree-Based Methods"
format:
  revealjs:
    scrollable: true
    navigation-mode: vertical
    controls-layout: bottom-right
    controls-tutorial: true
    incremental: false 
    chalkboard:
      src: chalkboard.json
      storage: chalkboard_pres
      theme: whiteboard
      chalk-width: 4
knitr:
  opts_chunk: 
    
    echo: true
    code-fold: true
    eval: true
    message: false
    warnings: false
    comment: "#>" 
revealjs-plugins:
  - pointer
  - verticator
filters: 
  - reveal-header
  - code-fullscreen
  - reveal-auto-agenda
editor_options: 
  chunk_output_type: console
editor: source
---

# Trees

## Trees

A Statistical tree will partition a region from a set of predictor variables that will predict an outcome of interest.

::: fragment
Trees will split a region based on a predictors ability to reduce the overall mean squared error.
:::

::: fragment
Trees are sometimes preferred to linear models due to the visual explanation of the model.
:::

## Trees

![](https://www.mathworks.com/help/stats/simpleregressiontree.png){fig-align="center"}

## Fitting a Tree

1.  Start with the entire dataset and define the maximum number of regions or number of observations per region of the tree.
2.  Calculate the MSE of the dataset.
3.  For each potential split, calculate the MSE. Choose the split that results in the lowest overall MSE.
4.  Create a node in the tree with the selected split as the split criterion.
5.  Repeat steps 2-4 for each subset, stopping if the maximum number of regions has been reached or if the subset size is too small.

# Pruning

## Pruning

Pruning is the process that will remove branches from a regression tree in order to prevent overfitting.

::: fragment
This will result in a subtree that has high predictive power with no overfitting.
:::

::: fragment
Due to the computational burden of pruning, it is recommended to implement *Cost Complexity Pruning.*
:::

## Cost Complexity Pruning

Let $\alpha$ be nonnegative tuning parameter that indexes a sequence of trees. Identify the tree that reduces:

$$
\sum^{|T|}_{m=1}\sum_{i:\ x_i \in R_m}(y_i-\hat y_{R_m})^2 +\alpha|T|
$$

-   $|T|$: Number of terminal nodes

-   $R_m$: rectangular region containing data

-   $y_i$: observed value

-   $\hat y_{R_m}$: predicted value in rectangular region.

## Pruning Algorithm

1.  Conduct a fitting algorithm to find the largest tree from the training data. Stop once every region has a small number of observations.
2.  Apply the cost complexity pruning algorithm to identify the best subset of trees.
3.  Use a K-fold cross-validation approach to choose the proper $\alpha$. For each kth fold:
    1.  Repeat steps 1 and 2.
    2.  Evaluate the mean squared prediction error as a function of $\alpha$.
4.  Average the results for each value of $\alpha$. Pick the $\alpha$ that minimizes the error.
5.  Return the subtree with the selected $\alpha$ from step 2

# Classification Trees

## Classification Trees

Classification Trees will construct a tree that will classify data based on the region (leaf) you land. The class majority is what is classified.

## Criterion: Gini Index

The Gini Index is used to determine the error rate in classification trees:

$$
G = \sum^K_{k=1} \hat p_{mk}(1-\hat p_{mk})
$$

# Regression Trees

## Regression Trees

Regression trees will construct a tree and predict the value of the outcome based on the average value of the region (leaf).

Trees are constructed by minimizing the residual sums of square.

# Bagging

## Bagging

When splitting the data to train and test data sets, the construction of the tree suffers from high variance.

::: fragment
This is due to splitting the data in a random way. One training data set will lead to different results from another training data set.
:::

::: fragment
To improve performance, we implement a *Bootstrap Aggregation (Bagging)* technique.
:::

::: fragment
Bagging will produce a forest of trees to classify a new observation.
:::

## Bagging Algorithm

Given a single training data set:

1.  Sample from the data with replacement.

2.  Build a tree from the sampled data:

    $$
    \hat f^{*b}(x)
    $$

3.  Repeat the process B times (B=100)

4.  Compute the final average for all predictions:

    $$
    \hat f_{bag}(x)=\frac{1}{B}\sum^B_{b=1}\hat f^{*b}(x)
    $$

## Classification

To classify an observation, you can record the classification of each $b$ tree. Then classify an observation by majority rule.

## Variable Importance

With the implementation of Bagging, you lose interpretability from the original tree due to the forest.

However, we can compute which variables reduced the RSS or Gini Index for all the trees. The variables with the largest reduction are considered important.

# Random Forests

## Random Forests

Random Forests is an extension of Bagging, where a forest is generated from a bootstrap-based approach. However, when making a split, a **random** set of predictors (m\<p) are chosen for the split, instead of the full set p.

::: fragment
This will ensure that trees are unique, uncorrelated.
:::

::: fragment
It ensures that no one predictor will have all the power and lower the variance.
:::

# Boosting

## Boosting

Boosting is a mechanism where a final tree is built slowly from smaller trees using the residuals.

::: fragment
This ensures a tree is built from a slow process and prevents overfitting.
:::

::: fragment
This is done to improve prediction capabilities.
:::

## Algorithm

1.  Set $\hat f(x) = 0$ and $r_i = y_i$ for all $i$ in the training set

2.  For $b=1, 2, \ldots, B$ repeat:

    1.  Fit tree $\hat f^b$ with $d$ splits ($d+1$ terminal nodes) to the training data $(X,r)$

    2.  Update $\hat f$

        $$
        \hat f(x) \leftarrow \hat f(x) + \lambda\hat f^b(x)
        $$

    3.  Update residuals

        $$
        r_i \leftarrow r_i - \lambda\hat f^{b}(x_i)
        $$

3.  Output boosted model:

    $$
    \hat f(x) = \sum^B_{b=1} \lambda \hat f^b(x)
    $$


# R Code

## Regression Trees

::: panel-tabset
### R Code

```{r}
#| code-fold: false
library(tree)
library(palmerpenguins)
library(tidyverse)

df <- penguins |>  drop_na()
train <- sample(1:nrow(df), nrow(df)/2)
tree_penguin <- penguins |>  tree(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,
                                  data = _,
                                  subset = train)
```

### Plot

```{r}
plot(tree_penguin)
text(tree_penguin, pretty = 0)
```
:::

## Classification Trees

::: panel-tabset
### Code

```{r}
#| code-fold: false

library(tree)
library(palmerpenguins)
library(tidyverse)
library(magrittr)
df <- penguins |>  drop_na()
train <- sample(1:nrow(df), nrow(df)/2)
tree_penguin_class <- df |>  tree(species ~ body_mass_g + flipper_length_mm + bill_length_mm + bill_depth_mm, 
                                  data = _,
                                  subset = train)
```

### Plot

```{r}
plot(tree_penguin_class)
text(tree_penguin_class, pretty = 0)
```
:::

## Pruning

::: panel-tabset
### Code

```{r}
#| code-fold: false

attach(df)
tree_penguin_cv <- cv.tree(tree_penguin)
```

### Summary

```{r}
tree_penguin_cv
```

### Plot

```{r}
plot(tree_penguin_cv$size,
     tree_penguin_cv$dev, type = "b")
```

### Pruning

```{r}
#| code-fold: false
prune_best <- prune.tree(tree_penguin, best = 7)
plot(prune_best)
text(prune_best, pretty = 0)
```
:::



## Bagging Regression Trees

::: panel-tabset
### R Code

```{r}
#| code-fold: false

library(randomForest)
library(palmerpenguins)
library(tidyverse)
library(magrittr)

penguins <- penguins  |>  drop_na()
train <- sample(1:nrow(penguins), nrow(penguins)/2)
bag_penguins <- penguins  |>  randomForest(body_mass_g ~ bill_depth_mm + bill_length_mm + flipper_length_mm,
                                           data = _,
                                           subset = train, 
                                           mtry = 3, 
                                           importance = T)

```

### Predictions

```{r}
yhat_bag <- predict(bag_penguins, newdata = penguins[-train , ])
test <- penguins[-train , ]$body_mass_g
plot(yhat_bag, test)
```
:::

## Bagging Classification Trees

::: panel-tabset
### R Code

```{r}
bag_penguins <- penguins |>  randomForest(species ~ body_mass_g + bill_depth_mm + bill_length_mm + flipper_length_mm, 
                                          data = _,
                                          subset = train, 
                                          mtry = 4, 
                                          importance = T)

```

### Prediction

```{r}
yhat_bag <- predict(bag_penguins, newdata = penguins[-train , ])
test <- penguins[-train , ]$species
table(yhat_bag, test)
```
:::

## Random Forests Regression Trees

::: panel-tabset
### R Code

```{r}
#| code-fold: false

bag_penguins <- penguins |>  randomForest(body_mass_g ~ bill_depth_mm + bill_length_mm + flipper_length_mm, 
                                          data = _,
                                          subset = train, 
                                          mtry = 2, 
                                          importance = T)

```

### Prediction

```{r}
#| code-fold: false

yhat_bag <- predict(bag_penguins, newdata = penguins[-train , ])
test <- penguins[-train , ]$body_mass_g
plot(yhat_bag, test)
```
:::

## Random Forests Classification Trees

::: panel-tabset
### R Code

```{r}
#| code-fold: false

bag_penguins <- penguins |>  randomForest(species ~ body_mass_g + bill_depth_mm + bill_length_mm + flipper_length_mm, 
                                          data = _,
                                          subset = train, 
                                          mtry = 2, 
                                          importance = T)
```

### Prediction

```{r}
#| code-fold: false

yhat_bag <- predict(bag_penguins, newdata = penguins[-train , ])
test <- penguins[-train , ]$species
table(yhat_bag, test)
```
:::

## Boosting Regression Trees

::: panel-tabset
### R Code

```{r}
#| code-fold: false

library(gbm)
boost_penguin <- gbm(body_mass_g ~ bill_depth_mm + bill_length_mm + flipper_length_mm, 
                     data = penguins[train , ],
                     distribution = "gaussian", 
                     n.trees = 5000,
                     interaction.depth = 4)
```

### Prediction

```{r}
#| code-fold: false

yhat_boost <- predict(boost_penguin, 
                      newdata = penguins[-train , ], 
                      n.trees = 5000)
test <- penguins[-train , ]$body_mass_g
plot(yhat_bag, test)
```
:::
