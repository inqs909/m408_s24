{
  "hash": "4c979a99ed6b39e571e50795b175dda3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Classification\"\nsubtitle: \"Logistic Regression\"\nformat:\n  revealjs:\n    scrollable: true\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: false \n    chalkboard:\n      src: chalkboard.json\n      storage: chalkboard_pres\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: false\n    eval: true\n    message: false\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - reveal-header\n  - code-fullscreen\n\neditor: source\n---\n\n\n## Learning Outcomes\n\n-   Logistic Regression\n\n-   Multinomial Regression\n\n## Classification\n\nThe practice of classifying data points into different categories.\n\n## Motivation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\nlibrary(tidyverse)\npenguins |> ggplot(aes(body_mass_g, flipper_length_mm, color = species)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](9a_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n## For Now ...\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins <- penguins |> drop_na() |>  \n  mutate(gentoo = ifelse(species == \"Gentoo\", \"Gentoo\", \"Other\"))\npenguins |> ggplot(aes(body_mass_g, flipper_length_mm, color = gentoo)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](9a_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n## Potential Model\n\n$$\n\\left(\\begin{array}{c}\nGentoo \\\\\nOther\n\\end{array}\\right) = \\boldsymbol X^\\mathrm T \\boldsymbol \\beta\n$$\n\n# Logistic Regression\n\n## Logistic Regression\n\nLogistic Regression is used to model the association between a set of predictors and a binary outcome.\n\n## Construct Model\n\n$$\n\\left(\\begin{array}{c}\nGentoo \\\\\nOther\n\\end{array}\\right) = \\boldsymbol X^\\mathrm T \\boldsymbol \\beta\n$$\n\n## Let ...\n\n$$\nY = \\left\\{\\begin{array}{cc}\n1 & Gentoo \\\\\n0 & Other\n\\end{array}\\right.\n$$\n\n## Construct a Model\n\n$$\nP\\left(Y = 1\\right) = \\boldsymbol X^\\mathrm T \\boldsymbol \\beta\n$$\n\n## Construct a Model\n\n$$\nP\\left(Y = 1\\right) = \\frac{\\exp(\\boldsymbol X^\\mathrm T \\boldsymbol \\beta)}{1 + \\exp(\\boldsymbol X^\\mathrm T \\boldsymbol \\beta)}\n$$\n\n## Construct a Model\n\n$$\n\\frac{P(Y = 1)}{1-P(Y = 1)} = \\exp(\\boldsymbol X^\\mathrm T \\boldsymbol \\beta)\n$$\n\n## The Logistic Model\n\n$$\n\\log\\left\\{\\frac{P(Y = 1)}{1-P(Y = 1)}\\right\\} = \\boldsymbol X^\\mathrm T \\boldsymbol \\beta\n$$\n\n## Odds and Log-Odds\n\n$$\n\\frac{P(Y = 1)}{1-P(Y = 1)}\n$$ \n\n$$\n\\log\\left\\{\\frac{P(Y = 1)}{1-P(Y = 1)}\\right\\}\n$$\n\n## Estimation\n\nEstimation is done by finding the values of $\\boldsymbol \\beta$ that maximizes the likelihood function given the data pair $(\\boldsymbol X_i, Y_i)$\n\n$$\nL(\\boldsymbol \\beta) = \\prod_{i=1}^n P(Y_i=1)^{Y_i}\\left\\{1-P(Y_i=1)\\right\\}^{1-Y_i}\n$$\n\n## Predicting Category\n\nOnce the estimates $\\hat \\beta$ are obtained, compute:\n\n$$ \nP\\left(Y_i = 1\\right) = \\frac{\\exp(\\boldsymbol X_i^\\mathrm T \\boldsymbol{\\hat\\beta})}{1 + \\exp(\\boldsymbol X^\\mathrm T \\boldsymbol{\\hat\\beta)}}\n$$\n\n$$\nY_i = \\left\\{\\begin{array}{cc}\n1 & P(Y_i =1) \\geq 0.5 \\\\\n0 & Otherwise\n\\end{array}\\right.\n$$\n\n# Ordinal Regression\n\n## Ordinal Regression\n\nOrdinal regression extends the logistic regression to more than one category ($J$ Categories) that has a natural order.\n\nAn example can be thought of as Grade Levels: A, B, C, D, F\n\n## Modeling Ordinal Responses\n\nWe can model Ordinal responses using the the proportional odds model and a logit formula:\n\n$$\n\\mathrm{logit}\\{P(Y\\leq j|X)\\} = \\boldsymbol X_{(j)} ^\\mathrm T \\boldsymbol \\beta _{(j)}\n$$\n\n## Linear Model\n\n$$\n\\boldsymbol X_{(j)}^\\mathrm T \\boldsymbol \\beta_{(j)} = \\beta_{0(j)} + \\sum^p_{i=1}X_{i}\\beta_i\n$$\n\n# Multinomial Regression\n\n## Multinomial Regression\n\n## Model\n\n$$\n\\log\\left\\{\\frac{P(Y = k)}{P(Y = \\mathrm{REF})}\\right\\} = \\boldsymbol X^\\mathrm T \\boldsymbol \\beta_k\n$$\n\n$\\mathrm{REF}$ is a reference value to be modeled.\n\n# R Examples\n\n## R Preparation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Data\nlibrary(GLMsData)\nlibrary(carData)\nlibrary(palmerpenguins)\n\n## Ordinal and Multinomial Models\nlibrary(VGAM)\n```\n:::\n\n\n## Logistic Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm(y ~ x,\n    data,\n    family = binomial())\n```\n:::\n\n\n## Logistic Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins <- penguins |> mutate(gentoo = ifelse(gentoo == \"Gentoo\", 1, 0))\nres <- penguins |> glm(gentoo ~ flipper_length_mm + body_mass_g, \n                       data = _,\n                       family = binomial())\nsummary(res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> glm(formula = gentoo ~ flipper_length_mm + body_mass_g, family = binomial(), \n#>     data = penguins)\n#> \n#> Coefficients:\n#>                     Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)       -1.316e+02  3.094e+01  -4.254 2.10e-05 ***\n#> flipper_length_mm  5.447e-01  1.384e-01   3.935 8.32e-05 ***\n#> body_mass_g        4.312e-03  1.648e-03   2.616  0.00889 ** \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 434.15  on 332  degrees of freedom\n#> Residual deviance:  38.75  on 330  degrees of freedom\n#> AIC: 44.75\n#> \n#> Number of Fisher Scoring iterations: 10\n```\n\n\n:::\n:::\n\n\n## Prediction Logistic Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(res, \n        newdata = data.frame(flipper_length_mm = 200, body_mass_g = 4005),\n        type = \"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>          1 \n#> 0.00441012\n```\n\n\n:::\n\n```{.r .cell-code}\npredict(res, \n        newdata = data.frame(flipper_length_mm = 220, body_mass_g = 4775),\n        type = \"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>         1 \n#> 0.9998484\n```\n\n\n:::\n:::\n\n\n\n## Ordinal Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvglm(y ~ x,\n    data,\n    family = propodds())\n```\n:::\n\n\n## Ordinal Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- vglm(poverty~religion+degree+country+age+gender,\n            family = propodds(),\n            data = WVS)\n```\n:::\n\n\n## Summary\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> vglm(formula = poverty ~ religion + degree + country + age + \n#>     gender, family = propodds(), data = WVS)\n#> \n#> Coefficients: \n#>                Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept):1 -0.729769   0.104044  -7.014 2.32e-12 ***\n#> (Intercept):2 -2.532483   0.110154 -22.990  < 2e-16 ***\n#> religionyes    0.179733   0.076565   2.347 0.018902 *  \n#> degreeyes      0.140918   0.066714   2.112 0.034663 *  \n#> countryNorway -0.322353   0.075444  -4.273 1.93e-05 ***\n#> countrySweden -0.603300   0.080895  -7.458 8.79e-14 ***\n#> countryUSA     0.617773   0.068391   9.033  < 2e-16 ***\n#> age            0.011141   0.001557   7.157 8.24e-13 ***\n#> gendermale     0.176370   0.052877   3.335 0.000851 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Names of linear predictors: logitlink(P[Y>=2]), logitlink(P[Y>=3])\n#> \n#> Residual deviance: 10402.59 on 10753 degrees of freedom\n#> \n#> Log-likelihood: -5201.296 on 10753 degrees of freedom\n#> \n#> Number of Fisher scoring iterations: 5 \n#> \n#> No Hauck-Donner effect found in any of the estimates\n#> \n#> \n#> Exponentiated coefficients:\n#>   religionyes     degreeyes countryNorway countrySweden    countryUSA \n#>     1.1968983     1.1513306     0.7244422     0.5470035     1.8547931 \n#>           age    gendermale \n#>     1.0112033     1.1928793\n```\n\n\n:::\n:::\n\n\n## Multinomial Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvglm(y ~ x,\n    data,\n    family = multinomial())\n```\n:::\n\n\n## Multinomial Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- penguins |> \n  vglm(species ~ body_mass_g + flipper_length_mm,\n       family = multinomial(),\n       data = _)\n```\n:::\n\n\n## Summary\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> vglm(formula = species ~ body_mass_g + flipper_length_mm, family = multinomial(), \n#>     data = penguins)\n#> \n#> Coefficients: \n#>                       Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept):1       149.242738  31.696208   4.709 2.50e-06 ***\n#> (Intercept):2       119.978676  31.301127   3.833 0.000127 ***\n#> body_mass_g:1        -0.003430   0.001669  -2.055 0.039874 *  \n#> body_mass_g:2        -0.004692   0.001649      NA       NA    \n#> flipper_length_mm:1  -0.654416   0.143847  -4.549 5.38e-06 ***\n#> flipper_length_mm:2  -0.482421   0.141237  -3.416 0.000636 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Names of linear predictors: log(mu[,1]/mu[,3]), log(mu[,2]/mu[,3])\n#> \n#> Residual deviance: 266.6108 on 660 degrees of freedom\n#> \n#> Log-likelihood: -133.3054 on 660 degrees of freedom\n#> \n#> Number of Fisher scoring iterations: 10 \n#> \n#> Warning: Hauck-Donner effect detected in the following estimate(s):\n#> '(Intercept):1', 'body_mass_g:2', 'flipper_length_mm:2'\n#> \n#> \n#> Reference group is level  3  of the response\n```\n\n\n:::\n:::\n",
    "supporting": [
      "9a_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}