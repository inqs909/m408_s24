{
  "hash": "ac04e7f39bd6a53ba5551d21d4ce1009",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Neural Networks in R\"\nformat:\n  revealjs:\n    scrollable: true\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: false \n    chalkboard:\n      src: chalkboard.json\n      storage: chalkboard_pres\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: true\n    message: false\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - reveal-header\n  - code-fullscreen\n\neditor: visual\n---\n\n\n## My Experience\n\n![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQlHz3FX7_stt68d_8oIh-f06VxyxxUYPgvE01qcYeJX5UcSFFlisdRQMJDSNbUp8wxbhc&usqp=CAU){fig-align=\"center\"}\n\n## My Experience\n\n![](https://s3.amazonaws.com/rails-camp-tutorials/blog/programming+memes/programming-or-googling.jpg){fig-align=\"center\"}\n\n## My Experience\n\n![](https://assets-global.website-files.com/5f3c19f18169b62a0d0bf387/60d33be7eedf8e1f31aabcec_BwENfmI0CU5dZGYlSyo142mpfG08-rYgTS-Qm47uMUXN6JXtmdZvtzVzTooUQdXTWmTD8uzF9N6XQJA2vUIMi53tunFyVtvOBJTNfOjHit2P_JkTmFzFsK7ep6Vb9781XZnRAryH.png){fig-align=\"center\"}\n\n## Learning Outcomes\n\n-   Tensorflow\n\n-   Torch\n\n# TensorFlow\n\n## TensorFlow\n\nTensorflow is an open-source machine learning platform developed by Google. Tensorflow is capable of completing the following tasks:\n\n-   Image Classification\n\n-   Text Classification\n\n-   Regression\n\n-   Time-Series\n\n## Keras\n\nKeras is the API that will talk to Tensorflow via different platforms.\n\n## More Information\n\n[TensorFlow for R](https://tensorflow.rstudio.com/)\n\n# Torch\n\n## Torch\n\nTorch is a scientific computing framework designed to support machine learning in CPU/GPU computing. Torch is capable of computing:\n\n-   Matrix Operations\n\n-   Linear Algebra\n\n-   Neural Networks\n\n-   Numerical Optimization\n\n-   and so much more!\n\n## Torch\n\nTorch can be accessed in both:\n\n-   Pytorch\n\n-   R Torch\n\n## R Torch\n\nR Torch is capable of handling:\n\n-   Image Recognition\n\n-   Tabular Data\n\n-   Time Series Forecasting\n\n-   Audio Processing\n\n## More Information\n\n[![](https://torch.mlverse.org/images/cover.jpg){fig-align=\"center\"}](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/)\n\n# R Code\n\n## MNIST\n\nThis is a database of handwritten digits.\n\nWe will use to construct neural networks that will classify images.\n\n## Tensorflow Installation R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"reticulate\")\n# install.packages(\"tensorflow\")\nlibrary(reticulate)\npath_to_python <- install_python()\nvirtualenv_create(\"r-reticulate\", python = path_to_python)\n# install.packages(\"keras\")\nlibrary(keras)\ninstall_keras(envname = \"r-reticulate\")\n```\n:::\n\n\n## Application of TensorFlow\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Set up python environment\nlibrary(keras)\n# install_keras()\nlibrary(reticulate)\npath_to_python <- install_python()\nuse_python(path_to_python)\nlibrary(tensorflow)\ntf$constant(\"Hello Tensorflow!\")\n###\nmnist <- dataset_mnist()\nx_train <- mnist$train$x\ng_train <- mnist$train$y\nx_test <- mnist$test$x\ng_test <- mnist$test$y\ndim(x_train)\ndim(x_test)\n###\nx_train <- array_reshape(x_train, c(nrow(x_train), 784))\nx_test <- array_reshape(x_test, c(nrow(x_test), 784))\ny_train <- to_categorical(g_train, 10)\ny_test <- to_categorical(g_test, 10)\n###\nx_train <- x_train / 255\nx_test <- x_test / 255\n###\nmodelnn <- keras_model_sequential()\nmodelnn %>%\n  layer_dense(units = 256, activation = \"relu\",\n       input_shape = c(784)) %>%\n  layer_dropout(rate = 0.4) %>%\n  layer_dense(units = 128, activation = \"relu\") %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units = 10, activation = \"softmax\")\n###\nsummary(modelnn)\n###\nmodelnn %>% compile(loss = \"categorical_crossentropy\",\n    optimizer = optimizer_rmsprop(), metrics = c(\"accuracy\")\n  )\n###\nsystem.time(\n  history <- modelnn %>%\n    fit(x_train, y_train, epochs = 30, batch_size = 128,\n        validation_split = 0.2)\n)\nplot(history, smooth = FALSE)\n###\naccuracy <- function(pred, truth)\n  mean(drop(as.numeric(pred)) == drop(truth))\nmodelnn %>% predict(x_test) %>% k_argmax() %>% accuracy(g_test)\n###\nmodellr <- keras_model_sequential() %>%\n  layer_dense(input_shape = 784, units = 10,\n       activation = \"softmax\")\nsummary(modellr)\n###\nmodellr %>% compile(loss = \"categorical_crossentropy\",\n     optimizer = optimizer_rmsprop(), metrics = c(\"accuracy\"))\nmodellr %>% fit(x_train, y_train, epochs = 30,\n      batch_size = 128, validation_split = 0.2)\nmodellr %>% predict(x_test) %>% k_argmax() %>% accuracy(g_test)\n```\n:::\n\n\n## Installation of Torch\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"torch\")\ninstall.packages(\"luz\")\ninstall.packages(\"torchvision\")\ninstall.packages(\"torchdatasets\")\ninstall.packages(\"zeallot\")\n```\n:::\n\n\n## Torch in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\nlibrary(luz) # high-level interface for torch\nlibrary(torchvision) # for datasets and image transformation\nlibrary(torchdatasets) # for datasets we are going to use\nlibrary(zeallot)\ntorch_manual_seed(13)\n\n###\ntrain_ds <- mnist_dataset(root = \".\", train = TRUE, download = TRUE)\ntest_ds <- mnist_dataset(root = \".\", train = FALSE, download = TRUE)\n\nstr(train_ds[1])\nstr(test_ds[2])\n\nlength(train_ds)\nlength(test_ds)\n###\n\n\n###\ntransform <- function(x) {\n  x %>%\n    torch_tensor() %>%\n    torch_flatten() %>%\n    torch_div(255)\n}\ntrain_ds <- mnist_dataset(\n  root = \".\",\n  train = TRUE,\n  download = TRUE,\n  transform = transform\n)\ntest_ds <- mnist_dataset(\n  root = \".\",\n  train = FALSE,\n  download = TRUE,\n  transform = transform\n)\n###\n\n\n###\nmodelnn <- nn_module(\n  initialize = function() {\n    self$linear1 <- nn_linear(in_features = 28*28, out_features = 256)\n    self$linear2 <- nn_linear(in_features = 256, out_features = 128)\n    self$linear3 <- nn_linear(in_features = 128, out_features = 10)\n\n    self$drop1 <- nn_dropout(p = 0.4)\n    self$drop2 <- nn_dropout(p = 0.3)\n\n    self$activation <- nn_relu()\n  },\n  forward = function(x) {\n    x %>%\n\n      self$linear1() %>%\n      self$activation() %>%\n      self$drop1() %>%\n\n      self$linear2() %>%\n      self$activation() %>%\n      self$drop2() %>%\n\n      self$linear3()\n  }\n)\n###\n\n\n###\nprint(modelnn())\n###\n\n\n###\nmodelnn <- modelnn %>%\n  setup(\n    loss = nn_cross_entropy_loss(),\n    optimizer = optim_rmsprop,\n    metrics = list(luz_metric_accuracy())\n  )\n###\n\n###\nsystem.time(\n   fitted <- modelnn %>%\n      fit(\n        data = train_ds,\n        epochs = 5,\n        valid_data = 0.2,\n        dataloader_options = list(batch_size = 256),\n        verbose = FALSE\n      )\n )\nplot(fitted)\n###\n\n###\naccuracy <- function(pred, truth) {\n   mean(pred == truth) }\n\n# gets the true classes from all observations in test_ds.\ntruth <- sapply(seq_along(test_ds), function(x) test_ds[x][[2]])\n\nfitted %>%\n  predict(test_ds) %>%\n  torch_argmax(dim = 2) %>%  # the predicted class is the one with higher 'logit'.\n  as_array() %>% # we convert to an R object\n  accuracy(truth)\n###\n\n###\nmodellr <- nn_module(\n  initialize = function() {\n    self$linear <- nn_linear(784, 10)\n  },\n  forward = function(x) {\n    self$linear(x)\n  }\n)\nprint(modellr())\n###\n\n###\nfit_modellr <- modellr %>%\n  setup(\n    loss = nn_cross_entropy_loss(),\n    optimizer = optim_rmsprop,\n    metrics = list(luz_metric_accuracy())\n  ) %>%\n  fit(\n    data = train_ds,\n    epochs = 5,\n    valid_data = 0.2,\n    dataloader_options = list(batch_size = 128)\n  )\n\nfit_modellr %>%\n  predict(test_ds) %>%\n  torch_argmax(dim = 2) %>%  # the predicted class is the one with higher 'logit'.\n  as_array() %>% # we convert to an R object\n  accuracy(truth)\n\n\n# alternatively one can use the `evaluate` function to get the results\n# on the test_ds\nevaluate(fit_modellr, test_ds)\n###\n```\n:::\n\n\n## My Opinion\n\n![](/files/torch.jpg){fig-align=\"center\"}\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}