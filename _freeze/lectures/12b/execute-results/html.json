{
  "hash": "ff7348e075cc53aba5d3723e8fa6a6f0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Nonparametric Regression\"\nformat:\n  revealjs:\n    scrollable: true\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: false \n    chalkboard:\n      src: chalkboard.json\n      storage: chalkboard_pres\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    \n    echo: true\n    code-fold: true\n    eval: true\n    message: false\n    warnings: false\n    comment: \"#>\" \nrevealjs-plugins:\n  - pointer\n  - verticator\nfilters: \n  - reveal-header\n  - code-fullscreen\n  - reveal-auto-agenda\neditor_options: \n  chunk_output_type: console\neditor: source\n---\n\n\n# Motivating Example\n\n## Plot\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nx <- rnorm(1000, sd = 0.25)\ny <- 1 + 5.3 * x - 45 * x^2 - 35.5 * x^3 + 60 * x^4 + rnorm(1000, sd = 0.5)\nggplot(tibble(x=x, y=y), aes(x,y)) + geom_point() + theme_bw()\n```\n\n::: {.cell-output-display}\n![](12b_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n## Plot\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nx <- rnorm(1000, sd = 3.5)\ny <- 1 + sinpi(x/8) + rnorm(1000, sd = 0.25)\nggplot(tibble(x=x, y=y), aes(x,y)) + geom_point() + theme_bw()\n```\n\n::: {.cell-output-display}\n![](12b_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n# Smoothing Splines\n\n## Regression Splines\n\nFor $L$ knots:\n\n$$\nY = \\beta_0 + \\sum^p_{j=1}x^j\\beta_j + \\sum_{l=1}^L \\beta_{p+l}(x - \\xi_l)^p_+ + \\varepsilon\n$$\n\n## Issues with Regression Splines\n\n::: incremental\n-   Must specify a correct number of knots\n-   Must specify the correct location of knots\n-   Must specify the degree p\n:::\n\n## Solutions\n\n::: incremental\n-   Use $p=3$, performs the best\n-   Implement a penalty term\n:::\n\n## Smooting Parameter\n\nA smoothing (penalty) parameter will allow us to specify a large number of knots to grid up the range of `X`.\n\n::: fragment\nThe smoothing parameter will will force the effect of certain knots to zero that are not relevant.\n:::\n\n::: fragment\nAs long as we choose a high number of knots (20-30), it will properly model the data without the worry of overfitting.\n:::\n\n::: fragment\nThe smoothing parameter can be estimated by using a cross-validation approach and identifying the which values lowers the MSE.\n:::\n\n## Likelihood Function\n\n$$\n\\sum^n_{i=1}\\{Y_i-g(X_i)\\}^2 + \\lambda\\int g^{\\prime\\prime}(t)^2dt\n$$\n\n-   $g(\\cdot)$: function used to model data (using cubic splines)\n-   $\\lambda$: smoothing parameter\n\n## Smoothing Splines in R\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nsmooth.spline(x, # Vector of predictor\n              y, # Vector of outcome\n              cv = TRUE, # Indicates to use LOOCV\n              nknots) # number of knots; can use default\n```\n:::\n\n\n## Example\n\nFit a smoothing spline model with the following simulated data. Search through the help documentation to print out $\\lambda$.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nx <- rnorm(1000, sd = 0.25)\ny <- 1 + 5.3 * x - 45 * x^2 - 35.5 * x^3 + 60 * x^4 + rnorm(1000, sd = 0.5)\n```\n:::\n\n\n# Local Regression\n\n## Local Models\n\nLocal models will construct a curve to model the data based on a set of given grid points.\n\n::: fragment\nThe grid points are selected by taking the range of `X` and choosing values that are equidistant from each other in `X`.\n:::\n\n::: fragment\nAt each grid point, fit a model based on the data points within its neighborhood.\n:::\n\n::: fragment\nData points closer to each grid point will have more importance than data points further away.\n:::\n\n## Local Neighborhood\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nx <- rnorm(1000, sd = 0.25)\ny <- 1 + 5.3 * x - 45 * x^2 - 35.5 * x^3 + 60 * x^4 + rnorm(1000, sd = 0.5)\nxx <- seq(from = range(x)[1], to = range(x)[2], by = 0.3)\n\nggplot(tibble(x=x, y=y), aes(x,y)) + \n  geom_point() +\n  geom_vline(xintercept = xx, col = \"red\", lty = 2) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](12b_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n## Importance (Weights)\n\nThe importance of a data point to a grid point is determined by the distance between the two points.\n\n::: fragment\nIf the distance is small, the data point is more important.\n:::\n\n::: fragment\nWe must assign a numerical value to the importance. Known as weight.\n:::\n\n::: fragment\nWeights can be obtained using a kernel function.\n:::\n\n## Kernel Function\n\n::: panel-tabset\n### Epanechnikov\n\n$$\nK(X_i, x_0) = w_i = 3/4\\{1-(X_i-x_0)^2\\}\n$$\n\n### Gaussian\n\n$$\nK(X_i, x_0) = w_i = \\frac{1}{\\sqrt{2\\pi}}\\exp\\{-(X_i-x_0)^2/2\\}\n$$\n\n### Triangular\n\n$$\nK(X_i, x_0) = w_i =  \\{1-|X_i-x_0|\\}\n$$\n:::\n\n## Local Constant Model\n\nFor each grid point $x_0$, find the weighted mean:\n\n$$\nf(x_0) = \\sum^n_{i=1} w_i X_i\n$$\n\n## Local Constant Model\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nx <- rnorm(1000, sd = 0.25)\ny <- 1 + 5.3 * x - 45 * x^2 - 35.5 * x^3 + 60 * x^4 + rnorm(1000, sd = 0.5)\nggplot(tibble(x=x, y=y), aes(x,y)) + \n  geom_point() +\n  stat_smooth(method = stats::loess, \n              method.args = list(degree = 0,\n                                 span = 0.05)) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](12b_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n## Local Linear Models\n\nFor each grid point $x_0$, it can be modeled with\n\n$$\n\\hat f(x_0)= \\hat \\beta_0 + \\hat\\beta_1 x_0\n$$ Estimated via:\n\n::: fragment\n$$\n\\sum^n_{i=1}\\{Y_i - \\hat f(X_i)\\}^2w_i\n$$\n:::\n\n## Local Linear Model\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nx <- rnorm(1000, sd = 0.25)\ny <- 1 + 5.3 * x - 45 * x^2 - 35.5 * x^3 + 60 * x^4 + rnorm(1000, sd = 0.5)\nggplot(tibble(x=x, y=y), aes(x,y)) + \n  geom_point() +\n  stat_smooth(method = stats::loess, \n              method.args = list(degree = 1,\n                                 span = 0.05)) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](12b_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n## Local Regression in R\n\nLocal Regression can be conducted via the `loess()` function\n\n::: panel-tabset\n### Local Constant\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nloess(y ~ x,\n      data,\n      span,\n      degree = 0)\n```\n:::\n\n\n### Local Linear\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nloess(y ~ x,\n      data,\n      span,\n      degree = 1)\n```\n:::\n\n:::\n\n# Generalized Additive Models\n\n## GAM\n\nGeneralized Additive Models extends the nonparametric models to include more than one predictor to explain the outcome `Y`.\n\n## GAM\n\n$$\nY = \\beta_0 + f_1(X_1) + f_2(X_2) + \\cdots + f_k(X_k) + \\varepsilon \n$$\n\n$$\nY = \\beta_0 + \\sum^k_{j=1} f_j(X_j) +\\varepsilon\n$$\n\n## GAM Estimation\n\nEach $\\hat f_j$ can be estimated using an approach mentioned before.\n\n$$\n\\hat Y = \\hat \\beta_0 + \\sum^k_{j=1} \\hat f_j(X_j) \n$$\n\n::: fragment\nEach $hat f_j$ can be estimated differently from other predictor functions.\n:::\n\n## GAMs for Non-Normal Variables\n\nGAMs can be extended to model outcome variables that do not follow a normal distribution.\n\n\n$$\ng(\\hat Y) = \\hat \\beta_0 + \\sum^k_{j=1} \\hat f_j(X_j) \n$$\n\n## GAMs in R\n\nThe `gam` package provides a set of functions to fit a GAM for multiple predictor variables\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(gam)\ngam(y ~ f_1 + f_2 + ...,\n    family,\n    data)\n```\n:::\n",
    "supporting": [
      "12b_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}