{
  "hash": "633e4edfa0a22ca43f88c1c245fa2f37",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: |\n  Recurrent \\\n  Neural Networks\nsubtitle: \"Time-Series, Document, and Audio Processing\"\nformat:\n  revealjs:\n    scrollable: true\n    include-in-header: \"math_commands.html\"\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: false \n    chalkboard:\n      src: chalkboard.json\n      storage: chalkboard_pres\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: true\n    message: false\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - reveal-header\n  - code-fullscreen\n  - reveal-auto-agenda\n\neditor: source\n---\n\n\n# Sequential Data\n\n## Sequential Data\n\nSequential data is data that is obtained in a series:\n\n$$\nX_{(0)}\\rightarrow\nX_{(1)}\\rightarrow\nX_{(2)}\\rightarrow\nX_{(3)}\\rightarrow\n\\cdots\\rightarrow\nX_{(J-1)}\\rightarrow\nX_{(J)}\n$$\n\n## Stochastic Procceses\n\nA stochastic process is a collection of random variables, that can be indexed by a parameters. Sequential data can be thought of as a stochastic process.\n\n::: fragment\nThe generation of a variable $X_{(j)}$ may or may not be dependent of the previous values.\n:::\n\n## Examples of Sequential Data\n\n-   Documents and Books\n\n-   Temperature\n\n-   Stock Prices\n\n-   Speech/Recordings\n\n-   Handwriting\n\n# Recurrent Neural Networks\n\n## RNN\n\nRecurrent Neural Networks are designed to analyze input data that is sequential data.\n\n::: fragment\nAn RNN can accounts for the position of a data point in the sequence as well as the distance it has to other data points.\n:::\n\n::: fragment\nUsing the data sequence, we can predict and outcome $Y$.\n:::\n\n## RNN\n\n![Source: ISLR2](img/islr2/Chapter10/10_12.png)\n\n## RNN Inputs\n\n$$\n\\boldsymbol X = (\\boldsymbol x_0, \\boldsymbol x_1, \\boldsymbol x_2, \\cdots, \\boldsymbol x_{J-1}, \\boldsymbol x_J)\n$$ where\n\n$$\n\\boldsymbol x_{j} = (x_{j1},x_{j1}, \\cdots, x_{jK})\n$$\n\n## Hidden Layer\n\n$$\nh_{j} = f(\\bbeta_{hx}\\boldsymbol x_{j} + \\bbeta_{hh}h_{j-1} + b_h)\n$$\n\n-   $\\bbeta_{hx}$ and $\\bbeta_{hh}$ are weight vectors for input-to-hidden, and hidden-to-hidden connections respectively.\n\n## Output Layer\n\n$$\ny_{j} = g(\\bbeta_{hy}h_{j} + b_y)\n$$\n\n# Document Classification\n\n## Document Classification\n\nDocument Classification is the process of classifying documents to different categories.\n\n## Corpus\n\nA corpus refers to a large and structured collection of texts, typically stored electronically, and used for linguistic analysis, natural language processing (NLP), and other computational linguistics tasks.\n\n## Corpus\n\nThe texts in a corpus can vary widely in nature, ranging from written documents, such as books, articles, and websites, to transcribed speech and social media posts\n\n## One-Hot Encoding\n\nOne-hot encoding converts categorical variables into a binary format where each category is represented by a binary vector. In this representation, each category corresponds to a unique index in the vector, and only one element in the vector is 1 (hot), while all other elements are 0 (cold).\n\n## One-Hot Encoding\n\n![](https://www.researchgate.net/publication/354354484/figure/fig8/AS:1080214163587113@1634554534870/Illustration-of-the-one-hot-encoding-and-word-embeddings-a-One-hot-encoding-and-b.jpg)\n\n## Bag-of-words Model\n\nThe Bag of Words model is a simple and fundamental technique in natural language processing (NLP). It represents text data as a collection of words, disregarding grammar and word order. Each document is represented by a vector where each dimension corresponds to a unique word in the entire corpus, and the value indicates the frequency of that word in the document. This model is widely used for tasks like text classification and sentiment analysis.\n\n## Text Mining\n\n[![](https://www.tidytextmining.com/images/cover.png)](https://www.tidytextmining.com/)\n\n## RNN\n\nA recurrent neural network can be used to account the sequential order of the words.\n\n![Source: ISLR2](img/islr2/Chapter10/10_12.png)\n\n# Time Series\n\n## Time Series\n\nA time series is a sequence of data points collected or recorded at successive, evenly spaced intervals of time. These data points can represent any variable that is observed or measured over time, such as temperature readings, stock prices, sales figures, or sensor data.\n\n## Autocorrelation\n\n\n$$\nX_{(0)}\\rightarrow\nX_{(1)}\\rightarrow\nX_{(2)}\\rightarrow\nX_{(3)}\\rightarrow\n\\cdots\\rightarrow\nX_{(J-1)}\\rightarrow\nX_{(J)}\n$$\n\n## More Information\n\n![](img/ts_408.png)\n\n## RNN\n\nA recurrent neural network can be used to account the sequential order of each measurement.\n\n![Source: ISLR2](img/islr2/Chapter10/10_12.png)\n\n\n# R Code Document Classification \n\n## R Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\nlibrary(luz) # high-level interface for torch\nlibrary(torchdatasets) # for datasets we are going to use\nlibrary(zeallot)\ntorch_manual_seed(909)\n```\n:::\n\n\n\n## IMDB Classification\n\n::: {.panel-tabset}\n\n\n## Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nmax_features <- 10000\nimdb_train <- imdb_dataset(\n  root = \".\", \n  download = TRUE,\n  split=\"train\",\n  num_words = max_features\n)\nimdb_test <- imdb_dataset(\n  root = \".\", \n  download = TRUE,\n  split=\"test\",\n  num_words = max_features\n)\n```\n:::\n\n\n## Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimdb_train[1]$x[1:12]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  [1]    2   25  124   25   26   11 1113  149    6  211   54    4\n```\n\n\n:::\n:::\n\n## Decode\n\n\n::: {.cell}\n\n```{.r .cell-code}\nword_index <- imdb_train$vocabulary\ndecode_review <- function(text, word_index) {\n   word <- names(word_index)\n   idx <- unlist(word_index, use.names = FALSE)\n   word <- c(\"<PAD>\", \"<START>\", \"<UNK>\", word)\n   words <- word[text]\n   paste(words, collapse = \" \")\n}\ndecode_review(imdb_train[1]$x[1:12], word_index)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] \"<START> you know you are in trouble watching a comedy when the\"\n```\n\n\n:::\n:::\n\n\n## One-hot Decode\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Matrix)\none_hot <- function(sequences, dimension) {\n   seqlen <- sapply(sequences, length)\n   n <- length(seqlen)\n   rowind <- rep(1:n, seqlen)\n   colind <- unlist(sequences)\n   sparseMatrix(i = rowind, j = colind,\n      dims = c(n, dimension))\n}\n\n# collect all values into a list\ntrain <- seq_along(imdb_train) %>% \n  lapply(function(i) imdb_train[i]) %>% \n  purrr::transpose()\ntest <- seq_along(imdb_test) %>% \n  lapply(function(i) imdb_test[i]) %>% \n  purrr::transpose()\n\n# num_words + padding + start + oov token = 10000 + 3\nx_train_1h <- one_hot(train$x, 10000 + 3)\nx_test_1h <- one_hot(test$x, 10000 + 3)\ndim(x_train_1h)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 25000 10003\n```\n\n\n:::\n\n```{.r .cell-code}\nnnzero(x_train_1h) / (25000 * (10000 + 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.01316756\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(3)\nival <- sample(seq(along = train$y), 2000)\nitrain <- seq_along(train$y)[-ival]\ny_train <- unlist(train$y)\n```\n:::\n\n\n\n:::\n\n## Neural Network\n\n::: {.panel-tabset}\n\n## Modules\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- nn_module(\n  initialize = function(input_size = 10000 + 3) {\n    self$dense1 <- nn_linear(input_size, 16)\n    self$relu <- nn_relu()\n    self$dense2 <- nn_linear(16, 16)\n    self$output <- nn_linear(16, 1)\n  },\n  forward = function(x) {\n    x |> \n      self$dense1() |>  \n      self$relu() |> \n      self$dense2() |>  \n      self$relu() |> \n      self$output() |> \n      torch_flatten(start_dim = 1)\n  }\n)\n```\n:::\n\n\n## Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- model |> \n  setup(\n    loss = nn_bce_with_logits_loss(),\n    optimizer = optim_rmsprop,\n    metrics = list(luz_metric_binary_accuracy_with_logits())\n  ) |> \n  set_opt_hparams(lr = 0.001)\n```\n:::\n\n\n## Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted <- model |> \n  fit(\n    # we transform the training and validation data into torch tensors\n    list(\n      torch_tensor(as.matrix(x_train_1h[itrain,]), dtype = torch_float()), \n      torch_tensor(unlist(train$y[itrain]))\n    ),\n    valid_data = list(\n      torch_tensor(as.matrix(x_train_1h[ival, ]), dtype = torch_float()), \n      torch_tensor(unlist(train$y[ival]))\n    ),\n    dataloader_options = list(batch_size = 512),\n    epochs = 10\n  )\n```\n:::\n\n\n## Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fitted)  \n```\n\n::: {.cell-output-display}\n![](15a_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n:::\n\n## RNN Document Classification\n\n::: {.panel-tabset}\n\n## Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmaxlen <- 500\nnum_words <- 10000\nimdb_train <- imdb_dataset(root = \".\", split = \"train\", num_words = num_words,\n                           maxlen = maxlen)\nimdb_test <- imdb_dataset(root = \".\", split = \"test\", num_words = num_words,\n                           maxlen = maxlen)\n\nvocab <- c(rep(NA, imdb_train$index_from - 1), imdb_train$get_vocabulary())\ntail(names(vocab)[imdb_train[1]$x])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] \"compensate\" \"you\"        \"the\"        \"rental\"     \"\"          \n#> [6] \"d\"\n```\n\n\n:::\n:::\n\n\n## Modules\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- nn_module(\n  initialize = function() {\n    self$embedding <- nn_embedding(10000 + 3, 32)\n    self$lstm <- nn_lstm(input_size = 32, hidden_size = 32, batch_first = TRUE)\n    self$dense <- nn_linear(32, 1)\n  },\n  forward = function(x) {\n    c(output, c(hn, cn)) %<-% (x %>% \n      self$embedding() %>% \n      self$lstm())\n    output[,-1,] %>%  # get the last output\n      self$dense() %>% \n      torch_flatten(start_dim = 1)\n  }\n)\n```\n:::\n\n\n## Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- model %>% \n  setup(\n    loss = nn_bce_with_logits_loss(),\n    optimizer = optim_rmsprop,\n    metrics = list(luz_metric_binary_accuracy_with_logits())\n  ) %>% \n  set_opt_hparams(lr = 0.001)\n```\n:::\n\n\n## Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted <- model %>% fit(\n  imdb_train, \n  epochs = 10,\n  dataloader_options = list(batch_size = 128),\n  valid_data = imdb_test\n)\n```\n:::\n\n\n:::\n\n\n## RNN Diagnostics\n\n::: {.panel-tabset}\n\n## Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fitted)\n```\n\n::: {.cell-output-display}\n![](15a_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n## Evaluation\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredy <- torch_sigmoid(predict(fitted, imdb_test)) > 0.5\nevaluate(fitted, imdb_test, dataloader_options = list(batch_size = 512))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> A `luz_module_evaluation`\n#> ── Results ─────────────────────────────────────────────────────────────────────\n#> loss: 0.3804\n#> acc: 0.843\n```\n\n\n:::\n:::\n\n\n:::\n\n# R Code Time-Series\n\n## Time-Series\n\n::: {.panel-tabset}\n\n## Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ISLR2)\nxdata <- data.matrix(\n NYSE[, c(\"DJ_return\", \"log_volume\",\"log_volatility\")]\n )\nistrain <- NYSE[, \"train\"]\nxdata <- scale(xdata)\n```\n:::\n\n\n\n## Lag\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlagm <- function(x, k = 1) {\n   n <- nrow(x)\n   pad <- matrix(NA, k, ncol(x))\n   rbind(pad, x[1:(n - k), ])\n}\n\narframe <- data.frame(log_volume = xdata[, \"log_volume\"],\n   L1 = lagm(xdata, 1), L2 = lagm(xdata, 2),\n   L3 = lagm(xdata, 3), L4 = lagm(xdata, 4),\n   L5 = lagm(xdata, 5)\n )\n```\n:::\n\n\n## Remove Missing\n\n\n::: {.cell}\n\n```{.r .cell-code}\narframe <- arframe[-(1:5), ]\nistrain <- istrain[-(1:5)]\n```\n:::\n\n\n## RNN Prep\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- nrow(arframe)\nxrnn <- data.matrix(arframe[, -1])\nxrnn <- array(xrnn, c(n, 3, 5))\nxrnn <- xrnn[,, 5:1]\nxrnn <- aperm(xrnn, c(1, 3, 2))\ndim(xrnn)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 6046    5    3\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n## Neural Network\n\n::: {.panel-tabset}\n## Modules\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- nn_module(\n  initialize = function() {\n    self$rnn <- nn_rnn(3, 12, batch_first = TRUE)\n    self$dense <- nn_linear(12, 1)\n    self$dropout <- nn_dropout(0.2)\n  },\n  forward = function(x) {\n    c(output, ...) %<-% (x |> \n      self$rnn())\n    output[,-1,] |> \n      self$dropout() |> \n      self$dense() |> \n      torch_flatten(start_dim = 1)\n  }\n)\n```\n:::\n\n\n## Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- model |> \n  setup(\n    optimizer = optim_rmsprop,\n    loss = nn_mse_loss()\n  ) |> \n  set_opt_hparams(lr = 0.001)\n```\n:::\n\n\n## Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted <- model %>% fit(\n    list(xrnn[istrain,, ], arframe[istrain, \"log_volume\"]),\n    epochs = 30, # = 200,\n    dataloader_options = list(batch_size = 64),\n    valid_data =\n      list(xrnn[!istrain,, ], arframe[!istrain, \"log_volume\"])\n  )\n```\n:::\n\n\n\n## Prediction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkpred <- as.numeric(predict(fitted, xrnn[!istrain,, ]))\nV0 <- var(arframe[!istrain, \"log_volume\"])\n1 - mean((kpred - arframe[!istrain, \"log_volume\"])^2) / V0\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.4055528\n```\n\n\n:::\n:::\n\n\n\n:::\n\n\n\n::: {.cell}\n\n:::\n",
    "supporting": [
      "15a_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}