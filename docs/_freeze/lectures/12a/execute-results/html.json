{
  "hash": "7e2cdab09898c82d7c1334c74f3cc1d0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Model Selection\"\nformat:\n  revealjs:\n    scrollable: true\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: false \n    chalkboard:\n      src: chalkboard.json\n      storage: chalkboard_pres\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: true\n    message: false\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - reveal-header\n  - code-fullscreen\n\neditor: visual\n---\n\n\n## Learning Outcomes\n\n-   Selection Criteria\n\n-   Subset Selection\n\n-   Shrinkage Methods\n\n## Model Building\n\nWhen given a set of predictors, we want to build a model that only contains predictors that best fits the data, without overfitting.\n\n## Model Building\n\nIdeally, we always want to choose a parsimonious model that best describes the outcome variable. The more predictors into the model, the less parsimonious and less powerful.\n\nChoosing the best model can be done based on selection criteria such as Mallow's $C_p$, AIC, AICc, BIC, and adjusted $R^2$.\n\n# Selection Criteria\n\n## Mallow's $C_p$\n\n$$\nC_p = \\frac{1}{n}(RSS + 2 d \\hat \\sigma^2) \n$$\n\n-   $RSS$: Residual Sum of Squares\n\n-   $\\hat \\sigma^2$: Mean Square Error\n\n-   $d$: number of predictors\n\n-   Lower is better\n\n## Aikaike Information Criteria (AIC)\n\n$$\n\\frac{1}{n\\hat\\sigma^2}(RSS+2d\\hat\\sigma^2)\n$$\n\n-   Lower is Better\n\n## Bayesian Information Criteria (BIC)\n\n$$\n\\frac{1}{n\\hat\\sigma^2}\\{RSS+\\log(n)d\\hat\\sigma^2\\}\n$$\n\n-   Lower is better\n\n## $R^2$\n\n$$\n1-\\frac{RSS}{TSS}\n$$\n\n-   $RSS=\\sum^n_{i=1}(y_i-\\hat y_i)^2$\n\n-   $TSS=\\sum^n_{i=1}(y_i-\\bar y)^2$\n\n-   Higher is Better\n\n## Adjusted $R^2$\n\n$$\n1-\\frac{RSS/(n-d-1)}{\nTSS/(n-1)}\n$$\n\n-   Higher is Better\n\n# Subset Selection\n\n## Model Building\n\n-   Best Subset Model\n\n    -   Fit all models and select the best model based criteria\n\n-   Forward Stepwise Model Building\n\n    -   Begin with the null model ($Y\\sim 1$) and add variables until a final model is chosen.\n\n-   Backward Stepwise Model Building\n\n    -   Begin with the full model, and remove variable until the final model is chosen.\n\n-   Hybrid Stepwise Regression\n\n    -   A hybrid approach between the forward and backward building approach.\n\n## Best Subset Model Building\n\n1.  Begin with the null model, no predictors\n2.  For $k=1,\\ldots, p$ (number of predictors):\n    1.  Fit all $\\left(^p_k\\right)$ models that contain $k$ predictors\n\n    2.  Define $M_k$ as the model with the largest $R²$\n3.  The final model is the model $M_k$ based on selection criteria\n\n## Forward Stepwise Model Building\n\n1.  Begin with the null model, no predictors\n2.  For $k=0,\\ldots, p-1$ (number of predictors):\n    1.  Fit all $p-k$ models that adds one new predictor to the orginal model containing $k$ predictors\n\n    2.  Define $M_{k+1}$ as the model with the largest $R²$ among $p-k$ models\n3.  The final model is the model $M_(k+1)$ based on selection criteria\n\n## Backward Stepwise Model Building\n\n1.  Begin with the full model $M_p$, with all predictors\n2.  For $k=p,p-1, \\ldots, 1$ (number of predictors):\n    1.  Fit all models that contain $k-1$ predictors\n\n    2.  Define $M_{k-1}$ as the model with the largest $R²$\n3.  The final model is the model $M_k$ based on selection criteria\n\n## R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(leaps)\nregsubsets(y ~ ., data)\n```\n:::\n\n\n## Full Subset\n\n::: panel-tabset\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(leaps)\nlm_full <- regsubsets(mpg ~ ., data = mtcars)\n```\n:::\n\n\n## Summary\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum_lm_full <- summary(lm_full)\nprint(sum_lm_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Subset selection object\n#> Call: regsubsets.formula(mpg ~ ., data = mtcars)\n#> 10 Variables  (and intercept)\n#>      Forced in Forced out\n#> cyl      FALSE      FALSE\n#> disp     FALSE      FALSE\n#> hp       FALSE      FALSE\n#> drat     FALSE      FALSE\n#> wt       FALSE      FALSE\n#> qsec     FALSE      FALSE\n#> vs       FALSE      FALSE\n#> am       FALSE      FALSE\n#> gear     FALSE      FALSE\n#> carb     FALSE      FALSE\n#> 1 subsets of each size up to 8\n#> Selection Algorithm: exhaustive\n#>          cyl disp hp  drat wt  qsec vs  am  gear carb\n#> 1  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n#> 2  ( 1 ) \"*\" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n#> 3  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#> 4  ( 1 ) \" \" \" \"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#> 5  ( 1 ) \" \" \"*\"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#> 6  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#> 7  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \"*\"  \" \" \n#> 8  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \"*\"  \"*\"\n```\n\n\n:::\n:::\n\n\n## Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(sum_lm_full$cp)\n```\n\n::: {.cell-output-display}\n![](12a_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n## Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(lm_full,3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> (Intercept)          wt        qsec          am \n#>    9.617781   -3.916504    1.225886    2.935837\n```\n\n\n:::\n:::\n\n:::\n\n## Forward Model Building\n\n::: panel-tabset\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_full <- regsubsets(mpg ~ ., data = mtcars,\n                      method =  \"forward\")\n```\n:::\n\n\n## Summary\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum_lm_full <- summary(lm_full)\nprint(sum_lm_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Subset selection object\n#> Call: regsubsets.formula(mpg ~ ., data = mtcars, method = \"forward\")\n#> 10 Variables  (and intercept)\n#>      Forced in Forced out\n#> cyl      FALSE      FALSE\n#> disp     FALSE      FALSE\n#> hp       FALSE      FALSE\n#> drat     FALSE      FALSE\n#> wt       FALSE      FALSE\n#> qsec     FALSE      FALSE\n#> vs       FALSE      FALSE\n#> am       FALSE      FALSE\n#> gear     FALSE      FALSE\n#> carb     FALSE      FALSE\n#> 1 subsets of each size up to 8\n#> Selection Algorithm: forward\n#>          cyl disp hp  drat wt  qsec vs  am  gear carb\n#> 1  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n#> 2  ( 1 ) \"*\" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n#> 3  ( 1 ) \"*\" \" \"  \"*\" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n#> 4  ( 1 ) \"*\" \" \"  \"*\" \" \"  \"*\" \" \"  \" \" \"*\" \" \"  \" \" \n#> 5  ( 1 ) \"*\" \" \"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#> 6  ( 1 ) \"*\" \"*\"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#> 7  ( 1 ) \"*\" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#> 8  ( 1 ) \"*\" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \"*\"  \" \"\n```\n\n\n:::\n:::\n\n\n## Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(sum_lm_full$bic)\n```\n\n::: {.cell-output-display}\n![](12a_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n## Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(lm_full,2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> (Intercept)         cyl          wt \n#>   39.686261   -1.507795   -3.190972\n```\n\n\n:::\n:::\n\n:::\n\n## Backward Model Building\n\n::: panel-tabset \n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_full <- regsubsets(mpg ~ ., data = mtcars,\n                      method =  \"backward\")\n```\n:::\n\n\n## Summary\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum_lm_full <- summary(lm_full)\nprint(sum_lm_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Subset selection object\n#> Call: regsubsets.formula(mpg ~ ., data = mtcars, method = \"backward\")\n#> 10 Variables  (and intercept)\n#>      Forced in Forced out\n#> cyl      FALSE      FALSE\n#> disp     FALSE      FALSE\n#> hp       FALSE      FALSE\n#> drat     FALSE      FALSE\n#> wt       FALSE      FALSE\n#> qsec     FALSE      FALSE\n#> vs       FALSE      FALSE\n#> am       FALSE      FALSE\n#> gear     FALSE      FALSE\n#> carb     FALSE      FALSE\n#> 1 subsets of each size up to 8\n#> Selection Algorithm: backward\n#>          cyl disp hp  drat wt  qsec vs  am  gear carb\n#> 1  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n#> 2  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \"*\"  \" \" \" \" \" \"  \" \" \n#> 3  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#> 4  ( 1 ) \" \" \" \"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#> 5  ( 1 ) \" \" \"*\"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#> 6  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#> 7  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \"*\"  \" \" \n#> 8  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \"*\"  \"*\"\n```\n\n\n:::\n:::\n\n\n## Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(sum_lm_full$adjr2)\n```\n\n::: {.cell-output-display}\n![](12a_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n## Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(lm_full,6)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> (Intercept)        disp          hp        drat          wt        qsec \n#> 10.71061639  0.01310313 -0.02179818  1.02065283 -4.04454214  0.99072948 \n#>          am \n#>  2.98468801\n```\n\n\n:::\n:::\n\n\n\n:::\n\n## Selection Critera\n\n\n# Shrinkage Methods\n\n## Shrinkage Methods\n\nShrinkage methods are techniques that will reduce a full parameterized model (a high number of predictors) to a lower parameterized model (a smaller number of predictors).\n\n## Ridge Regression\n\nRidge regression incorporates a shrinkage penalty term to the least squares formula. The shrinkage penalty term will reduce the $\\beta$ coefficients towards 0 based on a penalty parameter ($\\lambda$)\n\n## Ridge Regression\n\n$$\n\\sum^n_{i=1}\\left(Y_i-\\beta_0 +\\sum^p_{j=1}X_{ij}\\beta_j\\right)^2 + \\lambda\\sum^p_{j=1}\\beta_j^2\n$$\n\n## LASSO\n\nLeast Absolute Shrinkage and Selection Operator (LASSO) is known as a shrinkage method which forces $\\beta$ coefficients that do not have a significant predicit power towards and possibly equal to 0.\n\n## LASSO\n\n$$\n\\sum^n_{i=1}\\left(Y_i-\\beta_0 +\\sum^p_{j=1}X_{ij}\\beta_j\\right)^2 + \\lambda\\sum^p_{j=1}|\\beta_j|\n$$\n\n## Why Ridge or LASSO?\n\nEach method is capable on identifying the optimum MSE for the Bias-Variance trade-off scenario. This will lead to a lower prediction error. The key is to find the optimal penalty parameter. This can be done with a Cross-Validation technique (next lecture).\n\n## Ridge Regression in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\nglmnet(x,\n       y,\n       alpha = 0,\n       lambda)\n```\n:::\n\n\n## LASSO in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\nglmnet(x,\n       y,\n       alpha = 1,\n       lambda)\n```\n:::\n\n\n## Example\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\npenguins <- penguins |> drop_na()\nmod <- penguins |> model.matrix(~ flipper_length_mm + bill_depth_mm + bill_length_mm - 1,\n  data = _) # Must include -1 to remove intercept, needed for glmnet\nridge_mod <- glmnet(x = mod, \n                    y = penguins$body_mass_g,\n                    alpha = 0,\n                    lambda = 1.3) # lambda was chosen at random\ncoef(ridge_mod)[,1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>       (Intercept) flipper_length_mm     bill_depth_mm    bill_length_mm \n#>       -6400.54459          50.53481          17.07967           3.60524\n```\n\n\n:::\n:::\n\n\n## Example\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_mod <- glmnet(x = mod, \n                    y = penguins$body_mass_g,\n                    alpha = 1,\n                    lambda = 1.3) # lambda was chosen at random\ncoef(lasso_mod)[,1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>       (Intercept) flipper_length_mm     bill_depth_mm    bill_length_mm \n#>      -6372.371080         50.530548         16.241920          3.311152\n```\n\n\n:::\n:::\n",
    "supporting": [
      "12a_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}