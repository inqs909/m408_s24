{
  "hash": "6185604587ea04fc8591d47269488e46",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Deep Learning\"\nformat:\n  revealjs:\n    scrollable: true\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: false \n    chalkboard:\n      src: chalkboard.json\n      storage: chalkboard_pres\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: true\n    message: false\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - reveal-header\n  - code-fullscreen\n  - reveal-auto-agenda\n\neditor: source\n---\n\n\n\n# Neural Networks\n\n## Neural Networks\n\nNeural networks are a type of machine learning algorithm that are designed to mimic the function of the human brain. They consist of interconnected nodes or \"neurons\" that process information and generate outputs based on the inputs they receive.\n\n## Uses\n\nNeural networks are typically used for tasks such as image recognition, natural language processing, and prediction. They are capable of learning from data and improving their performance over time, which makes them well-suited for complex and dynamic problems.\n\n## Single Layer Neural Networks\n\nA single layer neural networks can be formulated as linear function:\n\n$$\nf(X) = \\beta_0 + \\sum^K_{k=1}\\beta_kh_k(X)\n$$\n\nWhere $X$ is a vector of inputs of length $p$ and $K$ is the number of activations, $\\beta_j$ are the regression coefficients and\n\n$$\nh_k(X) = A_k = g(w_{k0} + \\sum^p_{l1}w_{kl}X_{l})\n$$\n\nwith $g(\\cdot)$ being a nonlinear function and $w_{kl}$ are the weights.\n\n## Nonlinear (Activations) Function $g(\\cdot)$\n\n-   Sigmoidal: $g(z) = \\frac{e^z}{1+e^z}$\n\n-   ReLU (rectified linear unit): $g(z) = (z)_+ = zI(z\\geq0)$\n\n## Single Layer Neural Network\n\n![](https://www.oreilly.com/api/v2/epubs/9781789808452/files/assets/290136cc-48f2-47b1-bb95-ffdb625b987d.png){fig-align=\"center\"}\n\n## Multilayer Neural Network\n\nMultilayer Neural Networks create multiple hidden layers where each layer feeds into each other which will create a final outcome.\n\n## Multilayer Neural Network\n\n![](https://media.geeksforgeeks.org/wp-content/cdn-uploads/20230602113310/Neural-Networks-Architecture.png){fig-align=\"center\"}\n\n## Hidden Layer 1\n\nWith $p$ predictors of $X$:\n\n$$\nh^{(1)}_k(X) = A^{(1)}_k = g\\left\\{w^{(1)}_{k0} + \\sum^p_{j=1}w^{(1)}_{kj}X_{j}\\right\\}\n$$\nfor $k = 1, \\cdots, K$ nodes.\n\n## Hidden Layer 2\n\n$$\nh^{(2)}_l(X) = A^{(2)}_l = g\\left\\{w^{(2)}_{l0} + \\sum^K_{k=1}w^{(2)}_{lk}A^{(1)}_{k}\\right\\}\n$$\nfor $l = 1, \\cdots, L$ nodes.\n\n\n## Hidden Layer 3 +\n\n$$\nh^{(3)}_m(X) = A^{(3)}_l = g\\left\\{w^{(3)}_{m0} + \\sum^L_{l=1}w^{(3)}_{ml}A^{(2)}_{l}\\right\\}\n$$\nfor $m = 1, \\cdots, M$ nodes.\n\n## Output Layer\n\n$$\nf_t(X) = \\beta_{t0} + \\sum^M_{m=1}\\beta_{tm}h^{(3)}_m(X)\n$$\nfor outcomes $t = 1, \\cdots, T$\n\n# Stochastic Gradient Descent\n\n## Gradient Descent\n\nThis is an optimization algorithm used to identify the maximum or minimum of a functions.\n\n## Method\n\nFor a given $F(\\boldsymbol X)$, the minimum $\\boldsymbol X^\\prime$ can be found by iterating the following formula:\n\n$$\n\\boldsymbol X_{j+1} = \\boldsymbol X_{j} - \\gamma \\nabla F(\\boldsymbol X_j)\n$$\nwhere\n\n$$\n\\boldsymbol X_j \\rightarrow \\boldsymbol X^\\prime\n$$\n\n\n-   $\\nabla F$: gradient\n-   $\\gamma \\in (0,1]$: is the step size\n\n## Stochastic Gradient Descent\n\n\n Let:\n \n$$\nF(\\boldsymbol X) = \\sum^n_{i=1} f(y_i; \\boldsymbol X)\n$$\n\nInstead of minimizing $F(\\boldsymbol X)$ at each step, we minimize:\n\n$$\nF_\\alpha(\\boldsymbol X) = \\sum_{i\\in\\alpha} f(y_i; \\boldsymbol X)\n$$\nwhere $\\alpha$ indicates the data points randomly sampled to be used to compute the gradient.\n\n# R6\n\n## Object-Oriented Programming\n\nLoosely speaking Object-Oriented Programming involves developing R objects with specialized attributes.\n\n::: fragment\nThere are 3 main types of OOP: S3, R6, S4\n:::\n\n::: fragment\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxlm <- lm(mpg ~ wt, mtcars)\nattributes(xlm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> $names\n#>  [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n#>  [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n#>  [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n#> \n#> $class\n#> [1] \"lm\"\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsloop::s3_methods_generic(\"summary\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 37 × 4\n#>    generic class                 visible source             \n#>    <chr>   <chr>                 <lgl>   <chr>              \n#>  1 summary aov                   TRUE    stats              \n#>  2 summary aovlist               FALSE   registered S3method\n#>  3 summary aspell                FALSE   registered S3method\n#>  4 summary check_packages_in_dir FALSE   registered S3method\n#>  5 summary connection            TRUE    base               \n#>  6 summary data.frame            TRUE    base               \n#>  7 summary Date                  TRUE    base               \n#>  8 summary default               TRUE    base               \n#>  9 summary ecdf                  FALSE   registered S3method\n#> 10 summary factor                TRUE    base               \n#> # ℹ 27 more rows\n```\n\n\n:::\n:::\n\n\n:::\n\n## R6\n\nR6 is slightly different, where the generic functions belong to the object instead.\n\n::: fragment\nThis allows us to modify an R objects with different methodologies if needed.\n:::\n\n::: fragment\nThis is also useful when interacting with objects outside of R's environment.\n:::\n\n## `self`\n\n`self` is a special container within R6 that contains all the items and functions within the object.\n\n::: fragment\nYou must always use `self` when dealing with R6 objects.\n:::\n\n\n# TensorFlow\n\n## TensorFlow\n\nTensorflow is an open-source machine learning platform developed by Google. Tensorflow is capable of completing the following tasks:\n\n-   Image Classification\n\n-   Text Classification\n\n-   Regression\n\n-   Time-Series\n\n## Keras\n\nKeras is the API that will talk to Tensorflow via different platforms.\n\n## More Information\n\n[TensorFlow for R](https://tensorflow.rstudio.com/)\n\n# Torch\n\n## Torch\n\nTorch is a scientific computing framework designed to support machine learning in CPU/GPU computing. Torch is capable of computing:\n\n-   Matrix Operations\n\n-   Linear Algebra\n\n-   Neural Networks\n\n-   Numerical Optimization\n\n-   and so much more!\n\n## Torch\n\nTorch can be accessed in both:\n\n-   Pytorch\n\n-   R Torch\n\n## R Torch\n\nR Torch is capable of handling:\n\n-   Image Recognition\n\n-   Tabular Data\n\n-   Time Series Forecasting\n\n-   Audio Processing\n\n## More Information\n\n[![](https://torch.mlverse.org/images/cover.jpg){fig-align=\"center\"}](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/)\n\n# R Code\n\n## Installation of Torch\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"torch\")\ninstall.packages(\"luz\")\ninstall.packages(\"torchvision\")\ninstall.packages(\"torchdatasets\")\ninstall.packages(\"zeallot\")\n```\n:::\n\n\n## Torch Packages in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# \nlibrary(torch)\nlibrary(luz) # high-level interface for torch\nlibrary(torchvision) # for datasets and image transformation\nlibrary(torchdatasets) # for datasets we are going to use\nlibrary(zeallot)\ntorch_manual_seed(13)\n```\n:::\n\n\n\n## ISLR Torch Lab\n\nISLR uses Tensorflow.\n\nUse this instead:\n<https://hastie.su.domains/ISLR2/Labs/Rmarkdown_Notebooks/Ch10-deeplearning-lab-torch.html>\n\n# Single Layer R Code\n\n## Penguin Data\n\n::: {.panel-tabset}\n\n## Description\n\nBuild a single-layer neural network that will predict `body_mass_g` with the remaining predictors except for `year`. The hidden layer will contain 50 nodes, and the activation functions will be ReLU.\n\n## Train/Test\n  \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\n\npenguins <- penguins |> drop_na() |> select(-year)\ntraining <- penguins |> slice_sample(prop = .8)\ntesting <- penguins |> anti_join(training)\n```\n:::\n\n  \n  \n##  Training\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXtraining <- training |> \n  model.matrix(body_mass_g ~ . - 1, data = _) |> \n  scale() |> \n  torch_tensor(dtype = torch_float())\n\nYtraining <- training |> \n  select(body_mass_g) |> \n  as.matrix() |> \n  torch_tensor(dtype = torch_float())\n```\n:::\n\n\n## Testing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXtesting <- testing |> \n  model.matrix(body_mass_g ~ . - 1, data = _) |> \n  scale() |> \n  torch_tensor(dtype = torch_float())\n\nYtesting <- testing |> \n  select(body_mass_g) |> \n  as.matrix() |> \n  torch_tensor(dtype = torch_float())\n```\n:::\n\n\n:::\n\n\n## Model Description\n\n::: {.panel-tabset}\n## Overall\n  \n\n::: {.cell}\n\n```{.r .cell-code}\nmodnn <- nn_module(\n  initialize = function(input_size) {\n    self$hidden <- nn_linear(input_size, 50)\n    self$activation <- nn_relu()\n    self$dropout <- nn_dropout(0.4)\n    self$output <- nn_linear(50, 1)\n  },\n  forward = function(x) {\n    x |> \n      self$hidden() |>  \n      self$activation() |>  \n      self$dropout() |>  \n      self$output()\n  }\n)\n```\n:::\n\n  \n## Initialize\n\nCreates the functions needed to describe the details of each network.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninitialize = function(input_size) {\n    self$hidden <- nn_linear(input_size, 50)\n    self$activation <- nn_relu()\n    self$dropout <- nn_dropout(0.4)\n    self$output <- nn_linear(50, 1)\n  }\n```\n:::\n\n\n\n## Forward\n\nModels the neural network. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nforward = function(x) {\n    x |> \n      self$hidden() |>  \n      self$activation() |>  \n      self$dropout() |>  \n      self$output()\n  }\n```\n:::\n\n\n\n:::\n\n\n\n## Optimizer Set Up\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodnn <- modnn |> \n  setup(\n    loss = nn_mse_loss(), # Used for numerical counts\n    optimizer = optim_rmsprop,\n    metrics = list(luz_metric_mae())\n  ) |>\n  set_hparams(input_size = ncol(Xtraining))\n```\n:::\n\n\n\n## Fit a Model\n\n::: {.panel-tabset}\n## Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted <- modnn |> \n  fit(\n    data = list(Xtraining, Ytraining),\n    epochs = 100 # Can think as number of iterations\n  )\n```\n:::\n\n\n## Plot\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot(fitted)\n```\n\n::: {.cell-output-display}\n![](14a_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n:::\n\n\n## Testing Model\n\n::: {.panel-tabset}\n\n## Prediction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnpred <- predict(fitted, Xtesting)\n```\n:::\n\n\n## MAE\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(abs(as.matrix(Ytesting) - as.matrix(npred)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 422.8148\n```\n\n\n:::\n:::\n\n\n## Plot\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot(as.matrix(Ytesting), as.matrix(npred),\n     xlab = \"Truth\",\n     ylab = \"Predicted\")\n```\n\n::: {.cell-output-display}\n![](14a_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n:::\n\n\n# Multilayer R Code\n\n## Penguins Data\n\n::: {.panel-tabset}\n\n## Description\n\nUse the `penguins` data set to construct a 2-layer neural network to predict `species` with the other predictors, except for year. The 1st layer should have 10 nodes, the second layer should have 5 nodes, and use ReLU activations.\n\n## Prep\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntraining <- penguins |> slice_sample(prop = .8)\ntesting <- penguins |> anti_join(training)\n```\n:::\n\n\n## Training\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXtraining <- training |> \n  model.matrix(species ~ . - 1, data = _) |> \n  scale()\n\nXtesting <- testing |> \n  model.matrix(species ~ . - 1, data = _) |> \n  scale()\n```\n:::\n\n\n## Testing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nYtraining <- training |> \n  select(species) |> \n  as.matrix() |> \n  as.factor()\n\nYtesting <- testing |> \n  select(species) |> \n  as.matrix() |> \n  as.factor()\n```\n:::\n\n\n:::\n\n## Model\n\n::: {.panel-tabset}\n\n## Overall\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelnn2 <- nn_module(\n  initialize = function(input_size) {\n    self$hidden1 <- nn_linear(in_features = input_size, \n                              out_features = 10)\n    self$hidden2 <- nn_linear(in_features = 10, \n                              out_features = 5)\n    self$output <- nn_linear(in_features = 5, \n                             out_features = 3)\n    \n    self$drop1 <- nn_dropout(p = 0.4)\n    self$drop2 <- nn_dropout(p = 0.3)\n    \n    self$activation <- nn_relu()\n  },\n  forward = function(x) {\n    x |>  \n      self$hidden1() |>  \n      self$activation() |>  \n      self$drop1() |>  \n      \n      self$hidden2() |>  \n      self$activation() |>  \n      self$drop2() |>  \n      \n      self$output()\n  }\n)\n```\n:::\n\n\n## Initialize\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  initialize = function(input_size) {\n    self$hidden1 <- nn_linear(in_features = input_size, \n                              out_features = 10)\n    self$hidden2 <- nn_linear(in_features = 10, \n                              out_features = 5)\n    self$output <- nn_linear(in_features = 5, \n                             out_features = 3)\n    \n    self$drop1 <- nn_dropout(p = 0.4)\n    self$drop2 <- nn_dropout(p = 0.3)\n    \n    self$activation <- nn_relu()\n  }\n```\n:::\n\n\n## Forward\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforward = function(x) {\n    x |>  \n      self$hidden1() |>  \n      self$activation() |>  \n      self$drop1() |>  \n      \n      self$hidden2() |>  \n      self$activation() |>  \n      self$drop2() |>  \n      \n      self$output()\n  }\n```\n:::\n\n\n\n\n:::\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelnn2 <- modelnn2 |> \n  setup(loss = nn_cross_entropy_loss(),\n        optimizer = optim_rmsprop, \n        metrics = list(luz_metric_accuracy())) |>\n  set_hparams(input_size = ncol(Xtraining))\n```\n:::\n\n\n## Model Fitting\n\n::: {.panel-tabset}\n\n## Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted2 <- modelnn2 |> \n  fit(data = list(Xtraining, Ytraining), \n      epochs = 100)\n```\n:::\n\n\n## Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fitted2)\n```\n\n::: {.cell-output-display}\n![](14a_files/figure-revealjs/unnamed-chunk-25-1.png){width=960}\n:::\n:::\n\n\n:::\n\n\n\n## Testing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- fitted2 |>  \n  predict(Xtesting) |>  \n  torch_argmax(dim = 2) |>   \n  as_array() \n\nmean(as.numeric(Ytesting)==res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 1\n```\n\n\n:::\n:::\n",
    "supporting": [
      "14a_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}