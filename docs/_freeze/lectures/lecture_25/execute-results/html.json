{
  "hash": "bc2c70a5e6fa5ecde62f289be480bd33",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Tree-Based Methods\"\nformat:\n  revealjs:\n    scrollable: true\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: false \n    chalkboard:\n      src: chalkboard.json\n      storage: chalkboard_pres\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: true\n    message: false\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - reveal-header\n  - code-fullscreen\n\neditor: visual\n---\n\n\n## Learning Outcomes\n\n-   Trees\n\n-   Pruning\n\n-   Classification Trees\n\n-   Regression Trees\n\n-   R Examples\n\n# Trees\n\n## Trees\n\nA Statistical tree will partition a region from a set of predictor variables that will predict an outcome of interest.\n\n::: fragment\nTrees will split a region based on a predictors ability to reduce the overall mean squared error.\n:::\n\n::: fragment\nTrees are sometimes preferred to linear models due to the visual explanation of the model.\n:::\n\n## Trees\n\n![](https://www.mathworks.com/help/stats/simpleregressiontree.png){fig-align=\"center\"}\n\n## Fitting a Tree\n\n1.  Start with the entire dataset and define the maximum number of regions or number of observations per region of the tree.\n2.  Calculate the MSE of the dataset.\n3.  For each potential split, calculate the MSE. Choose the split that results in the lowest overall MSE.\n4.  Create a node in the tree with the selected split as the split criterion.\n5.  Repeat steps 2-4 for each subset, stopping if the maximum number of regions has been reached or if the subset size is too small.\n\n# Pruning\n\n## Pruning\n\nPruning is the process that will remove branches from a regression tree in order to prevent overfitting.\n\n::: fragment\nThis will result in a subtree that has high predictive power with no overfitting.\n:::\n\n::: fragment\nDue to the computational burden of pruning, it is recommended to implement *Cost Complexity Pruning.*\n:::\n\n## Cost Complexity Pruning\n\nLet $\\alpha$ be nonnegative tuning parameter that indexes a sequence of trees. Identify the tree that reduces:\n\n$$\n\\sum^{|T|}_{m=1}\\sum_{i:\\ x_i \\in R_m}(y_i-\\hat y_{R_m})^2 +\\alpha|T|\n$$\n\n-   $|T|$: Number of terminal nodes\n\n-   $R_m$: rectangular region containing data\n\n-   $y_i$: observed value\n\n-   $\\hat y_{R_m}$: predicted value in rectangular region.\n\n## Pruning Algorithm\n\n1.  Conduct a fitting algorithm to find the largest tree from the training data. Stop once every region has a small number of observations.\n2.  Apply the cost complexity pruning algorithm to identify the best subset of trees.\n3.  Use a K-fold cross-validation approach to choose the proper $\\alpha$. For each kth fold:\n    1.  Repeat steps 1 and 2.\n    2.  Evaluate the mean squared prediction error as a function of $\\alpha$.\n4.  Average the results for each value of $\\alpha$. Pick the $\\alpha$ that minimizes the error.\n5.  Return the subtree with the selected $\\alpha$ from step 2\n\n# Classification Trees\n\n## Classification Trees\n\nClassification Trees will construct a tree that will classify data based on the region (leaf) you land. The class majority is what is classified.\n\n## Criterion: Gini Index\n\nThe Gini Index is used to determine the error rate in classification trees:\n\n$$\nG = \\sum^K_{k=1} \\hat p_{mk}(1-\\hat p_{mk})\n$$\n\n# Regression Trees\n\n## Regression Trees\n\nRegression trees will construct a tree and predict the value of the outcome based on the average value of the region (leaf).\n\nTrees are constructed by minimizing the residual sums of square.\n\n\n# R Code\n\n## Regression Trees\n\n::: panel-tabset\n\n### Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tree)\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(magrittr)\ndf <- penguins %>% drop_na()\ntrain <- sample(1:nrow(df), nrow(df)/2)\ntree_penguin <- penguins %$% tree(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm, subset = train)\n```\n:::\n\n\n### Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(tree_penguin)\ntext(tree_penguin, pretty = 0)\n```\n\n::: {.cell-output-display}\n![](lecture_25_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n:::\n\n## Classification Trees\n\n::: panel-tabset\n\n### Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tree)\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(magrittr)\ndf <- penguins %>% drop_na()\ntrain <- sample(1:nrow(df), nrow(df)/2)\ntree_penguin_class <- df %$% tree(species ~ body_mass_g + flipper_length_mm + bill_length_mm + bill_depth_mm, subset = train)\n```\n:::\n\n\n### Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(tree_penguin_class)\ntext(tree_penguin_class, pretty = 0)\n```\n\n::: {.cell-output-display}\n![](lecture_25_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n:::\n\n## Pruning\n\n::: panel-tabset\n\n### Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nattach(df)\ntree_penguin_cv <- cv.tree(tree_penguin)\n```\n:::\n\n\n### Summary\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_penguin_cv\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> $size\n#> [1] 8 7 6 5 4 3 2 1\n#> \n#> $dev\n#> [1] 29082655 29153810 29153810 29392818 29382033 36684481 35584478 35584478\n#> \n#> $k\n#> [1]     -Inf  1120613  1141260  2532512  2599318  5547556  9660309 67364500\n#> \n#> $method\n#> [1] \"deviance\"\n#> \n#> attr(,\"class\")\n#> [1] \"prune\"         \"tree.sequence\"\n```\n\n\n:::\n:::\n\n\n### Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(tree_penguin_cv$size,\n     tree_penguin_cv$dev, type = \"b\")\n```\n\n::: {.cell-output-display}\n![](lecture_25_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n### Pruning\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprune_best <- prune.tree(tree_penguin, best = 7)\nplot(prune_best)\ntext(prune_best, pretty = 0)\n```\n\n::: {.cell-output-display}\n![](lecture_25_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n\n:::\n\n",
    "supporting": [
      "lecture_25_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}