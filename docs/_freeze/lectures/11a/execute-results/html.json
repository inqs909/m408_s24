{
  "hash": "62cf519bdefaf7f6e827e9d52be3b8a9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Classification\"\nformat:\n  revealjs:\n    scrollable: true\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: false \n    chalkboard:\n      src: chalkboard.json\n      storage: chalkboard_pres\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: false\n    eval: true\n    message: false\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - reveal-header\n  - code-fullscreen\n\neditor: source\n---\n\n\n## Learning Outcomes\n\n-   K-Nearest Neighbors\n\n-   Bayes' Classifier\n\n-   Linear and Quadratic Discriminant Analysis\n\n-   Naive Bayes\n\n## Classification\n\nThe practice of classifying data points into different categories.\n\n# K-Nearest Neighbors\n\n## K-Nearest Neighbors\n\nK-Nearest Neighbors will assign a category to a new data point based on the majority category of its **K** nearest neighbors.\n\n## KNN\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](11a_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n## Distances\n\nThe distance between a given point and the training data must be computed. These are the most commonly used distances:\n\n-   Manhattan\n\n-   Euclidean\n\n-   Minkowski\n\n## Manhattan Distance\n\n$$\nd(x,y) = \\sum_{i=1}^{p} |x_i - y_i|\n$$\n\n-   $x$: vector values of individual point\n\n-   $y$: vector values of new point\n\n-   $p$: length of vector\n\n## Euclidean Distance\n\n$$\nd(x,y) = \\sqrt{\\sum_{i=1}^{p} (x_i - y_i)^2}\n$$\n\n## Minkowski Distance\n\n$$\nd(x,y) = \\left( \\sum_{i=1}^{p} |x_i - y_i|^w \\right)^{\\frac{1}{w}}\n$$\n\n## Algorithm\n\nGiven a training data set, conduct the following steps:\n\n-   Compute the distance between a new data point and every point in the training data set.\n\n-   Choose the $K$ nearest training data points to the new point using the smallest distance.\n\n-   Categorize the new data point based on the majority of category from the $K$ nearest training data points.\n\n# Bayes Classifier\n\n## Bayes Classifier\n\nBayes Classifier is used to classify a data point to a category $c$\n\n$$\nf(\\boldsymbol x) = argmax_{c \\in C} f(C|\\boldsymbol X)\n$$\n\n## Probability\n\n$$\nf(C = c|\\boldsymbol X = x) = \\frac{f(\\boldsymbol X | C)\\pi_c}{f(\\boldsymbol X)}\n$$\n\n-   $f(\\boldsymbol X| C)$: conditional distribution of $\\boldsymbol X$\n\n-   $\\pi_c$: probability of observing category $C$\n\n-   $f(\\boldsymbol X)$: marginal distribution of $\\boldsymbol X$\n\n## Distribution of $f(\\boldsymbol X|C)$ and $f(\\boldsymbol X)$\n\nTo apply Bayes classifier, we must specify the form of $f(\\boldsymbol X| C)$ and $f(\\boldsymbol X)$. Common distributions are:\n\n-   Normal\n\n-   Bernoulli\n\n-   Multinomial\n\n# Linear Discriminant Analysis\n\n## LDA\n\nLinear Discriminant Analysis is used to classify a new data point, from a set of classifications, given information from a set of predictors.\n\nLDA classifies data using a Bayes classifier and imposing a normal distribution to the model.\n\n## LDA (p=1)\n\n$$\nf_k(\\boldsymbol X) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{\\frac{(x-\\mu_k)^2}{2\\sigma^2}\\right\\}\n$$\n\n$$\nf(X) = \\sum^K_{l=1} \\pi_l f_l(X)\n$$\n\n## LDA (p=1)\n\n$$\n\\delta_k = f_k(c_k|\\boldsymbol X ) = \\frac{f_k(\\boldsymbol X)\\pi_c}{f(\\boldsymbol X)}\n$$\n\n$$\n\\delta_k(x) = x\\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{\\sigma^2} + \\ln(\\pi_k) \n$$\n\n## LDA (p=1) Estimates\n\nLet $Y_i=c_l$, $l=1\\ldots, K$, and $X_i=x_i$ bet the data from n observations:\n\n$$\n\\hat\\mu_k = \\frac{1}{n_k}\\sum^n_{i=1(Y_i=c_k)} x_i\n$$\n\n$$\n\\hat\\sigma^2=\\frac{1}{n-K}\\sum^K_{l=1}\\sum_{i=1(Y_i=c_l)}^n(x_i-\\hat\\mu_l)^2 \n$$\n\n-   $n_k$: number of observations in class $k$\n\n## LDA (p\\>1)\n\n$$\nf_k(\\boldsymbol X) = \\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}\\exp\\left\\{(\\boldsymbol x-\\boldsymbol{\\mu_k})^{\\mathrm T}\\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu_k)\\right\\}\n$$\n\n$$\nf(\\boldsymbol X) = \\sum^K_{l=1} \\pi_l f_l(\\boldsymbol X)\n$$\n\n## LDA (p\\>1)\n\n$$\n\\delta_k(\\boldsymbol x) = \\boldsymbol x^{\\mathrm T}\\Sigma^{-1}\\boldsymbol \\mu_k-\\frac{1}{2}\\boldsymbol \\mu_k^{\\mathrm T}\\Sigma^{-1}\\boldsymbol \\mu_k + \\ln(\\pi_k) \n$$\n\n## LDA Classification\n\nClassify each new data point as class $c_k$ based on the largest $\\delta_k(\\boldsymbol X)$.\n\n# Quadratic Discriminant Analysis\n\n## QDA\n\nIn LDA, it is assumed that $\\Sigma$ from $\\boldsymbol X$ is the same for all classification groups. In Quadratic Discriminant Analysis, this assumption is relaxed, resulting in $\\Sigma_k$ for each classification.\n\n## QDA\n\n$$\nf_k(\\boldsymbol X) = \\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}\\exp\\left\\{(\\boldsymbol x-\\boldsymbol{\\mu_k})^{\\mathrm T}\\Sigma_k^{-1}(\\boldsymbol x-\\boldsymbol \\mu_k)\\right\\}\n$$\n\n## QDA\n\n$$\n\\delta_k(\\boldsymbol x) = -\\frac{1}{2}\\boldsymbol x^{\\mathrm T}\\Sigma_k^{-1}\\boldsymbol x + \\boldsymbol x^{\\mathrm T}\\Sigma_k^{-1}\\boldsymbol \\mu_k-\\frac{1}{2}\\boldsymbol \\mu_k^{\\mathrm T}\\Sigma_k^{-1}\\boldsymbol \\mu_k - \\frac{1}{2}\\ln|\\Sigma_k| + \\ln(\\pi_k) \n$$\n\n# Naive Bayes\n\n## Naive Bayes\n\nA Naive Bayes classifier, assumes the predictors in $\\boldsymbol X$ are independent of each other.\n\n## Naive Bayes\n\n$$\nf_k(\\boldsymbol X) = \\prod^p_{j} f_{jk}(x_j|c_k)\n$$\n\n## Naive Bayes\n\n::: columns\n::: {.column width=\"50%\"}\n### Quantitative\n\n-   Normal: $N(\\mu_{jk}, \\sigma^2_{jk})$\n\n-   Nonparametric\n\n    -   Kernel Density\n:::\n\n::: {.column width=\"50%\"}\n### Qualitative\n\n-   Nonparametric\n:::\n:::\n\n# R Code\n\n## LDA\n\n\n::: {.cell}\n\n:::\n\n\n## LDA Prediction\n\n\n::: {.cell}\n\n:::\n\n\n## LDA Confusion Matrix\n\n\n::: {.cell}\n\n:::\n\n\n## QDA\n\n\n::: {.cell}\n\n:::\n\n\n## QDA Prediction\n\n\n::: {.cell}\n\n:::\n\n\n## QDA Confusion Matrix\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n#>            \n#>             Adelie Chinstrap Gentoo\n#>   Adelie       144         2      0\n#>   Chinstrap      2        66      0\n#>   Gentoo         0         0    119\n```\n\n\n:::\n:::\n\n\n## Naive Bayes\n\n\n::: {.cell}\n\n:::\n\n\n## Naive Bayes Prediction\n\n\n::: {.cell}\n\n:::\n\n\n## Naive Bayes Confusion Matrix\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n#>            x_nb_predict\n#>             Adelie Chinstrap Gentoo\n#>   Adelie       141         5      0\n#>   Chinstrap      5        63      0\n#>   Gentoo         0         0    119\n```\n\n\n:::\n:::\n\n\n# Example\n\n## Create the KNN algorithm in R\n\n-   Manhattan\n\n-   Euclidean\n\n-   Minkowski ($w=5$)\n\nUse the `penguins` data set from `palmerpenguins` and categorize the following data: `bill_depth = 19`, `bill_length = 40`, `flipper_length = 185`, and `body_mass = 3345`.\n",
    "supporting": [
      "11a_files/figure-revealjs"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}