{
  "hash": "8f9e4ff1ac3d3cb81561a7ab0ab679aa",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Support Vector Machines\"\nsubtitle: \"Maximal Margin and Support Vector Classifiers\"\nformat:\n  revealjs:\n    scrollable: true\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: false \n    chalkboard:\n      src: chalkboard.json\n      storage: chalkboard_pres\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: true\n    message: false\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - reveal-header\n  - code-fullscreen\n\neditor: visual\n---\n\n\n## Learning Outcomes\n\n-   Maximal Margin Classifier\n-   Support Vector Classifier\n-   R Code\n\n# Maximal Margin Classifier\n\n## Motivating Example\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](13b_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n## Maximal Margin Classifier\n\nThe **Maximal Margin Classifier** will impose a hyperplane on a graph that will classify the data given a vector of predictor variables.\n\n## Hyperplane\n\nGiven a p-dimensional space, a hyperplane is flat affine subspace of p-1 dimensions. It is mathematically defined as:\n\n$$\n\\beta_0 + \\beta_1X_1 + \\beta_2 X_2 + \\cdots+\\beta_pX_p = 0\n$$\n\n## Hyperplane\n\n![](https://images.deepai.org/glossary-terms/3bb86574825445cba73a67222b744648/hyperplane.png){fig-align=\"center\"}\n\n## Constructing the Hyperplane\n\nA hyperplane is constructed by maximizing the margin $M$ of the data points that are farthest from the theoretical margin. The data points that define the outer edge of the margins are known as support vectors.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](13b_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n## Optimization Problem\n\n-   $\\overset{\\mathrm{maximize}}{\\tiny\\beta_0, \\beta_1,\\ldots,\\beta_p,M}\\ \\large M$\n\n-   subject to $\\sum^p_{j=1}\\beta_j^2 = 1$\n\n-   $y_i(\\beta_0 + \\beta_1X_{1i} + \\beta_2 X_{2i} + \\cdots+\\beta_pX_{pi})\\geq M \\ \\forall \\ i=1,\\ldots,n$\n\n## Maximal Margin Classifier\n\n![](https://bookdown.org/mpfoley1973/data-sci/images/svm_mmc.png){fig-align=\"center\"}\n\n# Support Vector Classifier\n\n## Support Vector Classifier\n\nMaximal Margin Classifiers have one fatal defect, the data points must be completely on one side of the margin. This does not allow room for error.\n\n::: fragment\nA Support Vector Classifier allows for data points to be misclassified if need be.\n:::\n\n::: fragment\nIt achieves this by implementing a Cost mechanism, denoted as $C$, to account for any errors for data points.\n:::\n\n## Support Vector Classifiers\n\n![](https://www.surveypractice.org/article/2715-using-support-vector-machines-for-survey-research/attachment/9153.png){fig-align=\"center\"}\n\n## Optimization Problem\n\n-   $\\overset{\\mathrm{maximize}}{\\tiny\\beta_0, \\beta_1,\\ldots,\\beta_p,\\epsilon_1,\\ldots, \\epsilon_n,M}\\ \\large M$\n\n-   subject to $\\sum^p_{j=1}\\beta_j^2 = 1$\n\n-   $y_i(\\beta_0 + \\beta_1X_{1i} + \\beta_2 X_{2i} + \\cdots+\\beta_pX_{pi})\\geq M (1-\\epsilon_i) \\ \\forall \\ i=1,\\ldots,n$\n\n-   $\\epsilon_i\\geq 0$\n\n-   $\\sum^n_{i=1} \\epsilon_i \\leq C$\n\n## Budget C\n\nThe tuning parameter $C$ is known as the budget parameter for error. When the data point is on the correct side of the margin, then it has an error of $0$. When a data point in on the wrong side the margin, it has a bit of error. When the data point is on the opposite side of the hyperplane, then it has an error greater than $1$. This is allowed as long as the sum of errors are less than or equal to $C$.\n\n# R Code\n\n## Support Vector Classifier\n\n::: panel-tabset\n### Simulation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- matrix ( rnorm (20 * 2) , ncol = 2)\ny <- c( rep (-1, 10) , rep (1, 10) )\nx[y == 1, ] <- x[y == 1, ] + 1\nplot (x, col = (3 - y))\n```\n\n::: {.cell-output-display}\n![](13b_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n### R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- data.frame (x = x, y = as.factor(y))\nlibrary(e1071)\nsvmfit <- svm (y ~ ., data = dat , kernel = \"linear\",\ncost = 10, scale = FALSE )\nplot(svmfit, dat)\n```\n\n::: {.cell-output-display}\n![](13b_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n### Support Vectors\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsvmfit$index\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  [1]  3  5  6  7  8  9 10 11 12 13 14 18 19\n```\n\n\n:::\n:::\n\n\n### Summary\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(svmfit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> svm(formula = y ~ ., data = dat, kernel = \"linear\", cost = 10, scale = FALSE)\n#> \n#> \n#> Parameters:\n#>    SVM-Type:  C-classification \n#>  SVM-Kernel:  linear \n#>        cost:  10 \n#> \n#> Number of Support Vectors:  13\n#> \n#>  ( 7 6 )\n#> \n#> \n#> Number of Classes:  2 \n#> \n#> Levels: \n#>  -1 1\n```\n\n\n:::\n:::\n\n:::\n\n## Cross-Validation Approach for C\n\n::: panel-tabset\n### R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2434)\ntune.out <- tune(svm, y ~ ., data = dat, kernel = \"linear\",\n                 ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))\n```\n:::\n\n\n### Summary\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(tune.out)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Parameter tuning of 'svm':\n#> \n#> - sampling method: 10-fold cross validation \n#> \n#> - best parameters:\n#>  cost\n#>    10\n#> \n#> - best performance: 0.25 \n#> \n#> - Detailed performance results:\n#>    cost error dispersion\n#> 1 1e-03  0.55  0.4377975\n#> 2 1e-02  0.55  0.4377975\n#> 3 1e-01  0.30  0.3496029\n#> 4 1e+00  0.35  0.3374743\n#> 5 5e+00  0.30  0.4216370\n#> 6 1e+01  0.25  0.3535534\n#> 7 1e+02  0.25  0.3535534\n```\n\n\n:::\n:::\n\n\n### Best Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbestmod <- tune.out$best.model\nsummary(bestmod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> best.tune(METHOD = svm, train.x = y ~ ., data = dat, ranges = list(cost = c(0.001, \n#>     0.01, 0.1, 1, 5, 10, 100)), kernel = \"linear\")\n#> \n#> \n#> Parameters:\n#>    SVM-Type:  C-classification \n#>  SVM-Kernel:  linear \n#>        cost:  10 \n#> \n#> Number of Support Vectors:  13\n#> \n#>  ( 7 6 )\n#> \n#> \n#> Number of Classes:  2 \n#> \n#> Levels: \n#>  -1 1\n```\n\n\n:::\n:::\n\n:::\n\n## Prediction\n\n::: panel-tabset\n### Set Up\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxtest <- matrix(rnorm(20 * 2), ncol = 2)\nytest <- sample(c(-1, 1), 20, rep = TRUE)\nxtest[ytest == 1,] <- xtest[ytest == 1, ] + 1\ntestdat <- data.frame (x = xtest , y = as.factor(ytest))\n```\n:::\n\n\n### Prediction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nypred <- predict(bestmod, testdat)\ntable(pred = ypred, truth = testdat$y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>     truth\n#> pred -1  1\n#>   -1  9  0\n#>   1   1 10\n```\n\n\n:::\n:::\n\n:::\n",
    "supporting": [
      "13b_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}