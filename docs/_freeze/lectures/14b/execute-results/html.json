{
  "hash": "89bc46c3d83fd4d2deadd5874d30f0af",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: |\n  Convolutional \\\n  Neural Networks\nsubtitle: \"Image Classification\"\nformat:\n  revealjs:\n    scrollable: true\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: false \n    chalkboard:\n      src: chalkboard.json\n      storage: chalkboard_pres\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: true\n    message: false\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - reveal-header\n  - code-fullscreen\n  - reveal-auto-agenda\n\neditor: source\n---\n\n\n# Convolutional Neural Networks\n\n## Convolutional Neural Networks\n\nConvolutional Neural Networks were developed in terms of image analysis.\n\n::: fragment\nThe idea is to mimic how a human minds will classify an image.\n:::\n\n::: fragment\nA convolutional neural networks is trained by using a set of images that have been previously classfied.\n:::\n\n::: fragment\nOnce the network is trained, we can give new types of images to be classified.\n:::\n\n## Convolutiona Neural Networks\n\n![](img/squirrel.jpg){fig-align=\"center\"}\n\n## Convolutional Neural Networks\n\nA CNN will identify certain features, arrange them, and match them to what is closely is known.\n\n## CNN\n\n![Credit: ISLR2](img/islr2/Chapter10/10_6.jpg){fig-align=\"center\"}\n\n# Layers\n\n## Convolution Filter\n\nA Convolution Filter will highlight certain features of an image.\n\n::: fragment\nThe matching features will contain a large value.\n:::\n\n::: fragment\nDismatching features will contain a smaller value.\n:::\n\n## Convolution Filter\n\n::: panel-tabset\n## Image\n\n$$\n\\left(\n\\begin{array}{ccc}\na & b & c \\\\\nd & e & f \\\\\ng & h & i \\\\\nj & k & l \n\\end{array}\n\\right)\n$$\n\n## Filter\n\n$$\n\\left(\n\\begin{array}{cc}\n\\alpha & \\beta \\\\\n\\gamma & \\delta\n\\end{array}\n\\right)\n$$\n\n## Both\n\n$$\n\\left(\n\\begin{array}{ccc}\na & b & c \\\\\nd & e & f \\\\\ng & h & i \\\\\nj & k & l \n\\end{array}\n\\right)\n*\n\\left(\n\\begin{array}{cc}\n\\alpha & \\beta \\\\\n\\gamma & \\delta\n\\end{array}\n\\right)\n$$\n\n## Results\n\n$$\n\\left(\n\\begin{array}{cc}\na\\alpha + b\\beta + d\\gamma + w\\delta & b\\alpha + c\\beta + e\\gamma + f\\delta \\\\\nd\\alpha + e\\beta + g\\gamma + h\\delta & e\\alpha + f\\beta + h\\gamma + i\\delta \\\\\ng\\alpha + h\\beta + j\\gamma + k\\delta & h\\alpha + i\\beta + k\\gamma + l\\delta\n\\end{array}\n\\right)\n$$\n:::\n\n## Convolutional Layers\n\nConvolution layers are a set of filters in a hidden layers. We can have $K$ layers that an image is passed through.\n\n## Pooling Layers\n\nThe act of summarizing a large matrix to a smaller matrix.\n\n## Max Pool\n\n$$\n\\left[\n\\begin{array}{cccc}\n1 & 3 & 9 & 5 \\\\\n6 & 2 & 3 & 4 \\\\\n1 & 0 & 6 & 4 \\\\\n8 & 4 & 2 & 7\n\\end{array}\n\\right] \\rightarrow\n\\left[\n\\begin{array}{cc}\n6 & 9 \\\\\n8 & 7\n\\end{array}\n\\right]\n$$\n\n# Architecture\n\n## Data Image\n\nFor each image, there are 3 channels (RGB) that represent the image.\n\n::: fragment\nAfterwards, the image is gridded up into pixels with each containing a 3-values (RGB).\n:::\n\n## Data Image\n\n![](https://insightimi.wordpress.com/wp-content/uploads/2021/02/1_r2wi31-arzxihybbjd7luq.png?w=1024){fig-align=\"center\"}\n\n## Convolve Image\n\nFor each RGB channel, apply a set of convolution filters.\n\n::: fragment\nThen pool the filters.\n:::\n\n::: fragment\nRepeat for next hidden layer.\n:::\n\n## Flattening\n\nOnce the images has been pooled to a select pixels or features. The images are flattened to a set of inputs.\n\n::: fragment\nThese inputs are used to a traditional neural network to classify an image.\n:::\n\n## Architecture\n\n![](https://www.embedded.com/wp-content/uploads/2023/06/23026adi-cnn1_f03_thumb.jpg){fig-align=\"center\"}\n\n## Training\n\nThe CNN is trained by supplying a set of pre-classified images.\n\n::: fragment\nThe parameters in the convolution filters are estimates using standard techniques.\n:::\n\n## Data Augmentation\n\n::: columns\n::: {.column width=\"50%\"}\n![](img/squirrel.jpg)\n:::\n\n::: {.column width=\"50%\"}\n![](img/squirrel_mirror.jpg)\n:::\n:::\n\n# R MNIST Code\n\n## MNIST\n\nThis is a database of handwritten digits.\n\nWe will use to construct neural networks that will classify images.\n\n## Torch Packages in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\nlibrary(luz) # high-level interface for torch\nlibrary(torchvision) # for datasets and image transformation\nlibrary(torchdatasets) # for datasets we are going to use\nlibrary(zeallot)\ntorch_manual_seed(13)\n```\n:::\n\n\n## MNIST\n\n\n::: {.cell}\n\n```{.r .cell-code}\n###\ntrain_ds <- mnist_dataset(root = \".\", train = TRUE, download = TRUE)\ntest_ds <- mnist_dataset(root = \".\", train = FALSE, download = TRUE)\n\ntrain_ds[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> $x\n#>       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n#>  [1,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n#>  [2,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n#>  [3,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n#>  [4,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n#>  [5,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n#>  [6,]    0    0    0    0    0    0    0    0    0     0     0     0     3\n#>  [7,]    0    0    0    0    0    0    0    0   30    36    94   154   170\n#>  [8,]    0    0    0    0    0    0    0   49  238   253   253   253   253\n#>  [9,]    0    0    0    0    0    0    0   18  219   253   253   253   253\n#> [10,]    0    0    0    0    0    0    0    0   80   156   107   253   253\n#> [11,]    0    0    0    0    0    0    0    0    0    14     1   154   253\n#> [12,]    0    0    0    0    0    0    0    0    0     0     0   139   253\n#> [13,]    0    0    0    0    0    0    0    0    0     0     0    11   190\n#> [14,]    0    0    0    0    0    0    0    0    0     0     0     0    35\n#> [15,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n#> [16,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n#> [17,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n#> [18,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n#> [19,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n#> [20,]    0    0    0    0    0    0    0    0    0     0     0     0    39\n#> [21,]    0    0    0    0    0    0    0    0    0     0    24   114   221\n#> [22,]    0    0    0    0    0    0    0    0   23    66   213   253   253\n#> [23,]    0    0    0    0    0    0   18  171  219   253   253   253   253\n#> [24,]    0    0    0    0   55  172  226  253  253   253   253   244   133\n#> [25,]    0    0    0    0  136  253  253  253  212   135   132    16     0\n#> [26,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n#> [27,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n#> [28,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n#>       [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25]\n#>  [1,]     0     0     0     0     0     0     0     0     0     0     0     0\n#>  [2,]     0     0     0     0     0     0     0     0     0     0     0     0\n#>  [3,]     0     0     0     0     0     0     0     0     0     0     0     0\n#>  [4,]     0     0     0     0     0     0     0     0     0     0     0     0\n#>  [5,]     0     0     0     0     0     0     0     0     0     0     0     0\n#>  [6,]    18    18    18   126   136   175    26   166   255   247   127     0\n#>  [7,]   253   253   253   253   253   225   172   253   242   195    64     0\n#>  [8,]   253   253   253   253   251    93    82    82    56    39     0     0\n#>  [9,]   253   198   182   247   241     0     0     0     0     0     0     0\n#> [10,]   205    11     0    43   154     0     0     0     0     0     0     0\n#> [11,]    90     0     0     0     0     0     0     0     0     0     0     0\n#> [12,]   190     2     0     0     0     0     0     0     0     0     0     0\n#> [13,]   253    70     0     0     0     0     0     0     0     0     0     0\n#> [14,]   241   225   160   108     1     0     0     0     0     0     0     0\n#> [15,]    81   240   253   253   119    25     0     0     0     0     0     0\n#> [16,]     0    45   186   253   253   150    27     0     0     0     0     0\n#> [17,]     0     0    16    93   252   253   187     0     0     0     0     0\n#> [18,]     0     0     0     0   249   253   249    64     0     0     0     0\n#> [19,]     0    46   130   183   253   253   207     2     0     0     0     0\n#> [20,]   148   229   253   253   253   250   182     0     0     0     0     0\n#> [21,]   253   253   253   253   201    78     0     0     0     0     0     0\n#> [22,]   253   253   198    81     2     0     0     0     0     0     0     0\n#> [23,]   195    80     9     0     0     0     0     0     0     0     0     0\n#> [24,]    11     0     0     0     0     0     0     0     0     0     0     0\n#> [25,]     0     0     0     0     0     0     0     0     0     0     0     0\n#> [26,]     0     0     0     0     0     0     0     0     0     0     0     0\n#> [27,]     0     0     0     0     0     0     0     0     0     0     0     0\n#> [28,]     0     0     0     0     0     0     0     0     0     0     0     0\n#>       [,26] [,27] [,28]\n#>  [1,]     0     0     0\n#>  [2,]     0     0     0\n#>  [3,]     0     0     0\n#>  [4,]     0     0     0\n#>  [5,]     0     0     0\n#>  [6,]     0     0     0\n#>  [7,]     0     0     0\n#>  [8,]     0     0     0\n#>  [9,]     0     0     0\n#> [10,]     0     0     0\n#> [11,]     0     0     0\n#> [12,]     0     0     0\n#> [13,]     0     0     0\n#> [14,]     0     0     0\n#> [15,]     0     0     0\n#> [16,]     0     0     0\n#> [17,]     0     0     0\n#> [18,]     0     0     0\n#> [19,]     0     0     0\n#> [20,]     0     0     0\n#> [21,]     0     0     0\n#> [22,]     0     0     0\n#> [23,]     0     0     0\n#> [24,]     0     0     0\n#> [25,]     0     0     0\n#> [26,]     0     0     0\n#> [27,]     0     0     0\n#> [28,]     0     0     0\n#> \n#> $y\n#> [1] 6\n```\n\n\n:::\n\n```{.r .cell-code}\n# test_ds[2]\n```\n:::\n\n\n## Transforming Data\n\nIn order to use torch, you must transform the data: - tensor - flatten - tensor divided by the potential values (255)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n###\ntransform <- function(x) {\n  x |>\n    torch_tensor()  |>\n    torch_flatten() |>\n    torch_div(255)\n}\ntrain_ds <- mnist_dataset(\n  root = \".\",\n  train = TRUE,\n  download = TRUE,\n  transform = transform\n)\ntest_ds <- mnist_dataset(\n  root = \".\",\n  train = FALSE,\n  download = TRUE,\n  transform = transform\n)\n```\n:::\n\n\n## Neural Network Model Set Up\n\nThe `nn_module` will begin to setup the neural network. It requires the `initialize` and `forward` functions.\n\n::: fragment\n`initialize` is a function that describes the elements of the neural network, the layers.\n:::\n\n::: fragment\n`nn_linear` will construct a linear framework for the number of inputs, and the number of outputs in the neural network.\n:::\n\n::: fragment\n`nn_dropout` will randomly \"zero\" an input elements of a tensor with probability `p`.\n:::\n\n::: fragment\n`nn_relu` specifies the linear unit function\n:::\n\n::: fragment\n`forward` describes how the neural network is formatted using the values from the `initialize` function.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n###\nmodelnn <- nn_module(\n  initialize = function() {\n    self$linear1 <- nn_linear(in_features = 28*28, out_features = 256)\n    self$linear2 <- nn_linear(in_features = 256, out_features = 128)\n    self$linear3 <- nn_linear(in_features = 128, out_features = 10)\n\n    self$drop1 <- nn_dropout(p = 0.4)\n    self$drop2 <- nn_dropout(p = 0.3)\n\n    self$activation <- nn_relu()\n  },\n  forward = function(x) {\n    x |>\n      self$linear1() |>\n      self$activation() |>\n      self$drop1() |>\n\n      self$linear2() |>\n      self$activation() |>\n      self$drop2() |>\n      self$linear3()\n  }\n)\n```\n:::\n\n\n## Set Up Neural Network\n\nTells `luz` (`torch`) how to execute the neural network.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelnn <- modelnn |>\n  setup(\n    loss = nn_cross_entropy_loss(),\n    optimizer = optim_rmsprop,\n    metrics = list(luz_metric_accuracy())\n  )\n```\n:::\n\n\n## Fit the Neural Network\n\n::: panel-tabset\n## Fitting\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem.time(\n   fitted <- modelnn |>\n      fit(\n        data = train_ds,\n        epochs = 5,\n        valid_data = 0.2,\n        dataloader_options = list(batch_size = 256),\n        verbose = FALSE\n      )\n )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>    user  system elapsed \n#> 144.086   0.572 143.670\n```\n\n\n:::\n:::\n\n\n## Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fitted)\n```\n\n::: {.cell-output-display}\n![](14b_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n:::\n\n## Test Efficiency of Neural Network\n\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy <- function(pred, truth) {\n   mean(pred == truth) }\n\n# gets the true classes from all observations in test_ds.\ntruth <- sapply(seq_along(test_ds), function(x) test_ds[x][[2]])\n\nfitted |>\n  predict(test_ds) |>\n  torch_argmax(dim = 2) |> # the predicted class is the one with higher 'logit'.\n  as_array() |>  # convert to an R object\n  accuracy(truth) # use function created\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.9539\n```\n\n\n:::\n:::\n\n\n# R Code CNN\n\n## CIFAR Data\n\nThe CIFAR database contains 60,000 images labeled with 20 superclasses with 5 animals for each superclass.\n\n## CIFAR Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntransform <- function(x) {\n  transform_to_tensor(x)\n}\n\ntrain_ds <- cifar100_dataset(\n  root = \"./\", \n  train = TRUE, \n  download = TRUE, \n  transform = transform\n)\n\ntest_ds <- cifar100_dataset(\n  root = \"./\", \n  train = FALSE, \n  transform = transform\n)\n\ntrain_ds[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> $x\n#> torch_tensor\n#> (1,.,.) = \n#>  Columns 1 to 9  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n#>   1.0000  0.9961  0.9961  0.9961  0.9961  0.9961  0.9961  0.9961  0.9961\n#>   1.0000  0.9961  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n#>   1.0000  0.9961  1.0000  1.0000  1.0000  1.0000  1.0000  0.9922  0.9882\n#>   1.0000  0.9961  1.0000  1.0000  1.0000  1.0000  1.0000  0.9922  0.9098\n#>   1.0000  0.9961  1.0000  1.0000  1.0000  1.0000  1.0000  0.9882  0.8353\n#>   1.0000  0.9961  1.0000  1.0000  1.0000  1.0000  1.0000  0.9961  0.8824\n#>   1.0000  0.9961  1.0000  1.0000  1.0000  1.0000  1.0000  0.9922  0.9765\n#>   1.0000  1.0000  0.9961  0.9961  1.0000  1.0000  1.0000  0.9961  0.9961\n#>   0.9922  1.0000  0.9882  0.9569  0.9804  0.9922  1.0000  0.9961  0.9804\n#>   0.9647  0.9961  0.9333  0.7020  0.7569  0.9490  1.0000  0.9843  0.8745\n#>   0.9686  0.9412  0.7255  0.5843  0.6118  0.8392  0.9922  0.9255  0.6863\n#>   0.9569  0.7098  0.5098  0.6471  0.5451  0.5255  0.8235  0.6863  0.5490\n#>   0.9059  0.4510  0.5255  0.6196  0.4235  0.4275  0.6118  0.4392  0.4431\n#>   0.9255  0.5098  0.4196  0.3451  0.2235  0.3216  0.5020  0.4314  0.3647\n#>   0.7098  0.4980  0.4314  0.2157  0.1098  0.2706  0.4902  0.3804  0.2980\n#>   0.5804  0.5137  0.5176  0.2196  0.1412  0.5373  0.6941  0.4784  0.4667\n#>   0.6824  0.6471  0.6275  0.4980  0.4235  0.5529  0.6706  0.6706  0.6392\n#>   0.4392  0.4235  0.5412  0.6863  0.5647  0.5725  0.6000  0.5922  0.5412\n#>   0.5765  0.5569  0.6431  0.6314  0.5137  0.5294  0.5529  0.4941  0.4745\n#>   0.7804  0.7333  0.5686  0.4980  0.6549  0.7059  0.7059  0.6196  0.6000\n#>   0.7216  0.7294  0.5137  0.3412  0.3569  0.4667  0.6471  0.7373  0.7725\n#>   0.7608  0.7569  0.7451  0.6510  0.5176  0.4314  0.5843  0.6784  0.6471\n#>   0.7686  0.7765  0.8118  0.8235  0.8118  0.7804  0.7725  0.6745  0.5608\n#>   0.6196  0.6784  0.7059  0.7608  0.8275  0.8118  0.8196  0.7608  0.6941\n#>   0.5569  0.5882  0.6157  0.6863  0.7098  0.7098  0.7490  0.7961  0.7922\n#>   0.6667  0.6510  0.6588  0.6510  0.6000  0.5451  0.6588  0.7098  0.6784\n#>   0.6431  0.6706  0.7137  0.7020  0.6118  0.5882  0.6471  0.6392  0.6706\n#>   0.6000  0.6275  0.6196  0.6314  0.6157  0.6471  0.6078  0.6275  0.6745\n#> ... [the output was truncated (use n=-1 to disable)]\n#> [ CPUFloatType{3,32,32} ]\n#> \n#> $y\n#> [1] 20\n```\n\n\n:::\n:::\n\n\n## Sample Image\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mar = c(0, 0, 0, 0), mfrow = c(2, 2))\nindex <- sample(seq(50000), 4)\nfor (i in index) plot(as.raster(as.array(train_ds[i][[1]]$permute(c(2,3,1)))))\n```\n\n::: {.cell-output-display}\n![](14b_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n## Defining CNN\n\n::: panel-tabset\n## Convulational Layers\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconv_block <- nn_module(\n  initialize = function(in_channels, out_channels) {\n    self$conv <- nn_conv2d(\n      in_channels = in_channels, \n      out_channels = out_channels, \n      kernel_size = c(3,3),\n      padding = \"same\"\n    )\n    self$relu <- nn_relu()\n    self$pool <- nn_max_pool2d(kernel_size = c(2,2))\n  },\n  forward = function(x) {\n    x |> \n      self$conv() |>  \n      self$relu() |> \n      self$pool()\n  }\n)\n```\n:::\n\n\n-   `in_channels`: Number of inputs planes (3 at the beginning)\n-   `out_channels`: Number of output planes (may vary)\n-   `kernel_size`: convolutional filter size\n-   `padding`: adds null values to images make the same\n-   `nn_relu`: Use ReLU\n-   `nn_max_pool2d`: Size Pooling Matrix\n\n## NN\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- nn_module(\n  initialize = function() {\n    self$conv <- nn_sequential(\n      conv_block(3, 32),\n      conv_block(32, 64),\n      conv_block(64, 128),\n      conv_block(128, 256)\n    )\n    self$output <- nn_sequential(\n      nn_dropout(0.5),\n      nn_linear(2*2*256, 512),\n      nn_relu(),\n      nn_linear(512, 100)\n    )\n  },\n  forward = function(x) {\n    x |> \n      self$conv() |> \n      torch_flatten(start_dim = 2) |>\n      self$output()\n  }\n)\n```\n:::\n\n\n-   `nn_sequential`: creates a sequence of functions\n-   `conv_block`: defined previously\n-   `Output`: Defines final neural network\n-   `Forward`: Defines overall neural network\n\n## Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> An `nn_module` containing 964,516 parameters.\n#> \n#> ── Modules ─────────────────────────────────────────────────────────────────────\n#> • conv: <nn_sequential> #388,416 parameters\n#> • output: <nn_sequential> #576,100 parameters\n```\n\n\n:::\n:::\n\n:::\n\n## Fitting CNN\n\n::: panel-tabset\n## Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem.time(\nfitted <- model |>  \n  setup(\n    loss = nn_cross_entropy_loss(),\n    optimizer = optim_rmsprop, \n    metrics = list(luz_metric_accuracy())\n  ) |> \n  set_opt_hparams(lr = 0.001) |> \n  fit(\n    train_ds,\n    epochs = 10, #30,\n    valid_data = 0.2,\n    dataloader_options = list(batch_size = 128)\n  )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>     user   system  elapsed \n#> 1398.430    6.421  902.224\n```\n\n\n:::\n:::\n\n\n## Summary\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(fitted)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> A `luz_module_fitted`\n#> ── Time ────────────────────────────────────────────────────────────────────────\n#> • Total time: 15m 2.2s\n#> • Avg time per training epoch: 1m 18.5s\n#> \n#> ── Results ─────────────────────────────────────────────────────────────────────\n#> Metrics observed in the last epoch.\n#> \n#> ℹ Training:\n#> loss: 2.4524\n#> acc: 0.357\n#> \n#> ── Model ───────────────────────────────────────────────────────────────────────\n#> An `nn_module` containing 964,516 parameters.\n#> \n#> ── Modules ─────────────────────────────────────────────────────────────────────\n#> • conv: <nn_sequential> #388,416 parameters\n#> • output: <nn_sequential> #576,100 parameters\n```\n\n\n:::\n:::\n\n\n## Evaluate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nevaluate(fitted, test_ds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> A `luz_module_evaluation`\n#> ── Results ─────────────────────────────────────────────────────────────────────\n#> loss: 2.5548\n#> acc: 0.3494\n```\n\n\n:::\n:::\n\n:::\n\n## Squirrel Image\n\nDownload labels json [here](https://github.com/MartinThoma/algorithms/blob/master/ML/confusion-matrix/labels/cifar-100-labels.json).\n\nDownload the squirrel image [here](https://m408.inqs.info/lectures/img/squirrel.jpg).\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [[1]]\n#> [1] \"keyboard\"\n#> \n#> [[2]]\n#> [1] \"worm\"\n#> \n#> [[3]]\n#> [1] \"clock\"\n#> \n#> [[4]]\n#> [1] \"bus\"\n#> \n#> [[5]]\n#> [1] \"lamp\"\n#> \n#> [[6]]\n#> [1] \"shark\"\n#> \n#> [[7]]\n#> [1] \"motorcycle\"\n#> \n#> [[8]]\n#> [1] \"couch\"\n#> \n#> [[9]]\n#> [1] \"telephone\"\n#> \n#> [[10]]\n#> [1] \"bowl\"\n#> \n#> [[11]]\n#> [1] \"sunflower\"\n#> \n#> [[12]]\n#> [1] \"pickup_truck\"\n#> \n#> [[13]]\n#> [1] \"sea\"\n#> \n#> [[14]]\n#> [1] \"whale\"\n#> \n#> [[15]]\n#> [1] \"plate\"\n#> \n#> [[16]]\n#> [1] \"streetcar\"\n#> \n#> [[17]]\n#> [1] \"cup\"\n#> \n#> [[18]]\n#> [1] \"television\"\n#> \n#> [[19]]\n#> [1] \"can\"\n#> \n#> [[20]]\n#> [1] \"poppy\"\n#> \n#> [[21]]\n#> [1] \"bottle\"\n#> \n#> [[22]]\n#> [1] \"sweet_pepper\"\n#> \n#> [[23]]\n#> [1] \"orchid\"\n#> \n#> [[24]]\n#> [1] \"bridge\"\n#> \n#> [[25]]\n#> [1] \"baby\"\n#> \n#> [[26]]\n#> [1] \"seal\"\n#> \n#> [[27]]\n#> [1] \"butterfly\"\n#> \n#> [[28]]\n#> [1] \"tulip\"\n#> \n#> [[29]]\n#> [1] \"turtle\"\n#> \n#> [[30]]\n#> [1] \"rocket\"\n#> \n#> [[31]]\n#> [1] \"table\"\n#> \n#> [[32]]\n#> [1] \"cloud\"\n#> \n#> [[33]]\n#> [1] \"mountain\"\n#> \n#> [[34]]\n#> [1] \"lawn_mower\"\n#> \n#> [[35]]\n#> [1] \"train\"\n#> \n#> [[36]]\n#> [1] \"ray\"\n#> \n#> [[37]]\n#> [1] \"aquarium_fish\"\n#> \n#> [[38]]\n#> [1] \"plain\"\n#> \n#> [[39]]\n#> [1] \"flatfish\"\n#> \n#> [[40]]\n#> [1] \"dolphin\"\n#> \n#> [[41]]\n#> [1] \"pine_tree\"\n#> \n#> [[42]]\n#> [1] \"snake\"\n#> \n#> [[43]]\n#> [1] \"skunk\"\n#> \n#> [[44]]\n#> [1] \"trout\"\n#> \n#> [[45]]\n#> [1] \"bed\"\n#> \n#> [[46]]\n#> [1] \"palm_tree\"\n#> \n#> [[47]]\n#> [1] \"skyscraper\"\n#> \n#> [[48]]\n#> [1] \"woman\"\n#> \n#> [[49]]\n#> [1] \"lobster\"\n#> \n#> [[50]]\n#> [1] \"caterpillar\"\n#> \n#> [[51]]\n#> [1] \"crab\"\n#> \n#> [[52]]\n#> [1] \"man\"\n#> \n#> [[53]]\n#> [1] \"chair\"\n#> \n#> [[54]]\n#> [1] \"boy\"\n#> \n#> [[55]]\n#> [1] \"beetle\"\n#> \n#> [[56]]\n#> [1] \"tank\"\n#> \n#> [[57]]\n#> [1] \"house\"\n#> \n#> [[58]]\n#> [1] \"pear\"\n#> \n#> [[59]]\n#> [1] \"forest\"\n#> \n#> [[60]]\n#> [1] \"bicycle\"\n#> \n#> [[61]]\n#> [1] \"maple_tree\"\n#> \n#> [[62]]\n#> [1] \"bear\"\n#> \n#> [[63]]\n#> [1] \"rose\"\n#> \n#> [[64]]\n#> [1] \"dinosaur\"\n#> \n#> [[65]]\n#> [1] \"spider\"\n#> \n#> [[66]]\n#> [1] \"cattle\"\n#> \n#> [[67]]\n#> [1] \"chimpanzee\"\n#> \n#> [[68]]\n#> [1] \"mouse\"\n#> \n#> [[69]]\n#> [1] \"tractor\"\n#> \n#> [[70]]\n#> [1] \"lizard\"\n#> \n#> [[71]]\n#> [1] \"otter\"\n#> \n#> [[72]]\n#> [1] \"cockroach\"\n#> \n#> [[73]]\n#> [1] \"raccoon\"\n#> \n#> [[74]]\n#> [1] \"rabbit\"\n#> \n#> [[75]]\n#> [1] \"mushroom\"\n#> \n#> [[76]]\n#> [1] \"orange\"\n#> \n#> [[77]]\n#> [1] \"bee\"\n#> \n#> [[78]]\n#> [1] \"shrew\"\n#> \n#> [[79]]\n#> [1] \"willow_tree\"\n#> \n#> [[80]]\n#> [1] \"wardrobe\"\n#> \n#> [[81]]\n#> [1] \"camel\"\n#> \n#> [[82]]\n#> [1] \"girl\"\n#> \n#> [[83]]\n#> [1] \"possum\"\n#> \n#> [[84]]\n#> [1] \"beaver\"\n#> \n#> [[85]]\n#> [1] \"oak_tree\"\n#> \n#> [[86]]\n#> [1] \"road\"\n#> \n#> [[87]]\n#> [1] \"crocodile\"\n#> \n#> [[88]]\n#> [1] \"snail\"\n#> \n#> [[89]]\n#> [1] \"wolf\"\n#> \n#> [[90]]\n#> [1] \"apple\"\n#> \n#> [[91]]\n#> [1] \"tiger\"\n#> \n#> [[92]]\n#> [1] \"castle\"\n#> \n#> [[93]]\n#> [1] \"hamster\"\n#> \n#> [[94]]\n#> [1] \"kangaroo\"\n#> \n#> [[95]]\n#> [1] \"fox\"\n#> \n#> [[96]]\n#> [1] \"leopard\"\n#> \n#> [[97]]\n#> [1] \"elephant\"\n#> \n#> [[98]]\n#> [1] \"lion\"\n#> \n#> [[99]]\n#> [1] \"porcupine\"\n#> \n#> [[100]]\n#> [1] \"squirrel\"\n```\n\n\n:::\n:::\n\n\n## All R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntransform <- function(x) {\n  transform_to_tensor(x)\n}\n\ntrain_ds <- cifar100_dataset(\n  root = \"./\", \n  train = TRUE, \n  download = TRUE, \n  transform = transform\n)\n\ntest_ds <- cifar100_dataset(\n  root = \"./\", \n  train = FALSE, \n  transform = transform\n)\n\ntrain_ds[1]\npar(mar = c(0, 0, 0, 0), mfrow = c(2, 2))\nindex <- sample(seq(50000), 4)\nfor (i in index) plot(as.raster(as.array(train_ds[i][[1]]$permute(c(2,3,1)))))\nconv_block <- nn_module(\n  initialize = function(in_channels, out_channels) {\n    self$conv <- nn_conv2d(\n      in_channels = in_channels, \n      out_channels = out_channels, \n      kernel_size = c(3,3),\n      padding = \"same\"\n    )\n    self$relu <- nn_relu()\n    self$pool <- nn_max_pool2d(kernel_size = c(2,2))\n  },\n  forward = function(x) {\n    x |> \n      self$conv() |>  \n      self$relu() |> \n      self$pool()\n  }\n)\nmodel <- nn_module(\n  initialize = function() {\n    self$conv <- nn_sequential(\n      conv_block(3, 32),\n      conv_block(32, 64),\n      conv_block(64, 128),\n      conv_block(128, 256)\n    )\n    self$output <- nn_sequential(\n      nn_dropout(0.5),\n      nn_linear(2*2*256, 512),\n      nn_relu(),\n      nn_linear(512, 100)\n    )\n  },\n  forward = function(x) {\n    x |> \n      self$conv() |> \n      torch_flatten(start_dim = 2) |>\n      self$output()\n  }\n)\n\nmodel()\nsystem.time(\nfitted <- model |>  \n  setup(\n    loss = nn_cross_entropy_loss(),\n    optimizer = optim_rmsprop, \n    metrics = list(luz_metric_accuracy())\n  ) |> \n  set_opt_hparams(lr = 0.001) |> \n  fit(\n    train_ds,\n    epochs = 10, #30,\n    valid_data = 0.2,\n    dataloader_options = list(batch_size = 128)\n  )\n)\n\nprint(fitted)\nevaluate(fitted, test_ds)\n\ncifar100_mapping <- jsonlite::read_json(\"data/cifar-100-labels.json\")\n\nx <- torch_empty(1, 3, 32, 32)\nimg_path <- file.path(\"img/squirrel.jpg\")\nimg <- img_path |>  \n  base_loader() |>  \n  transform_to_tensor() |>  \n  transform_resize(c(32, 32)) |>  \n  # normalize with imagenet mean and stds.\n  transform_normalize(\n    mean = c(0.4914, 0.4822, 0.4465),\n    std = c(0.2470, 0.2435, 0.2616)\n  )\nx[1,,, ] <- img\n\n\npreds <- fitted |> \n  predict(x) |> \n  torch_topk(dim = 2, k = 100)\n\ncifar100_mapping[as.integer(preds[[2]])]\n```\n:::\n\n\n\n\n# R Code Pre-Trained CNN\n\n## Pre-Trained CNN\n\nBoth Torch and Tensorflow has access to convolutional neural networks trained using the imagenet data base.\n\n## Pre-Trained CNN\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_imagenet <- torchvision::model_resnet18(pretrained = TRUE)\nmodel_imagenet$eval() # put the model in evaluation mode\n```\n:::\n\n\n## Loading Images\n\nDownload book images [here](https://www.statlearning.com/s/book_images.zip).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimg_dir <- \"img/books\"\nimage_names <- list.files(img_dir)\nnum_images <- length(image_names)\nx <- torch_empty(num_images, 3, 224, 224)\nfor (i in 1:num_images) {\n   img_path <- file.path(img_dir, image_names[i])\n   img <- img_path |> \n     base_loader() |>  \n     transform_to_tensor() |>  \n     transform_resize(c(224, 224)) |>  \n     # normalize with imagenet mean and stds.\n     transform_normalize(\n       mean = c(0.485, 0.456, 0.406),\n       std = c(0.229, 0.224, 0.225)\n     )\n   x[i,,, ] <- img\n}\n```\n:::\n\n\n## Prediction\n\n::: {.panel-tabset}\n\n## Prediction\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds <- model_imagenet(x)\ntop3 <- torch_topk(preds, dim = 2, k = 3)\n```\n:::\n\n\n## Labels\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmapping <- jsonlite::read_json(\"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\") |> \n  sapply(function(x) x[[2]])\n```\n:::\n\n\n## Labeling\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntop3_prob <- top3[[1]] |> \n  nnf_softmax(dim = 2) |> \n  torch_unbind() |> \n  lapply(as.numeric)\n\ntop3_class <- top3[[2]] |> \n  torch_unbind() |> \n  lapply(function(x) mapping[as.integer(x)])\n\nresult <- purrr::map2(top3_prob, top3_class, function(pr, cl) {\n  names(pr) <- cl\n  pr\n})\nnames(result) <- image_names\n```\n:::\n\n## Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> $flamingo.jpg\n#>    flamingo   spoonbill white_stork \n#> 0.978253186 0.016800024 0.004946792 \n#> \n#> $hawk_cropped.jpeg\n#>      kite       jay    magpie \n#> 0.6131909 0.2380927 0.1487164 \n#> \n#> $hawk.jpg\n#>         eel       agama common_newt \n#>   0.5320330   0.2608396   0.2071273 \n#> \n#> $huey.jpg\n#>           Lhasa Tibetan_terrier        Shih-Tzu \n#>      0.80046022      0.11664963      0.08289006 \n#> \n#> $kitty.jpg\n#>        Saint_Bernard           guinea_pig Bernese_mountain_dog \n#>            0.3915999            0.3394892            0.2689110 \n#> \n#> $weaver.jpg\n#> hummingbird    lorikeet   bee_eater \n#>   0.3568341   0.3490927   0.2940732\n```\n\n\n:::\n:::\n\n\n  \n:::\n\n\n## Squirrel\n\n::: {.panel-tabset}\n\n## Reload Image\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- torch_empty(1, 3, 224, 224)\nimg_path <- file.path(\"img/squirrel.jpg\")\nimg <- img_path |>  \n  base_loader() |>  \n  transform_to_tensor() |>  \n  transform_resize(c(224, 224)) |>  \n  # normalize with imagenet mean and stds.\n     transform_normalize(\n       mean = c(0.485, 0.456, 0.406),\n       std = c(0.229, 0.224, 0.225)\n     )\nx[1,,, ] <- img\n```\n:::\n\n## Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds <- model_imagenet(x)\ntop3 <- torch_topk(preds, dim = 2, k = 3)\nmapping[as.integer(top3[[2]])]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                  288                   48                   50 \n#>            \"leopard\"      \"Komodo_dragon\" \"American_alligator\"\n```\n\n\n:::\n:::\n\n\n:::\n\n\n## ALL R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimg_dir <- \"img/books\" ## CHANGE THIS\nimage_names <- list.files(img_dir)\nnum_images <- length(image_names)\nx <- torch_empty(num_images, 3, 224, 224)\nfor (i in 1:num_images) {\n   img_path <- file.path(img_dir, image_names[i])\n   img <- img_path |> \n     base_loader() |>  \n     transform_to_tensor() |>  \n     transform_resize(c(224, 224)) |>  \n     # normalize with imagenet mean and stds.\n     transform_normalize(\n       mean = c(0.485, 0.456, 0.406),\n       std = c(0.229, 0.224, 0.225)\n     )\n   x[i,,, ] <- img\n}\n\npreds <- model_imagenet(x)\ntop3 <- torch_topk(preds, dim = 2, k = 3)\n\nmodel_imagenet <- torchvision::model_resnet18(pretrained = TRUE)\nmodel_imagenet$eval() # put the model in evaluation mode\n\n\nmapping <- jsonlite::read_json(\"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\") |> \n  sapply(function(x) x[[2]])\n\ntop3_prob <- top3[[1]] |> \n  nnf_softmax(dim = 2) |> \n  torch_unbind() |> \n  lapply(as.numeric)\n\ntop3_class <- top3[[2]] |> \n  torch_unbind() |> \n  lapply(function(x) mapping[as.integer(x)])\n\nresult <- purrr::map2(top3_prob, top3_class, function(pr, cl) {\n  names(pr) <- cl\n  pr\n})\nnames(result) <- image_names\n\nprint(result)\n\nx <- torch_empty(1, 3, 224, 224)\nimg_path <- file.path(\"img/squirrel.jpg\") ## CHANGE THIS\nimg <- img_path |>  \n  base_loader() |>  \n  transform_to_tensor() |>  \n  transform_resize(c(224, 224)) |>  \n  # normalize with imagenet mean and stds.\n     transform_normalize(\n       mean = c(0.485, 0.456, 0.406),\n       std = c(0.229, 0.224, 0.225)\n     )\nx[1,,, ] <- img\n\npreds <- model_imagenet(x)\ntop3 <- torch_topk(preds, dim = 2, k = 3)\nmapping[as.integer(top3[[2]])]\n```\n:::\n\n::: {.cell}\n\n:::\n",
    "supporting": [
      "14b_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}