{
  "hash": "38c4bf2e7b391501abd904f53d67a991",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Introduction to Classification\"\nformat:\n  revealjs:\n    scrollable: true\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: false \n    chalkboard:\n      src: chalkboard.json\n      storage: chalkboard_pres\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: false\n    eval: true\n    message: false\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - reveal-header\n  - code-fullscreen\n\neditor: visual\n---\n\n\n## Learning Outcomes\n\n-   K-Nearest Neighbors\n\n-   Bayes' Classifier\n\n-   Logistic Regression\n\n## Classification\n\nThe practice of classifying data points into different categories.\n\n# K-Nearest Neighbors\n\n## K-Nearest Neighbors\n\nK-Nearest Neighbors will assign a category to a new data point based on the majority category of its **K** nearest neighbors.\n\n## KNN\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture_22_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n## Distances\n\nThe distance between a given point and the training data must be computed. These are the most commonly used distances:\n\n-   Manhattan\n\n-   Euclidean\n\n-   Minkowski\n\n## Manhattan Distance\n\n$$\nd(x,y) = \\sum_{i=1}^{p} |x_i - y_i|\n$$\n\n-   $x$: vector values of individual point\n\n-   $y$: vector values of new point\n\n-   $p$: length of vector\n\n## Euclidean Distance\n\n$$\nd(x,y) = \\sqrt{\\sum_{i=1}^{p} (x_i - y_i)^2}\n$$\n\n## Minkowski Distance\n\n$$\nd(x,y) = \\left( \\sum_{i=1}^{p} |x_i - y_i|^w \\right)^{\\frac{1}{w}}\n$$\n\n## Algorithm\n\nGiven a training data set, conduct the following steps:\n\n-   Compute the distance between a new data point and every point in the training data set.\n\n-   Choose the $K$ nearest training data points to the new point using the smallest distance.\n\n-   Categorize the new data point based on the majority of category from the $K$ nearest training data points.\n\n# Bayes Classifier\n\n## Bayes Classifier\n\nBayes Classifier is used to classify a data point to a category $c$\n\n$$\nf(\\boldsymbol x) = argmax_{c \\in C} f(C|\\boldsymbol X)\n$$\n\n## Probability\n\n$$\nf(C = c|\\boldsymbol X = x) = \\frac{f(\\boldsymbol X | C)\\pi_c}{f(\\boldsymbol X)}\n$$\n\n-   $f(\\boldsymbol X| C)$: conditional distribution of $\\boldsymbol X$\n\n-   $\\pi_c$: probability of observing category $C$\n\n-   $f(\\boldsymbol X)$: marginal distribution of $\\boldsymbol X$\n\n## Distribution of $f(\\boldsymbol X|C)$ and $f(\\boldsymbol X)$\n\nTo apply Bayes classifier, we must specify the form of $f(\\boldsymbol X| C)$ and $f(\\boldsymbol X)$. Common distributions are:\n\n-   Normal\n\n-   Bernoulli\n\n-   Multinomial\n\n# Logistic Regression\n\n## Logistic Regression\n\nLogistic regression can be used to classify an input of $\\boldsymbol X$ to a binary category.\n\n## Multinomial Regression\n\nAn extension of logistic regression, where there are more than two categories.\n\n## Fitting a model\n\nUse a glm to fit a model with a Bernoulli or multinomial distribution.\n\n# Example  \n\n## Create the KNN algorithm in R\n\n-   Manhattan\n\n-   Euclidean\n\n-   Minkowski ($w=5$)\n\nUse the `penguins` data set from `palmerpenguins` and categorize the following data: `bill_depth = 19`, `bill_length = 40`, `flipper_length = 185`, and `body_mass = 3345`.\n",
    "supporting": [
      "lecture_22_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}