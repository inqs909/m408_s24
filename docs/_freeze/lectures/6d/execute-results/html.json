{
  "hash": "80c247543eff9b7e863819cc27365a57",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear Regression\"\nsubtitle: \"Estimation Procedures\"\nformat:\n  revealjs:\n    scrollable: true\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: false \n    chalkboard:\n      src: chalkboard.json\n      storage: chalkboard_pres\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: false\n    eval: false\n    message: false\n    warnings: false\n    comment: \"#>\" \n\neditor: visual\n---\n\n\n\n\n## Learning Objectives\n\n-   Estimation\n\n-   Ordinary Least Squares\n\n-   Matrix Formulation\n\n-   Standard Errors\n\n-   Conduct in R\n\n# Estimation\n\n## Estimation\n\n-   Ordinary Least Squares\n\n-   Maximum Likelihood Approach\n\n-   Method of Moments\n\n## Standard Errors\n\n-   Find the variance of the estimate\n\n-   Find the information matrix\n\n-   Use for Inference\n\n# Ordinary Least Squares\n\n## Ordinary Least Squares\n\nFor a data pair $(X_i,Y_i)_{i=1}^n$, the ordinary least squares estimator will find the estimates of $\\hat\\beta_0$ and $\\hat\\beta_1$ that minimize the following function:\n\n$$\n\\sum^n_{i=1}\\{y_i-(\\beta_0+\\beta_1x_i)\\}^2\n$$\n\n## Estimates\n\n$$\n\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\bar x\n$$ $$\n\\hat\\beta_1 = \\frac{\\sum^n_{i=1}(y_i-\\bar y)(x_i-\\bar x)}{\\sum^n_{i=1}(x_i-\\bar x)^2}\n$$ $$\n\\hat\\sigma^2 = \\frac{1}{n-2}\\sum^n_{i=1}(y_i-\\hat y_i)^2\n$$\n\n# Matrix Formulation\n\n## Matrix Version of Model\n\n$$\nY_i = \\boldsymbol X_i^\\mathrm T \\boldsymbol \\beta + \\epsilon_i\n$$\n\n-   $Y_i$: Outcome Variable\n\n-   $\\boldsymbol X_i=(1, X_i)^\\mathrm T$: Predictors\n\n-   $\\boldsymbol \\beta = (\\beta_0, \\beta_1)^\\mathrm T$: Coefficients\n\n-   $\\epsilon_i$: error term\n\n## Data Matrix Formulation\n\nFor $n$ data points\n\n$$\n\\boldsymbol Y = \\boldsymbol X^\\mathrm T\\boldsymbol \\beta + \\boldsymbol \\epsilon\n$$\n\n-   $\\boldsymbol Y = (Y_1, \\cdots, Y_n)^\\mathrm T$: Outcome Variable\n\n-   $\\boldsymbol X=(\\boldsymbol X_1, \\cdots, \\boldsymbol X_n)^\\mathrm T$: Predictors\n\n-   $\\boldsymbol \\beta = (\\beta_0, \\beta_1)^\\mathrm T$: Coefficients\n\n-   $\\boldsymbol \\epsilon = (\\epsilon_1, \\cdots, \\epsilon_n)^\\mathrm T$: Error terms\n\n## Least Squares Formula\n\n$$\n(Y - \\boldsymbol X ^\\mathrm T\\boldsymbol \\beta)^\\mathrm T(Y - \\boldsymbol X ^\\mathrm T\\boldsymbol \\beta)\n$$\n\n## Estimates\n\n$$\n\\hat{\\boldsymbol \\beta} = (\\boldsymbol X ^\\mathrm T\\boldsymbol X)^{-1}\\boldsymbol X ^\\mathrm T\\boldsymbol Y\n$$\n\n# Standard Errors\n\n## Estimate for $\\sigma^2$\n\n$$\n\\hat \\sigma^2 = \\frac{1}{n-2} \\sum^n_{i=1} (Y_i-\\boldsymbol X_i^\\mathrm T\\hat{\\boldsymbol \\beta})^2\n$$\n\n## Standard Errors of $\\beta$'s\n\n$$\nSE(\\hat\\beta_0)=\\sqrt{\\frac{\\sum^n_{i=1}x_i^2\\hat\\sigma^2}{n\\sum^n_{i=1}(x_i-\\bar x)^2}}\n$$\n\n$$\nSE(\\hat\\beta_1)=\\sqrt\\frac{\\hat\\sigma^2}{\\sum^n_{i=1}(x_i-\\bar x)^2}\n$$\n\n## Standard Errors Matrix Form\n\n$$\nVar(\\hat {\\boldsymbol \\beta}) = (\\boldsymbol X ^\\mathrm T\\boldsymbol X)^{-1} \\hat \\sigma^2\n$$\n\n# R approaches\n\n## Built in Functions\n\nYou can use the `lm` to fit a linear model and extract the estimated values and standard errors\n\n## Matrix Formulation\n\nR is capable of conducting matrix operations with the following functions:\n\n-   `%*%`: matrix multiplication\n\n-   `t()`: transpose a matrix\n\n-   `solve()`: computes the inverse matrix\n\n## Minimization Problem\n\nMinimize the least squares using a numerical methods in R. The `optim()` function will minimize a function for set of parameters. We can minimize a function, least squares function, and supply initial values (0) for the parameters of interest.\n\n## Fit a Line using `lm` for the following data\n\n\n::: {.cell}\n\n:::\n\n\n## Fit a linear model using matrix operation\n\n\n::: {.cell}\n\n:::\n\n\n## Minimizing function using `optim`\n\nFind the value of x and y that will minimize the following function for any value a and b.\n\n$$\nf(x,y) = \\frac{(x-3)^2}{a^2} + \\frac{(y+4)^2}{b^2}\n$$\n\n\n::: {.cell}\n\n:::\n\n\n## Fit a linear model using `optim`\n\n\n::: {.cell}\n\n:::\n",
    "supporting": [
      "6d_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}