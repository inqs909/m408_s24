{
  "hash": "b348064c99d1a1db2b298bd4c3659792",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bagging, Random Forests, and Boosting\"\nformat:\n  revealjs:\n    scrollable: true\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: false \n    chalkboard:\n      src: chalkboard.json\n      storage: chalkboard_pres\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: true\n    message: false\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - reveal-header\n  - code-fullscreen\n\neditor: visual\n---\n\n\n\n## Learning Outcomes\n\n-   \n\n-   Bagging\n\n-   Random Forests\n\n-   Boosting\n\n# Bagging\n\n## Bagging\n\nWhen splitting the data to train and test data sets, the construction of the tree suffers from high variance.\n\n::: fragment\nThis is due to splitting the data in a random way. One training data set will lead to different results from another training data set.\n:::\n\n::: fragment\nTo improve performance, we implement a *Bootstrap Aggregation (Bagging)* technique.\n:::\n\n::: fragment\nBagging will produce a forest of trees to classify a new observation.\n:::\n\n## Bagging Algorithm\n\nGiven a single training data set:\n\n1.  Sample from the data with replacement.\n\n2.  Build a tree from the sampled data:\n\n    $$\n    \\hat f^{*b}(x)\n    $$\n\n3.  Repeat the process B times (B=100)\n\n4.  Compute the final average for all predictions:\n\n    $$\n    \\hat f_{bag}(x)=\\frac{1}{B}\\sum^B_{b=1}\\hat f^{*b}(x)\n    $$\n\n## Classification\n\nTo classify an observation, you can record the classification of each $b$ tree. Then classify an observation by majority rule.\n\n## Variable Importance\n\nWith the implementation of Bagging, you lose interpretability from the original tree due to the forest.\n\nHowever, we can compute which variables reduced the RSS or Gini Index for all the trees. The variables with the largest reduction are considered important.\n\n# Random Forests\n\n## Random Forests\n\nRandom Forests is an extension of Bagging, where a forest is generated from a bootstrap-based approach. However, when making a split, a **random** set of predictors (m\\<p) are chosen for the split, instead of the full set p.\n\n::: fragment\nThis will ensure that trees are unique, uncorrelated.\n:::\n\n::: fragment\nIt ensures that no one predictor will have all the power and lower the variance.\n:::\n\n# Boosting\n\n## Boosting\n\nBoosting is a mechanism where a final tree is built slowly from smaller trees using the residuals.\n\n::: fragment\nThis ensures a tree is built from a slow process and prevents overfitting.\n:::\n\n::: fragment\nThis is done to improve prediction capabilities.\n:::\n\n## Algorithm\n\n1.  Set $\\hat f(x) = 0$ and $r_i = y_i$ for all $i$ in the training set\n\n2.  For $b=1, 2, \\ldots, B$ repeat:\n\n    1.  Fit tree $\\hat f^b$ with $d$ splits ($d+1$ terminal nodes) to the training data $(X,r)$\n\n    2.  Update $\\hat f$\n\n        $$\n        \\hat f(x) \\leftarrow \\hat f(x) + \\lambda\\hat f^b(x)\n        $$\n\n    3.  Update residuals\n\n        $$\n        r_i \\leftarrow r_i - \\lambda\\hat f^{b}(x_i)\n        $$\n\n3.  Output boosted model:\n\n    $$\n    \\hat f(x) = \\sum^B_{b=1} \\lambda \\hat f^b(x)\n    $$\n\n# R Code\n\n## Bagging Regression Trees\n\n::: panel-tabset\n### R Code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest)\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(magrittr)\n\npenguins %<>% drop_na()\ntrain <- sample(1:nrow(penguins), nrow(penguins)/2)\nbag_penguins <- penguins %$% randomForest(body_mass_g ~ bill_depth_mm + bill_length_mm + flipper_length_mm, subset = train, mtry = 3, importance = T)\n```\n:::\n\n\n\n### Predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyhat_bag <- predict(bag_penguins, newdata = penguins[-train , ])\ntest <- penguins[-train , ]$body_mass_g\nplot(yhat_bag, test)\n```\n\n::: {.cell-output-display}\n![](13c_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n:::\n\n## Bagging Classification Trees\n\n::: panel-tabset\n### R Code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbag_penguins <- penguins %$% randomForest(species ~ body_mass_g + bill_depth_mm + bill_length_mm + flipper_length_mm, subset = train, mtry = 4, importance = T)\n```\n:::\n\n\n\n### Prediction\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyhat_bag <- predict(bag_penguins, newdata = penguins[-train , ])\ntest <- penguins[-train , ]$species\ntable(yhat_bag, test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>            test\n#> yhat_bag    Adelie Chinstrap Gentoo\n#>   Adelie        74         1      0\n#>   Chinstrap      7        28      0\n#>   Gentoo         1         0     56\n```\n\n\n:::\n:::\n\n\n:::\n\n## Random Forests Regression Trees\n\n::: panel-tabset\n### R Code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbag_penguins <- penguins %$% randomForest(body_mass_g ~ bill_depth_mm + bill_length_mm + flipper_length_mm, subset = train, mtry = 2, importance = T)\n```\n:::\n\n\n\n### Prediction\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyhat_bag <- predict(bag_penguins, newdata = penguins[-train , ])\ntest <- penguins[-train , ]$body_mass_g\nplot(yhat_bag, test)\n```\n\n::: {.cell-output-display}\n![](13c_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n:::\n\n## Random Forests Classification Trees\n\n::: panel-tabset\n### R Code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbag_penguins <- penguins %$% randomForest(species ~ body_mass_g + bill_depth_mm + bill_length_mm + flipper_length_mm, subset = train, mtry = 2, importance = T)\n```\n:::\n\n\n\n### Prediction\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyhat_bag <- predict(bag_penguins, newdata = penguins[-train , ])\ntest <- penguins[-train , ]$species\ntable(yhat_bag, test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>            test\n#> yhat_bag    Adelie Chinstrap Gentoo\n#>   Adelie        75         1      0\n#>   Chinstrap      6        28      0\n#>   Gentoo         1         0     56\n```\n\n\n:::\n:::\n\n\n:::\n\n## Boosting Regression Trees\n\n::: panel-tabset\n### R Code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gbm)\nboost_penguin <- gbm(body_mass_g ~ bill_depth_mm + bill_length_mm + flipper_length_mm, data = penguins[train , ],\ndistribution = \"gaussian\", n.trees = 5000,\ninteraction.depth = 4)\n```\n:::\n\n\n\n### Prediction\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyhat_boost <- predict(boost_penguin, newdata = penguins[-train , ], n.trees = 5000)\ntest <- penguins[-train , ]$body_mass_g\nplot(yhat_bag, test)\n```\n\n::: {.cell-output-display}\n![](13c_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n:::\n",
    "supporting": [
      "13c_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}