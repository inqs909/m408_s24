{
  "hash": "1336212b13265d02074ae35851d4c029",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Hypothesis Testing\"\nsubtitle: \"Model Inference\"\nformat:\n  revealjs:\n    scrollable: true\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: false \n    chalkboard:\n      src: chalkboard.json\n      storage: chalkboard_pres\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: false\n    message: false\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - reveal-header\n  - code-fullscreen\n\neditor: visual\n---\n\n\n## Learning Outcomes\n\n-   Hypothesis Tests\n\n-   Model Inference\n\n# Hypothesis Tests\n\n## Hypothesis Tests\n\nHypothesis tests are used to test whether claims are valid or not. This is conducted by collecting data, setting the **Null** and **Alternative** Hypothesis.\n\n## Null Hypothesis $H_0$\n\nThe null hypothesis is the claim that is initially believed to be true. For the most part, it is always equal to the hypothesized value.\n\n## Alternative Hypothesis $H_a$\n\nThe alternative hypothesis contradicts the null hypothesis.\n\n## Example of Null and Alternative Hypothesis\n\nWe want to see if $\\mu$ is different from $\\mu_0$\n\n| Null Hypothesis    | Alternative Hypothesis |\n|--------------------|------------------------|\n| $H_0: \\mu=\\mu_0$   | $H_a: \\mu\\ne\\mu_0$     |\n| $H_0: \\mu\\le\\mu_0$ | $H_a: \\mu>\\mu_0$       |\n| $H_0: \\mu\\ge\\mu_0$ | $H_0: \\mu<\\mu_0$       |\n\n## One-Side vs Two-Side Hypothesis Tests\n\nNotice how there are 3 types of null and alternative hypothesis, The first type of hypothesis ($H_a:\\mu\\ne\\mu_0$) is considered a 2-sided hypothesis because the rejection region is located in 2 regions. The remaining two hypotheses are considered 1-sided because the rejection region is located on one side of the distribution.\n\n| Null Hypothesis    | Alternative Hypothesis | Side    |\n|--------------------|------------------------|---------|\n| $H_0: \\mu=\\mu_0$   | $H_a: \\mu\\ne\\mu_0$     | 2-sided |\n| $H_0: \\mu\\le\\mu_0$ | $H_a: \\mu>\\mu_0$       | 1-sided |\n| $H_0: \\mu\\ge\\mu_0$ | $H_0: \\mu<\\mu_0$       | 1-sided |\n\n## Common Hypothesis Tests\n\n| Test               | \\# of Samples | Null Hypothesis            | Distribution  | DF                                                                               | Test Statistics                                          | Notes                                               |\n|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n| t-test             | 1             | $\\mu=\\mu_0$                | $t_{DF}$      | $DF=n-1$                                                                         | $\\frac{\\bar X-\\mu_0}{\\sqrt{s^2/n}}$                      | $n<30$                                              |\n| z-test             | 1             | $\\mu=\\mu_0$                | $N(0,1)$      | None                                                                             | $\\frac{\\bar X-\\mu_0}{\\sqrt{\\sigma^2/n}}$                 | $n>30$ $\\sigma^2$ known                             |\n| z-test             | 1             | $\\mu=\\mu_0$                | $N(0,1)$      | None                                                                             | $\\frac{\\bar X-\\mu_0}{\\sqrt{s^2/n}}$                      | $n>30$ $\\sigma^2$ unknown                           |\n| paired t-test      | 2             | $\\mu_1-\\mu_2=D$            | $t_{DF}$      | $DF=n-1$                                                                         | $\\frac{\\bar X_d-D}{\\sqrt{s_d^2/n}}$                      | $n$ is the number of pairs                          |\n| independent t-test | 2             | $\\mu_1-\\mu_2=D$            | $t_{DF}$      | $DF=n_1+n_2-2$                                                                   | $\\frac{\\bar X_1-\\bar X_2-D}{\\sqrt{s_p^2(1/n_1+1/n_2)}}$  | $s_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}$ |\n| independent t-test | 2             | $\\mu_1-\\mu_2=D$            | $t_{DF}$      | $DF=\\frac{(s^2_1/n_1+s^2_2/n_2)^2}{(s^2_1/n_1)^2/(n_1-1)+(s^2_2/n_2)^2/(n_2-1)}$ | $\\frac{\\bar X_1-\\bar X_2-D}{\\sqrt{s_1^2/n_1+s^2_2/n_2}}$ |                                                     |\n| ANOVA              | \\>2           | $\\mu_1=\\mu_2=\\cdots=\\mu_k$ | $F_{DF1,DF2}$ |                                                                                  |                                                          |                                                     |\n\n## Decision Making: P-Value\n\nThe p-value approach is one of the most common methods to report significant results. It is easier to interpret the p-value because it provides the probability of observing our test statistics, or something more extreme, given that the null hypothesis is true. Depending on the type of test, your p-value may be constructed as:\n\n| Alternative Hypothesis | p-value                 |\n|------------------------|-------------------------|\n| $\\mu>\\mu_0$            | $P(X>T(x))=p$           |\n| $\\mu<\\mu_0$            | $P(X<T(x))=p$           |\n| $\\mu\\ne\\mu_0$          | $2\\times P(X>|T(X)|)=p$ |\n\nIf $p < \\alpha$, then you reject $H_0$; otherwise, you will fail to reject $H_0$.\n\n## Decision Making: Confidence Interval Approach\n\nThe confidence interval approach can evaluate a hypothesis test where the alternative hypothesis is $\\mu\\ne\\mu_0$. For this approach you will construct a $(1-\\alpha)100\\%$ confidence interval as\n\n$$\nPE \\pm CV *SE\n$$\n\nwhere PE is the point estimate, CV is the critical value based on $\\alpha$ and SE is the standard error. This will result in a lower and upper bound denoted as: $(LB, UB)$. The confidence intervals provides a range values to capture the parameter $\\mu$ such that if you repeat this process $n$ times, $(1-\\alpha)100\\%$ of $n$ will capture the true value of $\\mu$. If $\\mu_0 \\in (LB,UB)$, then you fail to reject $H_0$. If $\\mu_0\\notin (LB,UB)$, then you reject $H_0$.\n\n# Model Inference\n\n## Testing $\\beta_j$\n\n$$\n\\frac{\\hat\\beta_j-\\theta}{\\mathrm{se}(\\hat\\beta_j)} \\sim t_{n-p^\\prime}\n$$\n\n## Confidence Intervals\n\n$$\n\\hat \\beta_j \\pm CV \\times se(\\hat\\beta_j)\n$$\n\n-   CV: Critical Value $P(X<CV) = 1-\\alpha/2$\n\n-   $\\alpha$: significance level\n\n-   SE: Standard Error\n\n## Model inference\n\nWe conduct model inference to determine if different models are better at explaining variation. A common example is to compare a linear model ($\\hat Y=\\hat\\beta_0 + \\hat\\beta_1 X$) to the mean of Y ($\\hat \\mu_y$). We determine the significance of the variation explained using an Analysis of Variance (ANOVA) table and F test.\n\n## ANOVA Table\n\n| Source | DF        | SS            | MS                    | F                        |\n|--------------|--------------|--------------|-----------------|---------------|\n| Model  | $DFR=k-1$ | $SSR$         | $MSR=\\frac{SSM}{DFR}$ | $\\hat F=\\frac{MSR}{MSE}$ |\n| Error  | $DFE=n-k$ | $SSE$         | $MSE=\\frac{SSE}{DFE}$ |                          |\n| Total  | $TDF=n-1$ | $TSS=SSR+SSE$ |                       |                          |\n\n$$\n\\hat F \\sim F(DFR, DFE)\n$$\n\n## Model Inference\n\nGiven:\n\n$$\nM1:\\ \\hat y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 \n$$\n\n$$\nM2:\\ \\hat y = \\beta_0 + \\beta_1 X_1  \n$$\n\nLet $M1$ be the FULL (larger) model, and let $M2$ be the RED (Reduced, smaller) model.\n\n## Model Inference\n\nHe can test the following Hypothesis:\n\n-   $H_0$: The error variations between the FULL and RED model are not different.\n-   $H_1$: The error variations between the FULL and RED model are different.\n\n## Test Statistic\n\n$$\n\\hat F = \\frac{[SSE(RED) - SSE(FULL)]/[DFE(RED)-DFE(FULL)]}{MSE(FULL)} \n$$\n\n$$\n\\hat F \\sim F[DFE(RED) - DFE(FULL), DFE(FULL)]\n$$\n\n# Linear Models Equivalent of Common Statistical Tests\n\n## 1-Sample t-test\n\nThis can be conducted as $Y=\\beta_0$\n\nTest the following hypothesis: $H_0:\\ \\beta_0 = 205$ OR $H_0: \\mu = 205$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\nlibrary(tidyverse)\npenguins <- penguins |> drop_na()\nxlm <- penguins  |>  lm(flipper_length_mm ~ 1, data = _)\ncoef(xlm)\nconfint(xlm)\n\n## OR\nt.test(penguins$flipper_length_mm, mu = 205)\n```\n:::\n\n\n## 2-Sample t-test\n\nThis can be conducted as $Y=\\beta_0 + \\beta_1 X$, where $X$ is a binary variable for 2 samples.\n\nTest the following hypothesis: $H_0:\\ \\beta_1 = 0$ OR $H_0: \\mu_1-\\mu_2 = 0$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmtcars |> lm(mpg ~ vs, data = _) |> summary()\n\n## OR\n\nmtcars |> t.test(mpg ~ vs, data = _)\n```\n:::\n\n\n## Paired t-test\n\nThis can be conducted as $Y_1 - Y_2=\\beta_0$\n\nTest the following hypothesis: $H_0:\\ \\beta_0 = 0$ OR $H_0: \\mu_1 - \\mu_2 = 0$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# DATA PREP\nbefore <-c(200.1, 190.9, 192.7, 213, 241.4, 196.9, 172.2, 185.5, 205.2, 193.7)\nafter <-c(392.9, 393.2, 345.1, 393, 434, 427.9, 422, 383.9, 392.3, 352.2)\ndf <- data.frame(before, after)\n\ndf |> lm(after - before ~ 1, data = _) |> (\\(x) c(coef(x), confint(x)))()\n## OR\ndf |> with(t.test(before, after, paired = T))\n```\n:::\n\n\n## ANOVA\n\nThis can be conducted as $Y=\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_k X_k$, where $X_k$ are binary variable for k samples.\n\nTest the following hypothesis: $H_0:\\ \\beta_1 =\\beta_2 =\\cdots=\\beta_k = 0$ OR $H_0: \\mu_1 =\\mu_2 =\\cdots=\\mu_k = 0$\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |> lm(flipper_length_mm ~ species, data = _) |> anova()\n\n## OR\n\npenguins |> aov(flipper_length_mm ~ species, data = _) |> anova()\n```\n:::\n\n\n# Applications in R\n\n## Individual Hypothesis Testing\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |> \n  lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm + species, data = _) |> \n  summary()\n```\n:::\n\n\n## Model Inference\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxlm <- penguins |> \n  lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm + species, data = _) \n\nsummary(xlm)\nanova(xlm)\n\n## OR\n\nxlm_null <- penguins |> lm(body_mass_g ~ 1, data = _)\nanova(xlm_null, xlm)\n\n## OR\n\nlibrary(supernova)\nsupernova(xlm)\n```\n:::\n\n\n## Comparing Different Models\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxlm1 <- penguins |> \n  lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm + species, data = _) \n\nxlm2 <- penguins |> \n  lm(body_mass_g ~ flipper_length_mm + species, data = _) \n\nanova(xlm2, xlm1)\n```\n:::\n",
    "supporting": [
      "8b_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}