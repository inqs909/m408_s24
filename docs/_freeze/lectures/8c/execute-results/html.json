{
  "hash": "ef478d9f52de4dee5a1d15d14afa23b0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Model Building\"\nformat:\n  revealjs:\n    scrollable: true\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: false \n    chalkboard:\n      src: chalkboard.json\n      storage: chalkboard_pres\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: false\n    eval: false\n    message: false\n    warnings: false\n    comment: \"#>\" \n\neditor: visual\n---\n\n\n\n\n\n## Learning Objective\n\n-   Fixing Assumptions\n\n-   Model Building\n\n# Fixing Assumptions\n\n## Fixing Assumptions\n\n-   Linearity\n\n-   Collinearity\n\n-   Unequal Variances\n\n-   Influential observations and Outliers\n\n## Residual Function\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_resid <- function(x){\n  res <- data.frame(obs = 1:nrow(x$model),\n                    x$model, \n                    resid = resid(x),\n                    fitted = fitted(x),\n                    sresid = rstandard(x),\n                    hatvals = hatvalues(x),\n                    jackknife =  rstudent(x),\n                    cooks = cooks.distance(x)\n                    )\n  return(res)\n}\n```\n:::\n\n\n\n## Linearity $\\log(x)$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rnorm(50, 6, 2)\ny <- 3 - 2 * log(x) + rnorm(50, sd = 0.25)\nx_lm <- lm(y ~ x)\nresid_df <- df_resid(x_lm)\nggplot(resid_df, aes(x, resid)) + geom_point() +\n  stat_smooth(se = F) +\n  geom_hline(yintercept = 0)+\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](8c_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nx_lm <- lm(y ~ log(x))\nresid_df <- df_resid(x_lm)\nggplot(resid_df, aes(x, resid)) + geom_point() +\n  stat_smooth(se = F) +\n  geom_hline(yintercept = 0)+\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](8c_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n## Linearity $1/x$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rnorm(50, 6, 1.5)\ny <- 3 - 2 / x + rnorm(50, sd = 0.2)\nx_lm <- lm(y ~ x)\nresid_df <- df_resid(x_lm)\nggplot(resid_df, aes(x, resid)) + geom_point() +\n  stat_smooth(se = F) +\n  geom_hline(yintercept = 0)+\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](8c_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nx_lm <- lm(y ~ I(1/x))\nresid_df <- df_resid(x_lm)\nggplot(resid_df, aes(x, resid)) + geom_point() +\n  stat_smooth(se = F) +\n  geom_hline(yintercept = 0)+\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](8c_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n## Other Common Covariate Function Transformations\n\n-   $\\log_{10}(x)$\n\n-   $x^2$\n\n-   $\\sqrt x$\n\n-   $x^3$\n\n## Unequal Variance\n\nThere are a couple of methods to adjust for unequal variances\n\n-   Generalized Least Squares\n\n-   Mixed-Effects Models\n\n## Collinearity\n\nThere are couple of methods to fix collinearity:\n\n-   Principal Components Analysis\n\n-   Ridge Regression\n\n-   Lasso Regression\n\n# Model Building\n\n## Model Building\n\nWhen given a set of predictors, we want to build a model that only contains predictors that best fits the data, without overfitting.\n\nIdeally, we always want to choose a parsimonious model that best describes the outcome variable. The more predictors into the model, the less parsimonious and less powerful.\n\nChoosing the best model can be done based on selection criteria such as Mallow's $C_p$, AIC, AICc, BIC, and adjusted $R^2$.\n\n## Model Building\n\n-   Best Subset Model\n\n    -   Fit all models and select the best model based criteria\n\n-   Forward Stepwise Model Building\n\n    -   Begin with the null model ($Y\\sim 1$) and add variables until a final model is chosen.\n\n-   Backward Stepwise Model Building\n\n    -   Begin with the full model, and remove variable until the final model is chosen.\n\n-   Hybrid Stepwise Regression\n\n    -   A hybrid approach between the forward and backward building approach.\n\n## Best Subset Model Building\n\n1.  Begin with the null model, no predictors\n2.  For $k=1,\\ldots, p$ (number of predictors):\n    1.  Fit all $\\left(^p_k\\right)$ models that contain $k$ predictors\n\n    2.  Define $M_k$ as the model with the largest $R²$\n3.  The final model is the model $M_k$ based on selection criteria\n\n## Forward Stepwise Model Building\n\n1.  Begin with the null model, no predictors\n2.  For $k=0,\\ldots, p-1$ (number of predictors):\n    1.  Fit all $p-k$ models that adds one new predictor to the orginal model containing $k$ predictors\n\n    2.  Define $M_{k+1}$ as the model with the largest $R²$ among $p-k$ models\n3.  The final model is the model $M_(k+1)$ based on selection criteria\n\n## Backward Stepwise Model Building\n\n1.  Begin with the full model $M_p$, with all predictors\n2.  For $k=p,p-1, \\ldots, 1$ (number of predictors):\n    1.  Fit all models that contain $k-1$ predictors\n\n    2.  Define $M_{k-1}$ as the model with the largest $R²$\n3.  The final model is the model $M_k$ based on selection criteria\n\n# Penalty Term Techniques\n\n## Ridge Regression\n\nMinimizes the following function\n\n$$\n\\sum^n_{i=1}(y_i-\\boldsymbol X_i^\\mathrm T \\boldsymbol \\beta)^2 +\\lambda \\boldsymbol \\beta ^\\mathrm T\\boldsymbol \\beta\n$$\n\n-   $\\boldsymbol X_i$: Design matrix for $i$th observation\n\n-   $\\boldsymbol \\beta$: regression coefficients\n\n-   $\\lambda > 0$: tuning parameter\n\n## Lasso Regression\n\nMinimize the following function:\n\n$$\n\\sum^n_{i=1}(y_i-\\boldsymbol X_i^\\mathrm T \\boldsymbol \\beta)^2 +\\lambda \\sum^p_{j=0}|\\beta_j|\n$$\n\n## Selecting the Ideal penalty term\n\nSelecting the correct penalty term is essential to ensure an ideal bias-variance trade-off. The best approach is to use a cross-validation approach, more specifically, the LOOCV (leave-one-out cross-validation).\n\n## Cross-Validation\n\n-   Choose a set of tuning parameters to test.\n\n-   For each $k$th turning parameter Calculate the tuning parameter error for each value\n\n    -   Utilize the leave-one-out approach\n\n        -   For each observation fit and compute:\n\n            $$\n            MSE_i = (y_i - \\hat y_{i(i)})^2\n            $$\n\n        -   Compute the following error:\n\n            $$\n            CVE_k = \\frac{1}{n}\\sum^n_{i=1}MSE_i \n            $$\n\n-   Identify the $k$th tuning parameter with the lowest $CVE_k$\n\n# Selection Criteria\n\n## Mallow's $C_p$\n\n$$\nC_p = \\frac{1}{n}(RSS + 2 d \\hat \\sigma^2) \n$$\n\n-   $RSS$: Residual Sum of Squares\n\n-   $\\hat \\sigma^2$: Mean Square Error\n\n-   $d$: number of predictors\n\n-   Lower is better\n\n## Aikaike Information Criteria (AIC)\n\n$$\n\\frac{1}{n\\hat\\sigma^2}(RSS+2d\\hat\\sigma^2)\n$$\n\n-   Lower is Better\n\n## Bayesian Information Criteria (BIC)\n\n$$\n\\frac{1}{n\\hat\\sigma^2}\\{RSS+\\log(n)d\\hat\\sigma^2\\}\n$$\n\n-   Lower is better\n\n## $R^2$\n\n$$\n1-\\frac{RSS}{TSS}\n$$\n\n-   $RSS=\\sum^n_{i=1}(y_i-\\hat y_i)^2$\n\n-   $TSS=\\sum^n_{i=1}(y_i-\\bar y)^2$\n\n-   Higher is Better\n\n## Adjusted $R^2$\n\n$$\n1-\\frac{RSS/(n-d-1)}{\nTSS/(n-1)}\n$$\n\n-   Higher is Better\n\n# Model Building R Code\n\n## R Code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(leaps)\nregsubsets(y ~ ., data)\n```\n:::\n\n\n\n## Full Subset\n\n::: panel-tabset\n## Code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_full <- regsubsets(mpg ~ ., data = mtcars)\n```\n:::\n\n\n\n## Summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum_lm_full <- summary(lm_full)\nprint(sum_lm_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Subset selection object\n#> Call: regsubsets.formula(mpg ~ ., data = mtcars)\n#> 10 Variables  (and intercept)\n#>      Forced in Forced out\n#> cyl      FALSE      FALSE\n#> disp     FALSE      FALSE\n#> hp       FALSE      FALSE\n#> drat     FALSE      FALSE\n#> wt       FALSE      FALSE\n#> qsec     FALSE      FALSE\n#> vs       FALSE      FALSE\n#> am       FALSE      FALSE\n#> gear     FALSE      FALSE\n#> carb     FALSE      FALSE\n#> 1 subsets of each size up to 8\n#> Selection Algorithm: exhaustive\n#>          cyl disp hp  drat wt  qsec vs  am  gear carb\n#> 1  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n#> 2  ( 1 ) \"*\" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n#> 3  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#> 4  ( 1 ) \" \" \" \"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#> 5  ( 1 ) \" \" \"*\"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#> 6  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#> 7  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \"*\"  \" \" \n#> 8  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \"*\"  \"*\"\n```\n\n\n:::\n:::\n\n\n\n## Plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(sum_lm_full$cp)\n```\n\n::: {.cell-output-display}\n![](8c_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n## Model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(lm_full,3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> (Intercept)          wt        qsec          am \n#>    9.617781   -3.916504    1.225886    2.935837\n```\n\n\n:::\n:::\n\n\n:::\n\n## Forward Model Building\n\n::: panel-tabset\n## Code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_full <- regsubsets(mpg ~ ., data = mtcars,\n                      method =  \"forward\")\n```\n:::\n\n\n\n## Summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum_lm_full <- summary(lm_full)\nprint(sum_lm_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Subset selection object\n#> Call: regsubsets.formula(mpg ~ ., data = mtcars, method = \"forward\")\n#> 10 Variables  (and intercept)\n#>      Forced in Forced out\n#> cyl      FALSE      FALSE\n#> disp     FALSE      FALSE\n#> hp       FALSE      FALSE\n#> drat     FALSE      FALSE\n#> wt       FALSE      FALSE\n#> qsec     FALSE      FALSE\n#> vs       FALSE      FALSE\n#> am       FALSE      FALSE\n#> gear     FALSE      FALSE\n#> carb     FALSE      FALSE\n#> 1 subsets of each size up to 8\n#> Selection Algorithm: forward\n#>          cyl disp hp  drat wt  qsec vs  am  gear carb\n#> 1  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n#> 2  ( 1 ) \"*\" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n#> 3  ( 1 ) \"*\" \" \"  \"*\" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n#> 4  ( 1 ) \"*\" \" \"  \"*\" \" \"  \"*\" \" \"  \" \" \"*\" \" \"  \" \" \n#> 5  ( 1 ) \"*\" \" \"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#> 6  ( 1 ) \"*\" \"*\"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#> 7  ( 1 ) \"*\" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#> 8  ( 1 ) \"*\" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \"*\"  \" \"\n```\n\n\n:::\n:::\n\n\n\n## Plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(sum_lm_full$cp)\n```\n\n::: {.cell-output-display}\n![](8c_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n## Model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(lm_full,3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> (Intercept)         cyl          hp          wt \n#>  38.7517874  -0.9416168  -0.0180381  -3.1669731\n```\n\n\n:::\n:::\n\n\n:::\n\n## Backward Model Building\n\n::: panel-tabset\n## Code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_full <- regsubsets(mpg ~ ., data = mtcars,\n                      method =  \"backward\")\n```\n:::\n\n\n\n## Summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum_lm_full <- summary(lm_full)\nprint(sum_lm_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Subset selection object\n#> Call: regsubsets.formula(mpg ~ ., data = mtcars, method = \"backward\")\n#> 10 Variables  (and intercept)\n#>      Forced in Forced out\n#> cyl      FALSE      FALSE\n#> disp     FALSE      FALSE\n#> hp       FALSE      FALSE\n#> drat     FALSE      FALSE\n#> wt       FALSE      FALSE\n#> qsec     FALSE      FALSE\n#> vs       FALSE      FALSE\n#> am       FALSE      FALSE\n#> gear     FALSE      FALSE\n#> carb     FALSE      FALSE\n#> 1 subsets of each size up to 8\n#> Selection Algorithm: backward\n#>          cyl disp hp  drat wt  qsec vs  am  gear carb\n#> 1  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n#> 2  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \"*\"  \" \" \" \" \" \"  \" \" \n#> 3  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#> 4  ( 1 ) \" \" \" \"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#> 5  ( 1 ) \" \" \"*\"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#> 6  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#> 7  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \"*\"  \" \" \n#> 8  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \"*\"  \"*\"\n```\n\n\n:::\n:::\n\n\n\n## Plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(sum_lm_full$cp)\n```\n\n::: {.cell-output-display}\n![](8c_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n## Model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(lm_full,3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> (Intercept)          wt        qsec          am \n#>    9.617781   -3.916504    1.225886    2.935837\n```\n\n\n:::\n:::\n\n\n:::\n\n## Ridge and Lasso Regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n# Ridge Regression\nglmnet(x, y, \n       alpha = 0, \n       lambda = grid)\n# Lasso Regression\nglmnet(x, y, \n       alpha = 1, \n       lambda = grid)\n```\n:::\n\n\n\n## Ridge Regression\n\n::: panel-tabset\n## Prep\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- model.matrix(mpg ~ ., data = mtcars)[,-1]\ny <- mtcars$mpg\n\ngrid_lambda <- seq(0, 100, by = 0.1)\n```\n:::\n\n\n\n## CV\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nridge_reg_cv <- cv.glmnet(x, y,\n                    alpha = 0,\n                    lambda = grid_lambda)\nridge_reg_cv\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:  cv.glmnet(x = x, y = y, lambda = grid_lambda, alpha = 0) \n#> \n#> Measure: Mean-Squared Error \n#> \n#>     Lambda Index Measure    SE Nonzero\n#> min    1.9   982   7.108 1.892      10\n#> 1se   10.9   892   8.998 2.994      10\n```\n\n\n:::\n:::\n\n\n\n## Model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nridge_reg <- glmnet(x, y,\n                    alpha = 0,\n                    lambda = grid_lambda)\n\ncoef(ridge_reg)[,974]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  (Intercept)          cyl         disp           hp         drat           wt \n#> 21.143813495 -0.371451745 -0.005255145 -0.011639875  1.054826728 -1.238823689 \n#>         qsec           vs           am         gear         carb \n#>  0.162434931  0.767311761  1.628364774  0.544002049 -0.549534073\n```\n\n\n:::\n:::\n\n\n:::\n\n## Lasso Regression\n\n::: panel-tabset\n## Prep\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- model.matrix(mpg ~ ., data = mtcars)[,-1]\ny <- mtcars$mpg\n\ngrid_lambda <- seq(0, 100, by = 0.1)\n```\n:::\n\n\n\n## CV\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_reg_cv <- cv.glmnet(x, y,\n                    alpha = 1,\n                    lambda = grid_lambda)\nlasso_reg_cv\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:  cv.glmnet(x = x, y = y, lambda = grid_lambda, alpha = 1) \n#> \n#> Measure: Mean-Squared Error \n#> \n#>     Lambda Index Measure    SE Nonzero\n#> min    0.9   992   8.322 2.079       3\n#> 1se    1.6   985  10.218 2.551       3\n```\n\n\n:::\n:::\n\n\n\n## Model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_reg <- glmnet(x, y,\n                    alpha = 1,\n                    lambda = grid_lambda)\n\ncoef(lasso_reg)[,993]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> (Intercept)         cyl        disp          hp        drat          wt \n#> 36.00374206 -0.88697325  0.00000000 -0.01170578  0.00000000 -2.70662163 \n#>        qsec          vs          am        gear        carb \n#>  0.00000000  0.00000000  0.00000000  0.00000000  0.00000000\n```\n\n\n:::\n:::\n\n\n:::\n",
    "supporting": [
      "8c_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}