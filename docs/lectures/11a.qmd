---
title: "Introduction to Classification"
format:
  revealjs:
    scrollable: true
    navigation-mode: vertical
    controls-layout: bottom-right
    controls-tutorial: true
    incremental: false 
    chalkboard:
      src: chalkboard.json
      storage: chalkboard_pres
      theme: whiteboard
      chalk-width: 4
knitr:
  opts_chunk: 
    echo: false
    eval: true
    message: false
    warnings: false
    comment: "#>" 
    
revealjs-plugins:
  - pointer
  - verticator
  
filters: 
  - reveal-header
  - code-fullscreen

editor: visual
---

## Learning Outcomes

-   K-Nearest Neighbors

-   Bayes' Classifier

-   Logistic Regression

## Classification

The practice of classifying data points into different categories.

# K-Nearest Neighbors

## K-Nearest Neighbors

K-Nearest Neighbors will assign a category to a new data point based on the majority category of its **K** nearest neighbors.

## KNN

```{r}
library(magrittr)
library(tidyverse)
library(palmerpenguins)

penguins %>% sample_n(100) %>%  
  ggplot(aes(x = flipper_length_mm, y = bill_length_mm, color = species)) +
  geom_point() +
  xlab("X") + ylab("Y") +
  guides(color = FALSE) +
  theme_bw()

```

## Distances

The distance between a given point and the training data must be computed. These are the most commonly used distances:

-   Manhattan

-   Euclidean

-   Minkowski

## Manhattan Distance

$$
d(x,y) = \sum_{i=1}^{p} |x_i - y_i|
$$

-   $x$: vector values of individual point

-   $y$: vector values of new point

-   $p$: length of vector

## Euclidean Distance

$$
d(x,y) = \sqrt{\sum_{i=1}^{p} (x_i - y_i)^2}
$$

## Minkowski Distance

$$
d(x,y) = \left( \sum_{i=1}^{p} |x_i - y_i|^w \right)^{\frac{1}{w}}
$$

## Algorithm

Given a training data set, conduct the following steps:

-   Compute the distance between a new data point and every point in the training data set.

-   Choose the $K$ nearest training data points to the new point using the smallest distance.

-   Categorize the new data point based on the majority of category from the $K$ nearest training data points.

# Bayes Classifier

## Bayes Classifier

Bayes Classifier is used to classify a data point to a category $c$

$$
f(\boldsymbol x) = argmax_{c \in C} f(C|\boldsymbol X)
$$

## Probability

$$
f(C = c|\boldsymbol X = x) = \frac{f(\boldsymbol X | C)\pi_c}{f(\boldsymbol X)}
$$

-   $f(\boldsymbol X| C)$: conditional distribution of $\boldsymbol X$

-   $\pi_c$: probability of observing category $C$

-   $f(\boldsymbol X)$: marginal distribution of $\boldsymbol X$

## Distribution of $f(\boldsymbol X|C)$ and $f(\boldsymbol X)$

To apply Bayes classifier, we must specify the form of $f(\boldsymbol X| C)$ and $f(\boldsymbol X)$. Common distributions are:

-   Normal

-   Bernoulli

-   Multinomial

# Logistic Regression

## Logistic Regression

Logistic regression can be used to classify an input of $\boldsymbol X$ to a binary category.

## Multinomial Regression

An extension of logistic regression, where there are more than two categories.

## Fitting a model

Use a glm to fit a model with a Bernoulli or multinomial distribution.

# Example  

## Create the KNN algorithm in R

-   Manhattan

-   Euclidean

-   Minkowski ($w=5$)

Use the `penguins` data set from `palmerpenguins` and categorize the following data: `bill_depth = 19`, `bill_length = 40`, `flipper_length = 185`, and `body_mass = 3345`.
