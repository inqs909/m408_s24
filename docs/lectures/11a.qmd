---
title: "Model Selection"
format:
  revealjs:
    scrollable: true
    navigation-mode: vertical
    controls-layout: bottom-right
    controls-tutorial: true
    incremental: false 
    chalkboard:
      src: chalkboard.json
      storage: chalkboard_pres
      theme: whiteboard
      chalk-width: 4
knitr:
  opts_chunk: 
    echo: true
    eval: true
    message: false
    warnings: false
    comment: "#>" 
    
revealjs-plugins:
  - pointer
  - verticator
  
filters: 
  - reveal-header
  - code-fullscreen

editor: visual
---

## Learning Outcomes

-   Shrinkage Methods

-   Regression Splines

-   Smoothing Splines

-   Generalized Additive Models

# Shrinkage Methods

## Shrinkage Methods

Shrinkage methods are techniques that will reduce a full parameterized model (a high number of predictors) to a lower parameterized model (a smaller number of predictors).

## Ridge Regression

Ridge regression incorporates a shrinkage penalty term to the least squares formula. The shrinkage penalty term will reduce the $\beta$ coefficients towards 0 based on a penalty parameter ($\lambda$)

## Ridge Regression

$$
\sum^n_{i=1}\left(Y_i-\beta_0 +\sum^p_{j=1}X_{ij}\beta_j\right)^2 + \lambda\sum^p_{j=1}\beta_j^2
$$

## LASSO

Least Absolute Shrinkage and Selection Operator (LASSO) is known as a shrinkage method which forces $\beta$ coefficients that do not have a significant predicit power towards and possibly equal to 0.

## LASSO

$$
\sum^n_{i=1}\left(Y_i-\beta_0 +\sum^p_{j=1}X_{ij}\beta_j\right)^2 + \lambda\sum^p_{j=1}|\beta_j|
$$

## Why Ridge or LASSO?

Each method is capable on identifying the optimum MSE for the Bias-Variance trade-off scenario. This will lead to a lower prediction error. The key is to find the optimal penalty parameter. This can be done with a Cross-Validation technique (next lecture).

## Ridge Regression in R

```{r}
#| eval: false
library(glmnet)
glmnet(x,
       y,
       alpha = 0,
       lambda)

```

## LASSO in R

```{r}
#| eval: false
library(glmnet)
glmnet(x,
       y,
       alpha = 1,
       lambda)

```

## Example

```{r}
library(glmnet)
library(tidyverse)
library(palmerpenguins)
penguins <- penguins |> drop_na()
mod <- penguins |> model.matrix(~ flipper_length_mm + bill_depth_mm + bill_length_mm - 1,
  data = _) # Must include -1 to remove intercept, needed for glmnet
ridge_mod <- glmnet(x = mod, 
                    y = penguins$body_mass_g,
                    alpha = 0,
                    lambda = 1.3) # lambda was chosen at random
coef(ridge_mod)[,1]
```

## Example

```{r}
lasso_mod <- glmnet(x = mod, 
                    y = penguins$body_mass_g,
                    alpha = 1,
                    lambda = 1.3) # lambda was chosen at random
coef(lasso_mod)[,1]
```

# Nonparametric Regression

## Nonparametric Regression

# Regression Splines

# Smoothing Splines

# Generalized Additive Models
