---
title: "Support Vector Machines"
format:
  revealjs:
    scrollable: true
    navigation-mode: vertical
    controls-layout: bottom-right
    controls-tutorial: true
    incremental: false 
    chalkboard:
      src: chalkboard.json
      storage: chalkboard_pres
      theme: whiteboard
      chalk-width: 4
knitr:
  opts_chunk: 
    echo: true
    eval: true
    message: false
    warnings: false
    comment: "#>" 
    
revealjs-plugins:
  - pointer
  - verticator
  
filters: 
  - reveal-header
  - code-fullscreen

editor: visual
---

## Learning Outcomes

-   Maximal Margin Classifier
-   Support Vector Classifier
-   Support Vector Machines

# Maximal Margin Classifier

## Motivating Example

```{r}
#| echo: false
#| eval: true
library(palmerpenguins)
library(tidyverse)
library(magrittr)
penguins %<>% mutate(gentoo = ifelse(species == "Gentoo", "A", "B")) %>%  drop_na()
penguins %>% ggplot(aes(x = bill_depth_mm, y = flipper_length_mm, color = gentoo)) +
  geom_point() + theme_bw() + xlab("X") + ylab("Y") + theme(legend.position = "none") 
```

## Maximal Margin Classifier

The **Maximal Margin Classifier** will impose a hyperplane on a graph that will classify the data given a vector of predictor variables.

## Hyperplane

Given a p-dimensional space, a hyperplane is flat affine subspace of p-1 dimensions. It is mathematically defined as:

$$
\beta_0 + \beta_1X_1 + \beta_2 X_2 + \cdots+\beta_pX_p = 0
$$

## Hyperplane

![](https://images.deepai.org/glossary-terms/3bb86574825445cba73a67222b744648/hyperplane.png){fig-align="center"}

## Constructing the Hyperplane

A hyperplane is constructed by maximizing the margin $M$ of the data points that are farthest from the theoretical margin. The data points that define the outer edge of the margins are known as support vectors.

```{r}
#| echo: false
#| eval: true
library(palmerpenguins)
library(tidyverse)
library(magrittr)
penguins %<>% mutate(gentoo = ifelse(species == "Gentoo", "A", "B")) %>%  drop_na()
penguins %>% ggplot(aes(x = bill_depth_mm, y = flipper_length_mm, color = gentoo)) +
  geom_point() + theme_bw() + xlab("X") + ylab("Y") + theme(legend.position = "none") 
```

## Optimization Problem

-   $\overset{\mathrm{maximize}}{\tiny\beta_0, \beta_1,\ldots,\beta_p,M}\ \large M$

-   subject to $\sum^p_{j=1}\beta_j^2 = 1$

-   $y_i(\beta_0 + \beta_1X_{1i} + \beta_2 X_{2i} + \cdots+\beta_pX_{pi})\geq M \ \forall \ i=1,\ldots,n$

## Maximal Margin Classifier

![](https://bookdown.org/mpfoley1973/data-sci/images/svm_mmc.png){fig-align="center"}

# Support Vector Classifier

## Support Vector Classifier

Maximal Margin Classifiers have one fatal defect, the data points must be completely on one side of the margin. This does not allow room for error.

::: fragment
A Support Vector Classifier allows for data points to be misclassified if need be.
:::

::: fragment
It achieves this by implementing a Cost mechanism, denoted as $C$, to account for any errors for data points.
:::

## Support Vector Classifiers

![](https://www.surveypractice.org/article/2715-using-support-vector-machines-for-survey-research/attachment/9153.png){fig-align="center"}

## Optimization Problem

-   $\overset{\mathrm{maximize}}{\tiny\beta_0, \beta_1,\ldots,\beta_p,\epsilon_1,\ldots, \epsilon_n,M}\ \large M$

-   subject to $\sum^p_{j=1}\beta_j^2 = 1$

-   $y_i(\beta_0 + \beta_1X_{1i} + \beta_2 X_{2i} + \cdots+\beta_pX_{pi})\geq M (1-\epsilon_i) \ \forall \ i=1,\ldots,n$

-   $\epsilon_i\geq 0$

-   $\sum^n_{i=1} \epsilon_i \leq C$

## Budget C

The tuning parameter $C$ is known as the budget parameter for error. When the data point is on the correct side of the margin, then it has an error of $0$. When a data point in on the wrong side the margin, it has a bit of error. When the data point is on the opposite side of the hyperplane, then it has an error greater than $1$. This is allowed as long as the sum of errors are less than or equal to $C$.

## R Code: Support Vector Classifier

::: panel-tabset
### Simulation

```{r}
x <- matrix ( rnorm (20 * 2) , ncol = 2)
y <- c( rep (-1, 10) , rep (1, 10) )
x[y == 1, ] <- x[y == 1, ] + 1
plot (x, col = (3 - y))
```

### R Code

```{r}
dat <- data.frame (x = x, y = as.factor(y))
library(e1071)
svmfit <- svm (y ~ ., data = dat , kernel = "linear",
cost = 10, scale = FALSE )
plot(svmfit, dat)
```

### Support Vectors

```{r}
svmfit$index
```

### Summary

```{r}
summary(svmfit)
```
:::

## Cross-Validation Approach for C

::: panel-tabset
### R Code

```{r}
set.seed(2434)
tune.out <- tune(svm, y ~ ., data = dat, kernel = "linear",
                 ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
```

### Summary

```{r}
summary(tune.out)
```

### Best Model

```{r}
bestmod <- tune.out$best.model
summary(bestmod)
```
:::

## Prediction

::: panel-tabset
### Set Up

```{r}
xtest <- matrix(rnorm(20 * 2), ncol = 2)
ytest <- sample(c(-1, 1), 20, rep = TRUE)
xtest[ytest == 1,] <- xtest[ytest == 1, ] + 1
testdat <- data.frame (x = xtest , y = as.factor(ytest))
```

### Prediction

```{r}
ypred <- predict(bestmod, testdat)
table(pred = ypred, truth = testdat$y)
```
:::

# Support Vector Machines

## Motivating Example

![](https://www.surveypractice.org/article/2715-using-support-vector-machines-for-survey-research/attachment/9153.png){fig-align="center"}

## Motivating Example

![](https://i.stack.imgur.com/shjDw.png){fig-align="center"}

## Support Vector Machines

A Support Vector Machine will create a nonlinear boundary instead of a line.

::: fragment
It incorporates a kernel function that will compute the similarities between two support vectors.
:::

::: fragment
The kernel function can be loosely claimed how the data is modeled.
:::

## Nonlinear Boundary

![](https://miro.medium.com/v2/resize:fit:982/1*J0k7TxTLoL5ZG-Hq6v34Jg.png){fig-align="center"}

## Nonlinear Boundary

![](https://s3.amazonaws.com/production.scholastica/public/attachments/b275a94b-e5ba-4e3b-8bbe-5e3235e95892/large/image4.png){fig-align="center"}

## Support Vector Machines Kernels

-   Linear

-   Polynomial

-   Radial

## R Code: SVM

::: panel-tabset
### Data

```{r}
set.seed(1)
x <- matrix(rnorm(200 * 2), ncol = 2)
x[1:100, ] <- x[1:100, ] + 2
x[101:150, ] <- x[101:150 , ] - 2
y <- c(rep(1, 150), rep(2, 50))
dat <- data.frame (x = x, y = as.factor(y))
```

### Plot

```{r}
plot(x,col=y)
```

### SVM

```{r}
library(e1071)
train <- sample(200, 100)
svmfit <- svm(y ~ ., data = dat[train, ], kernel = "radial",
gamma = 1, cost = 1)
plot(svmfit, dat[train, ])
```
:::
