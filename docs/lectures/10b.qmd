---
title: "Introduction to Statistical Learning"
format:
  revealjs:
    scrollable: true
    navigation-mode: vertical
    controls-layout: bottom-right
    controls-tutorial: true
    incremental: false 
    chalkboard:
      src: chalkboard.json
      storage: chalkboard_pres
      theme: whiteboard
      chalk-width: 4
knitr:
  opts_chunk: 
    echo: true
    eval: true
    message: false
    warnings: false
    comment: "#>" 
    
revealjs-plugins:
  - pointer
  - verticator
  
filters: 
  - reveal-header
  - code-fullscreen

editor: visual
---

## Learning Outcomes

-   Introduction to Statistical Learning

-   Classification vs Regression

-   Supervised vs Unsupervised Machine Learning

-   Model Adequacy

## Announcements

**Graduate Seminar:**

**Survival Analysis: The Life and Death of Statistics**

**6-7 PM Del Norte 1550**

# Introduction to Statistical Learning

## Introduction to Statistical Learning

**What is Statistical Learning?**

::: {#.fragment}
Statistical learning is the task of predicting an outcome of interest given a set of predictor variables.
:::

## Statistical Learning Model

$$
Y = f(\boldsymbol X) + \varepsilon
$$

-   $Y$: Outcome variable

-   $f(\cdot)$: systematic component explaining $Y$

-   $\boldsymbol X$: vector of predictor variables

-   $\varepsilon$: error term

## Modeling $f(\cdot)$: Parametric

-   Linear Models

-   Generalized Linear Models (GLM)

## Modeling $f(\cdot)$: Nonparametric

-   Generalized Additive Models

-   Local-Linear Models

-   Smoothing Splines

## Prediction

-   Statistical Learning is only concerned with an accurate $Y$

-   $f(\cdot)$ is considered a black box

-   We will not know how $\boldsymbol X$ explains $Y$

-   We choose flexible (nonparametric) models

## Model Interpretability

-   With a focus on prediction, model interpretability declines

-   We will not know how changes in $\boldsymbol X$ will affect $Y$

# Classification vs Regression

## Regression

Regression in statistical learning terms indicates predicting a continuous random variable.

<div>

What are the methods that we learned to model continuous random variables?

</div>

## Example

```{r}
#| include: false

library(tidyverse)
library(mgcv)
n  <- 1e3
dat <- data.frame("x1" = rnorm(n), "x2" = rnorm(n), "x3" = rnorm(n))
dat$y <- with(dat, sin(x1) + 0.5*x2^2 + 0.2*x3 + pmax(x2, 0.2) * rnorm(n))
b <- gam(y ~ s(x1) + s(x2) + x3, data = dat, method = "REML")

p_obj <- plot(b, residuals = TRUE)
p_obj <- p_obj[[1]] # just one smooth so select the first component
sm_df <- as.data.frame(p_obj[c("x", "se", "fit")])
data_df <- as.data.frame(p_obj[c("raw", "p.resid")])
```

::: panel-tabset
## Scatter Plot

```{r}
#| echo: false
ggplot(sm_df, aes(x = x, y = fit)) +
  geom_point(data = data_df, mapping = aes(x = raw, y = p.resid)) +
  # geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
  #             alpha = 0.3) +
  # geom_line() +
  # labs(x = p_obj$xlab, y = p_obj$ylab) + 
  # geom_smooth(method = "lm") +
  xlab("X") + ylab("Y") +
  theme_bw()
```

## Regression

```{r}
#| echo: false
ggplot(sm_df, aes(x = x, y = fit)) +
  geom_point(data = data_df, mapping = aes(x = raw, y = p.resid)) +
  # geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
  #             alpha = 0.3) +
  # geom_line() +
  # labs(x = p_obj$xlab, y = p_obj$ylab) + 
  geom_smooth(method = "lm") +
  xlab("X") + ylab("Y") +
  theme_bw()
```

## GAM

```{r}
#| echo: false
ggplot(sm_df, aes(x = x, y = fit)) +
  geom_point(data = data_df, mapping = aes(x = raw, y = p.resid)) +
  geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
              alpha = 0.3) +
  geom_line() +
  labs(x = p_obj$xlab, y = p_obj$ylab) +
  # geom_smooth(method = "lm") +
  xlab("X") + ylab("Y") +
  theme_bw()
```

## Combined

```{r}
#| echo: false
ggplot(sm_df, aes(x = x, y = fit)) +
  geom_point(data = data_df, mapping = aes(x = raw, y = p.resid)) +
  geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
              alpha = 0.3) +
  geom_line() +
  labs(x = p_obj$xlab, y = p_obj$ylab) +
  geom_smooth(method = "lm") +
  xlab("X") + ylab("Y") +
  theme_bw()
```
:::

## Classification

Classification in statistical learning terms indicates predicting a categorical random variable.

<div>

What are the methods that we learned to model categorical random variables?

</div>

## Example

::: panel-tabset
## Data

```{r}
#| echo: false
ggplot(data = iris, 
       aes(x = Petal.Length, y = Petal.Width, col = Species)) +
  geom_point() + theme_bw()
```

## KNN

```{r}
#| echo: false
library(class)
set.seed(1236894)
setosa<- rbind(iris[iris$Species=="setosa",])
versicolor<- rbind(iris[iris$Species=="versicolor",])
virginica<- rbind(iris[iris$Species=="virginica",])


ind <- sample(1:nrow(setosa), nrow(setosa)*0.6)

iris.train<- rbind(setosa[ind,], versicolor[ind,], virginica[ind,])
iris.test<- rbind(setosa[-ind,], versicolor[-ind,], virginica[-ind,])
iris_pred <- knn(train = iris.train[,1:4], test = iris.test[,1:4], cl = iris.train$Species, k=5)
table(iris.test$Species,iris_pred)
```

## SVM

```{r}
#| echo: false

library(e1071)
svm_model <- svm(Species ~ ., data=iris,
                 kernel="radial") #linear/polynomial/sigmoid
pred = predict(svm_model,iris)
tab = table(Predicted=pred, Actual = iris$Species)
tab
```

## NN

```{r}
#| echo: false
library(neuralnet)
data(iris)
iris$setosa <- iris$Species=="setosa"
iris$virginica <- iris$Species == "virginica"
iris$versicolor <- iris$Species == "versicolor"
iris.train.idx <- sample(x = nrow(iris), size = nrow(iris)*0.5)
iris.train <- iris[iris.train.idx,]
iris.valid <- iris[-iris.train.idx,]
library(neuralnet)
iris.net <- neuralnet(setosa+versicolor+virginica ~ 
                      Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, 
                      data=iris.train, hidden=c(10,10), rep = 5, err.fct = "ce", 
                      linear.output = F, lifesign = "minimal", stepmax = 1000000,
                      threshold = 0.001)
iris.prediction <- compute(iris.net, iris.valid[-5:-8])
idx <- apply(iris.prediction$net.result, 1, which.max)
predicted <- c('setosa', 'versicolor', 'virginica')[idx]
table(predicted, iris.valid$Species)
```
:::

# Supervised vs Unsupervised Machine Learning

## Machine Learning

Machine learning is a set of methods used for predicting and classifying data. Several statistical methods are considered machine learning techniques.

::: {#.fragement}
#### Common Methods

-   Regression

-   Mixed-Effects

-   Nonparametric Regression

-   Neural Networks

-   Tree-based methods

-   Bayesian Methods
:::

## Training Data

Training Data is the data set used to construct a model.

## Supervised

Supervised Machine Learning techniques are techniques where the training data contains the outcome.

## Unsupervised

Unsupervised Machine Learning techniques are techniques where the training data **does not** contains the outcome.

# Model Adequacy

## Quality of Fit: Regression

$$
MSE = \frac{1}{n}\sum^n_{i=1}\{y_i - f(\boldsymbol x_i)\}^2
$$

## Quality of Fit: Classification

$$
ER = \frac{1}{n}\sum^n_{i=1}I(y_i \ne \hat y_i)
$$

## Bias-Variance Tradeoff

```{r}
#| echo: false
ggplot(sm_df, aes(x = x, y = fit)) +
  geom_point(data = data_df, mapping = aes(x = raw, y = p.resid)) +
  geom_ribbon(aes(ymin = fit - se, ymax = fit + se, y = NULL),
              alpha = 0.3) +
  geom_line() +
  labs(x = p_obj$xlab, y = p_obj$ylab) +
  geom_smooth(method = "lm") +
  xlab("X") + ylab("Y") +
  theme_bw()
```

## Bias-Variance Tradeoff
