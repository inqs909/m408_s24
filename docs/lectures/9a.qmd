---
title: "Generalized Linear Models"
subtitle: "Estimation"
format:
  revealjs:
    scrollable: true
    navigation-mode: vertical
    controls-layout: bottom-right
    controls-tutorial: true
    incremental: false 
    chalkboard:
      src: chalkboard.json
      storage: chalkboard_pres
      theme: whiteboard
      chalk-width: 4
knitr:
  opts_chunk: 
    echo: false
    eval: false
    message: false
    warnings: false
    comment: "#>" 
    
revealjs-plugins:
  - pointer
  - verticator
  
filters: 
  - reveal-header
  - code-fullscreen

editor: visual
---

## Learning Outcomes

-   Estimation Procedures

    -   Regression Coefficients

    -   Dispersion Parameter

-   Newton-Raphson Algorithm

# Estimating: $\boldsymbol \beta$

## Estimating $\boldsymbol\beta$

To obtain the estimates of $\boldsymbol \beta$ we can use the maximum log-likelihood approach to obtain $\hat{\boldsymbol\beta}$.

$$
L(\boldsymbol \beta) =  \prod^n_{i=1}f\left(y_i|\boldsymbol X_i;\boldsymbol \beta,\phi\right)
$$

## Maximum Likelihood Approach

$$
\ell(\boldsymbol \beta) =  \sum^n_{i=1}\log\left\{f\left(y_i|\boldsymbol X_i;\boldsymbol \beta,\phi\right)\right\}
$$

## Numerical Approaches

-   Newton-Rhapson Algorithm

-   Fisher-Scoring Algorithm

-   Nelder-Mead

-   BFGS

# Estimating: $\phi$

## Estimating $\phi$

Depending on the random variable, the dispersion parameter will need to be estimated to conduct inference procedures. There are 4 methods to estimate the dispersion parameter:

-   Maximum Likelihood

-   Maximum (Modified) Profile Likelihood Approach

-   Mean Deviance Estimator

-   Pearson Estimator

## Maximum Likelihood Approach

$$
\ell(\phi) =  \sum^n_{i=1}\log\left\{f\left(y_i|\boldsymbol X_i;\boldsymbol \beta,\phi\right)\right\}
$$

## Maximum (Modified) Profile Likelihood Approach

$$
\ell_p(\phi) = \frac{p}{2}\log \phi + \sum^n_{i=1}\log\left\{f\left(y_i|\boldsymbol X_i;\hat{\boldsymbol \beta},\phi\right)\right\}
$$

## Mean Deviance Estimator

$$
\tilde \phi = \frac{D(y,\hat\mu)}{n-p}
$$

-   $D(y,\hat\mu)=2\sum^n_{i=1}\left\{t(y,y) - t(y,\mu) \right\}$

-   $t(y,\mu)=y\theta-\kappa(\theta)$

-   $p$: number of regression coefficients

## Pearson Estimator

$$
\bar \phi = \frac{\Lambda^2}{n-p}
$$

-   $\Lambda^2=\sum^n_{i=1}\frac{y_i-\hat\mu_i}{V(\hat\mu_i)}$

-   $\hat \mu_i = g^{-1}(\hat\beta_0 + \sum^n_{j=1}{X_{ij}\hat\beta_j})$

-   $V(\hat\mu_i)=\frac{d^2\kappa(\hat\theta_i)}{d\theta_i^2}$

# Newton-Raphson Algorithm
