---
title: "Linear Regression"
format:
  revealjs:
    scrollable: true
    navigation-mode: vertical
    controls-layout: bottom-right
    controls-tutorial: true
    incremental: false 
    chalkboard:
      src: chalkboard.json
      storage: chalkboard_pres
      theme: whiteboard
      chalk-width: 4
knitr:
  opts_chunk: 
    echo: false
    eval: true
    message: false
    warnings: false
    comment: "#>" 

editor: visual
---

```{r}
#| include: false
#| eval: true
#| label: setup

library(DT)
library(tidyverse)
library(GLMsData)
library(broom)
library(palmerpenguins)
library(magrittr)

x <- rnorm(50, 4)
y <- 3 + 2 * x + rnorm(50, sd = 2)
df <- tibble(x, y)



```

## Learning Objectives

-   Associations

-   Linear Regression

-   Conducting it in R

# Associations

## Types of Associations

-   Categorical vs Continuous

-   Categorical vs Categorical

-   Continuous vs Continuous

## Categorical vs Continuous

Describing the relationship between categorical and continuous variables can be done by stratifying by the categorical variable and finding the mean or median, or conducting a statistical test:

-   t-tests

-   ANOVA

Measuring the association between species and flipper length from `palmerpenguins`:

```{r}
#| echo: true
# Stratification
penguins %>% drop_na %>%
  group_by(species) %>% 
  summarise(mean = mean(flipper_length_mm),
            median = median(flipper_length_mm))
# ANOVA
species_aov <- aov(flipper_length_mm ~ species, penguins)
tidy(species_aov)
```

## Categorical vs Categorical

Comparing 2 categorical variables can be done by computing proportions or conducting a chi-square test. Below is an example comparing

```{r}
#| echo: true

# Count and Proportions
penguins %>% group_by(species, island) %>% 
  summarise(count = n(), proportions = n() / nrow(penguins))

# Chi-Square Test
penguins %$% chisq.test(species, island) 

```

## Continuous vs Continuous

To understand the relationship between two continuous random variables, we can use the following:

-   Correlation

-   Linear Regression

## Correlation

Correlations measures the association between 2 continuous random variables.

-   Pearson's Correlation

-   Spearman's Correlation

-   Kendall's Tau

```{r}
#| echo: true
#| warning: false
#| message: false

# Pearson's Correlation
penguins %>% drop_na %$% cor.test(body_mass_g, flipper_length_mm)

# Spearman's Correlation 
penguins %>% drop_na %$% cor.test(body_mass_g, flipper_length_mm, method = "spearman")

# Kendall's Tau
penguins %>% drop_na %$% cor.test(body_mass_g, flipper_length_mm, method = "kendall")

```

## Scatter Plot

```{r}
ggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm)) + geom_point() + theme_bw()
```

# Linear Regression

## Linear Regression

Linear regression is used to model the association between a set of predictor variables (x's) and an outcome variable (y). Linear regression will fit a line that best describes the data points.

## Scatter Plot

```{r}
ggplot(df, aes(x = x, y = y)) + geom_point() + theme_bw()
```

## Simple Linear Regression

Simple linear regression will model the association between one predictor variable and an outcome:

$$
Y = \beta_0 + \beta_1 X + \epsilon
$$

-   $\beta_0$: Intercept term

-   $\beta_1$: Slope term

-   $\epsilon\sim N(0,\sigma^2)$

## Estimated Model

$$
\hat Y = \hat \beta_0 + \hat \beta_1 X
$$

# R Examples

## R Formulas

In R, we create a formula using the `tilde`

Outcome Variable `tilde` Independent Variables

```{r}
#| echo: true
#| eval: false
ooutcome_variable_name ~ independent_variable_name
```

## R Example

Applying a linear regression model to `body_mass_g` (Predictor) and `flipper_length_mm` (Outcome) from `penguins`:

::: panel-tabset
## R Output

```{r}
#| echo: true

# summary(lm(flipper_length_mm ~ body_mass_g, data = penguins))

# OR

penguins %$% lm(flipper_length_mm ~ body_mass_g) %>% summary
```

## Scatter Plot

```{r}
#| echo: true
#| warning: false
#| message: false
ggplot(penguins, aes(y = flipper_length_mm, x = body_mass_g)) + geom_point() + 
  geom_smooth(method = "lm") +
  theme_bw()
```

### Interpretation

$$
\hat y = 136.73 + 0.015 x
$$
:::

## Simulation Example

## Generate X Values

Simulate 250 independent values (x) from $N(3, 1)$ and plot a histogram

```{r}
#| echo: true
x <- rnorm(205, 3, 1)
```

## Generate Y Values

Create a new variable with the following formula:

$$
Y_i = 3 X_i + 20
$$

Create a Scatter Plot

## Generate Error Term

Generate $\epsilon_i\sim N(0, 2)$

Create a histogram

## Add error term

Create final form of $Y_i$:

$$
Y_i = 3 X_i + 20 + \epsilon_i
$$

Create Scatter Plot

## Fit Model

## Simulation Study

To confirm that `lm` works, repeat the process 100 times and obtain the average $\beta$ estimates:

1.  Generate 250 X values
2.  Create Y Value
3.  Add error term
4.  Fit Model
5.  Store Values
