[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Term: Spring 2024\nInstructor: Isaac Quintanilla Salinas\nContact: isaac.qs@csuci.edu\nOffice Location: BTE 2840\nOffice Hours:\n\n\n\nOH\nCourse\nTea Time\nR OH\n\n\n\n\nDay\nMW\nTBD\nTuesday\n\n\nLocation\nBTE 2840\nLook for Group\nBTE 2840\n\n\nTime\n3-4 PM\nTBD\n10 AM - 12 PM\n\n\n\nOr by Zoom appointment: calendly.com/isaac-qs/office-hours\nLecture: Monday and Wednesday 1:30-3:00 PM in BT 1462\nCourse Website: m408.inqs.info AND Canvas\n\n\n\nIntroduction to data management, regression, and machine learning. Bayesian methods, multivariate data, multivariate normal distribution, multivariate regression, principal components, factor, canonical correlation, discriminant analyses, and clustering. Extensive use of appropriate statistical and programming software.\n\n\n\n\nPrepare students for advanced courses in data-management, machine learning, and statistics, by providing the necessary foundation and context\nEnable students to start careers as data scientists by providing experience working with real-world data, tools, and techniques\nEmpower students to apply computational and inferential thinking to address real-world problems\n\n\n\n\n\nAn Introduction to Statistical Learning (SL)\n\nGareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani\nAvailable to Download from the Broome Library\nwww.statlearning.com\n\nStatistical Computing (SC)\n\nIsaac Quintanilla Salinas\nwww.inqs.info/stat_comp\nhypothes.is/groups/xMmDdj2A/m408\n\n\n\n\n\nFor this course, we will use R, Quarto, and RStudio. Please download and install on your computer.\n\nR is a free statistical software program that is available for download at: www.r-project.org.\nR Markdown is a scientific documentation known as an RMD file that can be used to provide reproducible code and documents.\nRStudio provides free and open source tools for your data analysis in R: posit.co/downloads\ncsucistats is a developmental R package that will contain RMD templates to submit assignments for class: csucistats\n\n\n\n\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n25%\n\n\nExam 1\n25%\n\n\nExam 2\n25%\n\n\nExam 3\n25%\n\n\n\nAt the end of the quarter, course grades will be assigned according to the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA+\n98 – 100\nB+\n87 – &lt;90\nC+\n77 – &lt;80\nD+\n67 – &lt;70\n\n\n\n\nA\n93 – &lt;98\nB\n83 – &lt;87\nC\n73 – &lt;77\nD\n63 – &lt;67\nF\n&lt; 60\n\n\nA–\n90 - &lt;93\nB-\n80 – &lt;83\nC–\n70 – &lt;73\nD–\n60 – &lt;63\n\n\n\n\n\n\n\nHomework will be assigned on a regular basis and posted on m408.inqs.info/hw.html and Canvas. The homework is to help you practice the concepts learned in lecture and to help you study. You must turn in your own individual homework and show your understanding of the material. At the end of the semester, the two lowest homework grades will be dropped. Late work will be accepted, but with a 10 point penalty. The last day late work will be accepted is on 5/14/2023 at 11:59 PM.\n\n\n\nThere will be three exams. Exam #1 will most likely be during the 6th week of the semester. Exam #2 will most likely be during the 11th week of the semester. Exam #3 will be on finals week. While the exams are not considered cumulative, the material builds on each other. Developing a strong understanding of the material through out the course is important for your success. At the end of the semester, your lowest exam grade will be replaced by your median average exam grade. This course will operate under a zero-tolerance policy. Communication about the exam, sharing material, or not following directions given will be subject to the University’s academic integrity policy.\n\n\n\nThere will be 6 extra credit opportunities worth a total of 10% of your overall grade. (There are no make-ups for missed extra credit assignments!) More information will be provided on the extra credit assignments on a later date. Information on the extra credit can be found here.\n\n\n\n\nThe following outline may be subject to change. Any changes will be announced in class.\n\n\n\n\n\n\n\n\n\nWeek\nTopic\nReading\nAssignment/Exam\n\n\n\n\n1/22-1/26\nIntro to Course/Intro to R/Notebooks\nSC: Ch 1\n\n\n\n1/29-2/2\nControl Flow\nSC: Ch 2\nHW #1\n\n\n2/5-2/9\nControl Flow/Functional Programming\nSC: Ch 13\nHW #2\n\n\n2/12-2/16\nFunctional Programming\nSC: Ch 15\nHW #3\n\n\n2/19-2/23\nData Manipulation\nSC: Ch 3\nExam #1\n\n\n2/26-3/1\nIntroduction to Statistical Learning\nSL: Ch 2\nHW #4\n\n\n3/4-3/8\nLinear Regression\nSL: Ch 3\nHW #5\n\n\n3/11-3/15\nMultivariable Linear Regression\nSL: Ch 3\n\n\n\n3/18-3/22\nSpring Break\n\n\n\n\n3/25-3/29\nClassification\nSL: Ch 4\nHW #6\n\n\n4/1-4/5\nResampling Methods\nSL: Ch 5\nExam #2\n\n\n4/8-4/12\nModel Selection\nSL: Ch 6\nHW # 7\n\n\n4/15-4/19\nNonparametric Regression\nSL: Ch 7\nHW # 8\n\n\n4/22-4/26\nTree-Based Methods\nSL: Ch 8\nHW # 9\n\n\n4/29-5/3\nSupport Vector Machines\nSL: Ch 9\nHW # 10\n\n\n5/6-5/10\nDeep Learning\nSL: Ch 10\n\n\n\n5/13-5/17\nExam 3\n\n\n\n\n\n\n\n\n\nAcademic Honesty:\nPlease conduct yourself with honesty and integrity. Do not submit others’ work as your own. For assignments and quizzes that allow you to work with a group, only put your name on what the group submits if you genuinely contributed to the work. Work completely independently on exams, using only the materials that are indicated as allowed. Failure to observe academic honesty results in substantial penalties that can include failing the course.\nDisabilities:\nIf you are a student with a disability requesting reasonable accommodations in this course, you need to contact Disability Accommodations and Support Services (DASS) located on the second floor of Arroyo Hall, via email accommodations@csuci.edu or call 805-437-3331. All requests for reasonable accommodations require registration with DASS in advance of need: https://www.csuci.edu/dass/students/apply-for-services.htm. Faculty, students and DASS will work together regarding classroom accommodations. You are encouraged to discuss approved.\nEmergency Procedure Notice to Students:\nCSUCI is following guidelines and public orders from the California Department of Public Health and Ventura County Public Health for the COVID-19 pandemic as it pertains to CSUCI students, employees and visitors on the campus. Students are expected to adhere to all health and safety requirements as noted on the University’s Spring 2023 Semester website or they may be subject to removal from the classroom.\n\n\n\n\nThe California Faculty Association (the labor union of Lecturers, Professors, Coaches, Counselors, and Librarians across the 23 CSU campuses) is in a difficult contract dispute with California State University management. We will be going on strike from January 22 – January 26 unless management gives us a fair contract that recognizes the dignity of CSU faculty, staff, and students. Our working conditions are student learning conditions; we seek to protect both. For further information go to: www.calfac.org/strike."
  },
  {
    "objectID": "syllabus.html#math-408-advanced-data-analysis",
    "href": "syllabus.html#math-408-advanced-data-analysis",
    "title": "Syllabus",
    "section": "",
    "text": "Term: Spring 2024\nInstructor: Isaac Quintanilla Salinas\nContact: isaac.qs@csuci.edu\nOffice Location: BTE 2840\nOffice Hours:\n\n\n\nOH\nCourse\nTea Time\nR OH\n\n\n\n\nDay\nMW\nTBD\nTuesday\n\n\nLocation\nBTE 2840\nLook for Group\nBTE 2840\n\n\nTime\n3-4 PM\nTBD\n10 AM - 12 PM\n\n\n\nOr by Zoom appointment: calendly.com/isaac-qs/office-hours\nLecture: Monday and Wednesday 1:30-3:00 PM in BT 1462\nCourse Website: m408.inqs.info AND Canvas\n\n\n\nIntroduction to data management, regression, and machine learning. Bayesian methods, multivariate data, multivariate normal distribution, multivariate regression, principal components, factor, canonical correlation, discriminant analyses, and clustering. Extensive use of appropriate statistical and programming software.\n\n\n\n\nPrepare students for advanced courses in data-management, machine learning, and statistics, by providing the necessary foundation and context\nEnable students to start careers as data scientists by providing experience working with real-world data, tools, and techniques\nEmpower students to apply computational and inferential thinking to address real-world problems\n\n\n\n\n\nAn Introduction to Statistical Learning (SL)\n\nGareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani\nAvailable to Download from the Broome Library\nwww.statlearning.com\n\nStatistical Computing (SC)\n\nIsaac Quintanilla Salinas\nwww.inqs.info/stat_comp\nhypothes.is/groups/xMmDdj2A/m408\n\n\n\n\n\nFor this course, we will use R, Quarto, and RStudio. Please download and install on your computer.\n\nR is a free statistical software program that is available for download at: www.r-project.org.\nR Markdown is a scientific documentation known as an RMD file that can be used to provide reproducible code and documents.\nRStudio provides free and open source tools for your data analysis in R: posit.co/downloads\ncsucistats is a developmental R package that will contain RMD templates to submit assignments for class: csucistats\n\n\n\n\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n25%\n\n\nExam 1\n25%\n\n\nExam 2\n25%\n\n\nExam 3\n25%\n\n\n\nAt the end of the quarter, course grades will be assigned according to the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA+\n98 – 100\nB+\n87 – &lt;90\nC+\n77 – &lt;80\nD+\n67 – &lt;70\n\n\n\n\nA\n93 – &lt;98\nB\n83 – &lt;87\nC\n73 – &lt;77\nD\n63 – &lt;67\nF\n&lt; 60\n\n\nA–\n90 - &lt;93\nB-\n80 – &lt;83\nC–\n70 – &lt;73\nD–\n60 – &lt;63\n\n\n\n\n\n\n\nHomework will be assigned on a regular basis and posted on m408.inqs.info/hw.html and Canvas. The homework is to help you practice the concepts learned in lecture and to help you study. You must turn in your own individual homework and show your understanding of the material. At the end of the semester, the two lowest homework grades will be dropped. Late work will be accepted, but with a 10 point penalty. The last day late work will be accepted is on 5/14/2023 at 11:59 PM.\n\n\n\nThere will be three exams. Exam #1 will most likely be during the 6th week of the semester. Exam #2 will most likely be during the 11th week of the semester. Exam #3 will be on finals week. While the exams are not considered cumulative, the material builds on each other. Developing a strong understanding of the material through out the course is important for your success. At the end of the semester, your lowest exam grade will be replaced by your median average exam grade. This course will operate under a zero-tolerance policy. Communication about the exam, sharing material, or not following directions given will be subject to the University’s academic integrity policy.\n\n\n\nThere will be 6 extra credit opportunities worth a total of 10% of your overall grade. (There are no make-ups for missed extra credit assignments!) More information will be provided on the extra credit assignments on a later date. Information on the extra credit can be found here.\n\n\n\n\nThe following outline may be subject to change. Any changes will be announced in class.\n\n\n\n\n\n\n\n\n\nWeek\nTopic\nReading\nAssignment/Exam\n\n\n\n\n1/22-1/26\nIntro to Course/Intro to R/Notebooks\nSC: Ch 1\n\n\n\n1/29-2/2\nControl Flow\nSC: Ch 2\nHW #1\n\n\n2/5-2/9\nControl Flow/Functional Programming\nSC: Ch 13\nHW #2\n\n\n2/12-2/16\nFunctional Programming\nSC: Ch 15\nHW #3\n\n\n2/19-2/23\nData Manipulation\nSC: Ch 3\nExam #1\n\n\n2/26-3/1\nIntroduction to Statistical Learning\nSL: Ch 2\nHW #4\n\n\n3/4-3/8\nLinear Regression\nSL: Ch 3\nHW #5\n\n\n3/11-3/15\nMultivariable Linear Regression\nSL: Ch 3\n\n\n\n3/18-3/22\nSpring Break\n\n\n\n\n3/25-3/29\nClassification\nSL: Ch 4\nHW #6\n\n\n4/1-4/5\nResampling Methods\nSL: Ch 5\nExam #2\n\n\n4/8-4/12\nModel Selection\nSL: Ch 6\nHW # 7\n\n\n4/15-4/19\nNonparametric Regression\nSL: Ch 7\nHW # 8\n\n\n4/22-4/26\nTree-Based Methods\nSL: Ch 8\nHW # 9\n\n\n4/29-5/3\nSupport Vector Machines\nSL: Ch 9\nHW # 10\n\n\n5/6-5/10\nDeep Learning\nSL: Ch 10\n\n\n\n5/13-5/17\nExam 3\n\n\n\n\n\n\n\n\n\nAcademic Honesty:\nPlease conduct yourself with honesty and integrity. Do not submit others’ work as your own. For assignments and quizzes that allow you to work with a group, only put your name on what the group submits if you genuinely contributed to the work. Work completely independently on exams, using only the materials that are indicated as allowed. Failure to observe academic honesty results in substantial penalties that can include failing the course.\nDisabilities:\nIf you are a student with a disability requesting reasonable accommodations in this course, you need to contact Disability Accommodations and Support Services (DASS) located on the second floor of Arroyo Hall, via email accommodations@csuci.edu or call 805-437-3331. All requests for reasonable accommodations require registration with DASS in advance of need: https://www.csuci.edu/dass/students/apply-for-services.htm. Faculty, students and DASS will work together regarding classroom accommodations. You are encouraged to discuss approved.\nEmergency Procedure Notice to Students:\nCSUCI is following guidelines and public orders from the California Department of Public Health and Ventura County Public Health for the COVID-19 pandemic as it pertains to CSUCI students, employees and visitors on the campus. Students are expected to adhere to all health and safety requirements as noted on the University’s Spring 2023 Semester website or they may be subject to removal from the classroom.\n\n\n\n\nThe California Faculty Association (the labor union of Lecturers, Professors, Coaches, Counselors, and Librarians across the 23 CSU campuses) is in a difficult contract dispute with California State University management. We will be going on strike from January 22 – January 26 unless management gives us a fair contract that recognizes the dignity of CSU faculty, staff, and students. Our working conditions are student learning conditions; we seek to protect both. For further information go to: www.calfac.org/strike."
  },
  {
    "objectID": "lectures/lecture_9.html#learning-objectives",
    "href": "lectures/lecture_9.html#learning-objectives",
    "title": "Data Manipulation",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nDirectories/R Projects\nReading/Writing Data\nMerging Data\ndplyr Functions"
  },
  {
    "objectID": "lectures/lecture_9.html#directories",
    "href": "lectures/lecture_9.html#directories",
    "title": "Data Manipulation",
    "section": "Directories",
    "text": "Directories\nDirectories is the file system located on your computer.\nA file path indicates the location of certain files relative to your main (home) folder."
  },
  {
    "objectID": "lectures/lecture_9.html#working-directory",
    "href": "lectures/lecture_9.html#working-directory",
    "title": "Data Manipulation",
    "section": "Working Directory",
    "text": "Working Directory\nThis is the folder where R will save and read all the files when the file path is not specified.\nTo get the current working directory:\n\ngetwd()\n\nTo set the working directory:\n\nsetwd(\"new_file_path\")"
  },
  {
    "objectID": "lectures/lecture_9.html#r-projects",
    "href": "lectures/lecture_9.html#r-projects",
    "title": "Data Manipulation",
    "section": "R Projects",
    "text": "R Projects\nR Projects are ways for RStudio to organize files together for specific"
  },
  {
    "objectID": "lectures/lecture_9.html#read-data",
    "href": "lectures/lecture_9.html#read-data",
    "title": "Data Manipulation",
    "section": "Read Data",
    "text": "Read Data\n\nEasiest way is to have RStudio do it for you\nUse Base R functions\nUse readr package for tabular/text files\nUse readxl package for excel files\nUse haven package to read SAS, SPSS, or Stata files."
  },
  {
    "objectID": "lectures/lecture_9.html#example",
    "href": "lectures/lecture_9.html#example",
    "title": "Data Manipulation",
    "section": "Example",
    "text": "Example\n\ndata1 &lt;- read_csv(\"files/data/data_3_1.csv\")\ndata2 &lt;- read_csv(\"/home/inqs/Repos/M408_S23/data/data_3_2.csv\")"
  },
  {
    "objectID": "lectures/lecture_9.html#example-1",
    "href": "lectures/lecture_9.html#example-1",
    "title": "Data Manipulation",
    "section": "Example",
    "text": "Example\nDownload the following zip file: data\nLoad data data_3_1.csv and data_3_2.csv."
  },
  {
    "objectID": "lectures/lecture_9.html#example-2",
    "href": "lectures/lecture_9.html#example-2",
    "title": "Data Manipulation",
    "section": "Example",
    "text": "Example\nLoad the following data: https://m408.inqs.info/files/data/data_3_3.csv"
  },
  {
    "objectID": "lectures/lecture_9.html#write-data",
    "href": "lectures/lecture_9.html#write-data",
    "title": "Data Manipulation",
    "section": "Write Data",
    "text": "Write Data\nSeveral functions that you can use to write functions from the readr and readxl.\nI recommend using the write_csv function and provide csv files."
  },
  {
    "objectID": "lectures/lecture_9.html#rdata",
    "href": "lectures/lecture_9.html#rdata",
    "title": "Data Manipulation",
    "section": "RData",
    "text": "RData\nRData is the data file specific for R.\nLoad Data\n\nload(\"data.RData\")\n\nWrite Data\n\nsave(RObject, file = \"data.RData\")"
  },
  {
    "objectID": "lectures/lecture_9.html#join",
    "href": "lectures/lecture_9.html#join",
    "title": "Data Manipulation",
    "section": "*_join()",
    "text": "*_join()\n\nThe *_join() functions are used to merge 2 data frames together."
  },
  {
    "objectID": "lectures/lecture_9.html#example-3",
    "href": "lectures/lecture_9.html#example-3",
    "title": "Data Manipulation",
    "section": "Example",
    "text": "Example\nMerge data sets data_3_1.csv and data_3_2.csv using the full_join()"
  },
  {
    "objectID": "lectures/lecture_9.html#mutate",
    "href": "lectures/lecture_9.html#mutate",
    "title": "Data Manipulation",
    "section": "mutate()",
    "text": "mutate()\n\nAdds a new variable to a data frame\nExample:\n\n\nmtcars %&gt;% mutate(log_mpg=log(mpg)) %&gt;% \n  head\n\n#&gt; mutate: new variable 'log_mpg' (double) with 25 unique values and 0% NA\n\n\n#&gt;                    mpg cyl disp  hp drat    wt  qsec vs am gear carb  log_mpg\n#&gt; Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 3.044522\n#&gt; Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 3.044522\n#&gt; Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 3.126761\n#&gt; Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 3.063391\n#&gt; Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 2.928524\n#&gt; Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 2.895912"
  },
  {
    "objectID": "lectures/lecture_9.html#mutate-1",
    "href": "lectures/lecture_9.html#mutate-1",
    "title": "Data Manipulation",
    "section": "mutate()",
    "text": "mutate()\n\nEach argument is a new variable added\nExample:\n\n\nmtcars %&gt;% mutate(log_mpg=log(mpg),log_hp=log(hp)) %&gt;%\n  head\n\n#&gt; mutate: new variable 'log_mpg' (double) with 25 unique values and 0% NA\n\n\n#&gt;         new variable 'log_hp' (double) with 22 unique values and 0% NA\n\n\n#&gt;                    mpg cyl disp  hp drat    wt  qsec vs am gear carb  log_mpg\n#&gt; Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 3.044522\n#&gt; Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 3.044522\n#&gt; Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 3.126761\n#&gt; Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 3.063391\n#&gt; Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 2.928524\n#&gt; Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 2.895912\n#&gt;                     log_hp\n#&gt; Mazda RX4         4.700480\n#&gt; Mazda RX4 Wag     4.700480\n#&gt; Datsun 710        4.532599\n#&gt; Hornet 4 Drive    4.700480\n#&gt; Hornet Sportabout 5.164786\n#&gt; Valiant           4.653960"
  },
  {
    "objectID": "lectures/lecture_9.html#example-4",
    "href": "lectures/lecture_9.html#example-4",
    "title": "Data Manipulation",
    "section": "Example",
    "text": "Example\nUsing the penguins dataset from palmerpenguins, create a new variable that is the ln of flipper_length_mm."
  },
  {
    "objectID": "lectures/lecture_9.html#select",
    "href": "lectures/lecture_9.html#select",
    "title": "Data Manipulation",
    "section": "select()",
    "text": "select()\n-This selects the variables to keep in the data frame\n-Example:\n\nmtcars %&gt;% mutate(log_mpg=log(mpg),log_hp=log(hp)) %&gt;% \n  select(mpg,log_mpg,hp,log_hp) %&gt;% \n  head\n\n#&gt; mutate: new variable 'log_mpg' (double) with 25 unique values and 0% NA\n\n\n#&gt;         new variable 'log_hp' (double) with 22 unique values and 0% NA\n\n\n#&gt; select: dropped 9 variables (cyl, disp, drat, wt, qsec, …)\n\n\n#&gt;                    mpg  log_mpg  hp   log_hp\n#&gt; Mazda RX4         21.0 3.044522 110 4.700480\n#&gt; Mazda RX4 Wag     21.0 3.044522 110 4.700480\n#&gt; Datsun 710        22.8 3.126761  93 4.532599\n#&gt; Hornet 4 Drive    21.4 3.063391 110 4.700480\n#&gt; Hornet Sportabout 18.7 2.928524 175 5.164786\n#&gt; Valiant           18.1 2.895912 105 4.653960"
  },
  {
    "objectID": "lectures/lecture_9.html#example-5",
    "href": "lectures/lecture_9.html#example-5",
    "title": "Data Manipulation",
    "section": "Example",
    "text": "Example\nUsing the penguins dataset from palmerpenguins, only select the variables that are continuous data points."
  },
  {
    "objectID": "lectures/lecture_9.html#filter",
    "href": "lectures/lecture_9.html#filter",
    "title": "Data Manipulation",
    "section": "filter()",
    "text": "filter()\n\nSelects observations that satisfy a condition\nExample:\n\n\nmtcars %&gt;% mutate(log_mpg=log(mpg),log_hp=log(hp)) %&gt;% \n  select(mpg,log_mpg,hp,log_hp) %&gt;% \n  filter(log_hp&lt;5) %&gt;% \n  head\n\n#&gt; mutate: new variable 'log_mpg' (double) with 25 unique values and 0% NA\n\n\n#&gt;         new variable 'log_hp' (double) with 22 unique values and 0% NA\n\n\n#&gt; select: dropped 9 variables (cyl, disp, drat, wt, qsec, …)\n\n\n#&gt; filter: removed 15 rows (47%), 17 rows remaining\n\n\n#&gt;                 mpg  log_mpg  hp   log_hp\n#&gt; Mazda RX4      21.0 3.044522 110 4.700480\n#&gt; Mazda RX4 Wag  21.0 3.044522 110 4.700480\n#&gt; Datsun 710     22.8 3.126761  93 4.532599\n#&gt; Hornet 4 Drive 21.4 3.063391 110 4.700480\n#&gt; Valiant        18.1 2.895912 105 4.653960\n#&gt; Merc 240D      24.4 3.194583  62 4.127134"
  },
  {
    "objectID": "lectures/lecture_9.html#example-6",
    "href": "lectures/lecture_9.html#example-6",
    "title": "Data Manipulation",
    "section": "Example",
    "text": "Example\nUsing the penguins dataset from palmerpenguins, filter the data set to look at penguins that are a Gentoo species."
  },
  {
    "objectID": "lectures/lecture_9.html#if_else",
    "href": "lectures/lecture_9.html#if_else",
    "title": "Data Manipulation",
    "section": "if_else()",
    "text": "if_else()\n\nA function that provides T (1) if the condition is met and F (0) otherwise\nExample:\n\n\nmtcars %&gt;% mutate(log_mpg=log(mpg),log_hp=log(hp)) %&gt;% \n  select(mpg,log_mpg,hp,log_hp) %&gt;% \n  filter(log_hp&lt;5) %&gt;% \n  mutate(hilhp=if_else(log_hp&gt;mean(log_hp),1,0)) %&gt;% \n  head\n\n#&gt; mutate: new variable 'log_mpg' (double) with 25 unique values and 0% NA\n\n\n#&gt;         new variable 'log_hp' (double) with 22 unique values and 0% NA\n\n\n#&gt; select: dropped 9 variables (cyl, disp, drat, wt, qsec, …)\n\n\n#&gt; filter: removed 15 rows (47%), 17 rows remaining\n\n\n#&gt; mutate: new variable 'hilhp' (double) with 2 unique values and 0% NA\n\n\n#&gt;                 mpg  log_mpg  hp   log_hp hilhp\n#&gt; Mazda RX4      21.0 3.044522 110 4.700480     1\n#&gt; Mazda RX4 Wag  21.0 3.044522 110 4.700480     1\n#&gt; Datsun 710     22.8 3.126761  93 4.532599     1\n#&gt; Hornet 4 Drive 21.4 3.063391 110 4.700480     1\n#&gt; Valiant        18.1 2.895912 105 4.653960     1\n#&gt; Merc 240D      24.4 3.194583  62 4.127134     0"
  },
  {
    "objectID": "lectures/lecture_9.html#example-7",
    "href": "lectures/lecture_9.html#example-7",
    "title": "Data Manipulation",
    "section": "Example",
    "text": "Example\nUsing the penguins dataset from palmerpenguins, create a new variable that dichotomizes a penguin if their bill is longer than the average bill_length_mm."
  },
  {
    "objectID": "lectures/lecture_9.html#group_by",
    "href": "lectures/lecture_9.html#group_by",
    "title": "Data Manipulation",
    "section": "group_by()",
    "text": "group_by()\n\nThis groups the data frame\nExample:\n\n\nmtcars %&gt;% mutate(log_mpg=log(mpg),log_hp=log(hp)) %&gt;% \n  select(mpg,log_mpg,hp,log_hp) %&gt;% \n  filter(log_hp&lt;5) %&gt;% \n  mutate(hilhp=if_else(log_hp&gt;mean(log_hp),1,0)) %&gt;% \n  group_by(hilhp) %&gt;% \n  head\n\n#&gt; mutate: new variable 'log_mpg' (double) with 25 unique values and 0% NA\n\n\n#&gt;         new variable 'log_hp' (double) with 22 unique values and 0% NA\n\n\n#&gt; select: dropped 9 variables (cyl, disp, drat, wt, qsec, …)\n\n\n#&gt; filter: removed 15 rows (47%), 17 rows remaining\n\n\n#&gt; mutate: new variable 'hilhp' (double) with 2 unique values and 0% NA\n\n\n#&gt; group_by: one grouping variable (hilhp)\n\n\n#&gt; # A tibble: 6 × 5\n#&gt; # Groups:   hilhp [2]\n#&gt;     mpg log_mpg    hp log_hp hilhp\n#&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  21      3.04   110   4.70     1\n#&gt; 2  21      3.04   110   4.70     1\n#&gt; 3  22.8    3.13    93   4.53     1\n#&gt; 4  21.4    3.06   110   4.70     1\n#&gt; 5  18.1    2.90   105   4.65     1\n#&gt; 6  24.4    3.19    62   4.13     0"
  },
  {
    "objectID": "lectures/lecture_9.html#example-8",
    "href": "lectures/lecture_9.html#example-8",
    "title": "Data Manipulation",
    "section": "Example",
    "text": "Example\nUsing the penguins dataset from palmerpenguins, group by species and find the average ln flipper_length_mm"
  },
  {
    "objectID": "lectures/lecture_7.html#learning-objectives",
    "href": "lectures/lecture_7.html#learning-objectives",
    "title": "*apply Functions",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\n*apply()\napply()\nlapply()\nsapply\nmapply()\ntapply()"
  },
  {
    "objectID": "lectures/lecture_7.html#apply-1",
    "href": "lectures/lecture_7.html#apply-1",
    "title": "*apply Functions",
    "section": "*apply()",
    "text": "*apply()\nThe *apply() functions are a set of functions that completes iterative tasks to each element of an R object."
  },
  {
    "objectID": "lectures/lecture_7.html#apply-3",
    "href": "lectures/lecture_7.html#apply-3",
    "title": "*apply Functions",
    "section": "apply()",
    "text": "apply()\nThe apply function returns a vector, array, or list of values by applying a function to the margins of an array. You will need to specify the following arguments:\n\nX: an array to be indexed and applied\nMARGIN: specifyng which index(es) to subset by\nFUN: function to be applied\n…: further arguments to be applied to FUN, must be labeled\n\n\napply(X, MARGIN, FUN, ...)"
  },
  {
    "objectID": "lectures/lecture_7.html#example",
    "href": "lectures/lecture_7.html#example",
    "title": "*apply Functions",
    "section": "Example",
    "text": "Example\nFind the standard deviation of all the columns of the following matrix:\n\nx &lt;- matrix(rnorm(1000), nrow = 10)"
  },
  {
    "objectID": "lectures/lecture_7.html#example-1",
    "href": "lectures/lecture_7.html#example-1",
    "title": "*apply Functions",
    "section": "Example",
    "text": "Example\nFind the \\(25th\\), \\(50th\\), and \\(75th\\) quartiles for each row of the following matrix:\n\nx &lt;- matrix(rnorm(1000), nrow = 20)"
  },
  {
    "objectID": "lectures/lecture_7.html#lapply-1",
    "href": "lectures/lecture_7.html#lapply-1",
    "title": "*apply Functions",
    "section": "lapply()",
    "text": "lapply()\nThe lapply function applies a function to all the elements of a vector or matrix, and it will return a list. You will need to specify the following arguments:\n\nX: object to be iterated\nFUN: a function to be applied\n…: further arguments to be passed along to FUN\n\n\nlapply(X, FUN, ...)"
  },
  {
    "objectID": "lectures/lecture_7.html#example-2",
    "href": "lectures/lecture_7.html#example-2",
    "title": "*apply Functions",
    "section": "Example",
    "text": "Example\nCreate a function that returns a labeled list for with the following values: mean, standard deviation, median, max, and min."
  },
  {
    "objectID": "lectures/lecture_7.html#sapply-1",
    "href": "lectures/lecture_7.html#sapply-1",
    "title": "*apply Functions",
    "section": "sapply()",
    "text": "sapply()\nThe sapply() function will apply a function to each element of a list or vector, and it will return a simplified object, vector, matrix, or array. The sapply() function uses 4 main arguments:\n\nX: a vector or list to be iterated\nFUN: a function to be applied\n…: arguments passed along to FUN, must be labeled\nsimplify: indicates how to simplify the function, defaults to n-dimensional array based on output\n\n\nsapply(X, FUN, ..., simplify = TRUE)"
  },
  {
    "objectID": "lectures/lecture_7.html#example-3",
    "href": "lectures/lecture_7.html#example-3",
    "title": "*apply Functions",
    "section": "Example",
    "text": "Example\nUsing the vector below, compute the length of each string using sapply and str_length() from tidyverse\n\nfruits &lt;- c(\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\", \"fig\", \"grape\", \"honeydew\", \"kiwi\", \"lemon\")"
  },
  {
    "objectID": "lectures/lecture_7.html#example-4",
    "href": "lectures/lecture_7.html#example-4",
    "title": "*apply Functions",
    "section": "Example",
    "text": "Example\nUsing the list generated below, compute the mean of each element of the list using sapply.\n\n# Generate a list of 10 lists, each containing 5 random numbers\nlists &lt;- lapply(1:10, function(i) {\n  means &lt;- rpois(1, 3)\n  rnorm(5, means)\n})"
  },
  {
    "objectID": "lectures/lecture_7.html#example-5",
    "href": "lectures/lecture_7.html#example-5",
    "title": "*apply Functions",
    "section": "Example",
    "text": "Example\nUsing the vector below, use the sapply() to find \\(\\log(x)\\) for each value and return a matrix:\n\nnumbers &lt;- 4:400"
  },
  {
    "objectID": "lectures/lecture_7.html#mapply-1",
    "href": "lectures/lecture_7.html#mapply-1",
    "title": "*apply Functions",
    "section": "mapply()",
    "text": "mapply()\nThe mapply() is the multivariate function of sapply(). The mapply() function has 3 major arguments:\n\nFUN: function applied to data\n…: arguments to be iterated, must be labeled.\nMoreArgs: A list containing other arguments that are necessary to FUN\n\n\nmapply(FUN, ..., MoreArgs = NULL)"
  },
  {
    "objectID": "lectures/lecture_7.html#example-6",
    "href": "lectures/lecture_7.html#example-6",
    "title": "*apply Functions",
    "section": "Example",
    "text": "Example\nLet x and y be two vectors, shown below, represent the x and y coordinates of a point. Using mapply(), compute the the distance between the points and the origin.\n\nx &lt;- c(2, 3, 4, 5)\ny &lt;- c(4, 6, 8, 10)"
  },
  {
    "objectID": "lectures/lecture_7.html#tapply-1",
    "href": "lectures/lecture_7.html#tapply-1",
    "title": "*apply Functions",
    "section": "tapply()",
    "text": "tapply()\nThe tapply() function will apply function to a group of values based on and indexed lists. It takes 3 arguments:\n\nX: a vector that can split\nINDEX: the index list made up of factors\nFUN: the function that will be applied\n…: further arguments to be passed to FUN\n\n\ntapply(X, INDEX, FUN = NULL, ...)"
  },
  {
    "objectID": "lectures/lecture_7.html#example-7",
    "href": "lectures/lecture_7.html#example-7",
    "title": "*apply Functions",
    "section": "Example",
    "text": "Example\nUsing the penguins data set from the palmerpenguins package, compute the average bill_length_mm for each island."
  },
  {
    "objectID": "lectures/lecture_7.html#example-8",
    "href": "lectures/lecture_7.html#example-8",
    "title": "*apply Functions",
    "section": "Example",
    "text": "Example\nThe vectors below provide the heights of different trees in the sample. Compute the median for each type of tree.\n\nheights &lt;- c(70, 72, 68, 65, 80, 75, 60, 68, 90, 72)\nspecies &lt;- c(\"maple\", \"oak\", \"pine\", \"maple\", \"oak\", \"pine\", \"maple\", \"oak\", \"pine\", \"maple\")"
  },
  {
    "objectID": "lectures/lecture_5.html#learning-objectives",
    "href": "lectures/lecture_5.html#learning-objectives",
    "title": "Control Flow",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nbreak Statements\nwhile Loops"
  },
  {
    "objectID": "lectures/lecture_5.html#break-statements-1",
    "href": "lectures/lecture_5.html#break-statements-1",
    "title": "Control Flow",
    "section": "break Statements",
    "text": "break Statements\nThe break statement is used to stop a loop if the condition is met. This is used along with an if statement.\n\nfor (i in vector){\n  perform task\n  if (condition){\n    break\n  } else {\n    perform task\n  }\n}"
  },
  {
    "objectID": "lectures/lecture_5.html#example",
    "href": "lectures/lecture_5.html#example",
    "title": "Control Flow",
    "section": "Example",
    "text": "Example\nSimulate from a \\(N(1,1)\\) distribution until you have 30 positive numbers or you simulate one negative number."
  },
  {
    "objectID": "lectures/lecture_5.html#while-loops-1",
    "href": "lectures/lecture_5.html#while-loops-1",
    "title": "Control Flow",
    "section": "while Loops",
    "text": "while Loops\nA while loop is a combination of a for loop and a break statement. The loop will continue indefinitely until a condition becomes false.\n\nwhile (condition){\n  perform task\n  condition &lt;- update condition\n}"
  },
  {
    "objectID": "lectures/lecture_5.html#example-1",
    "href": "lectures/lecture_5.html#example-1",
    "title": "Control Flow",
    "section": "Example",
    "text": "Example\nSimulate from a \\(N(0,1)\\) distribution until you have 50 positive numbers.\\"
  },
  {
    "objectID": "lectures/lecture_5.html#example-2",
    "href": "lectures/lecture_5.html#example-2",
    "title": "Control Flow",
    "section": "Example",
    "text": "Example\nFind the value of \\(x\\) where the function \\(y=1/x\\) relative converges (\\(\\frac{|y_{old}-y_{new}|}{y_{old}}\\)) at a level of \\(10^-6\\) as \\(x\\rightarrow \\infty\\)."
  },
  {
    "objectID": "lectures/lecture_5.html#example-3",
    "href": "lectures/lecture_5.html#example-3",
    "title": "Control Flow",
    "section": "Example",
    "text": "Example\nGenerate the first 1000 prime numbers."
  },
  {
    "objectID": "lectures/lecture_5.html#example-4",
    "href": "lectures/lecture_5.html#example-4",
    "title": "Control Flow",
    "section": "Example",
    "text": "Example\nWrite a loop that will generate the first 1000 numbers of the Fibonacci sequence."
  },
  {
    "objectID": "lectures/lecture_5.html#example-5",
    "href": "lectures/lecture_5.html#example-5",
    "title": "Control Flow",
    "section": "Example",
    "text": "Example\nGenerate the 10th row of Pascal’s Triangle."
  },
  {
    "objectID": "lectures/lecture_30.html#my-experience",
    "href": "lectures/lecture_30.html#my-experience",
    "title": "Neural Networks in R",
    "section": "My Experience",
    "text": "My Experience"
  },
  {
    "objectID": "lectures/lecture_30.html#my-experience-1",
    "href": "lectures/lecture_30.html#my-experience-1",
    "title": "Neural Networks in R",
    "section": "My Experience",
    "text": "My Experience"
  },
  {
    "objectID": "lectures/lecture_30.html#my-experience-2",
    "href": "lectures/lecture_30.html#my-experience-2",
    "title": "Neural Networks in R",
    "section": "My Experience",
    "text": "My Experience"
  },
  {
    "objectID": "lectures/lecture_30.html#learning-outcomes",
    "href": "lectures/lecture_30.html#learning-outcomes",
    "title": "Neural Networks in R",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nTensorflow\nTorch"
  },
  {
    "objectID": "lectures/lecture_30.html#tensorflow-1",
    "href": "lectures/lecture_30.html#tensorflow-1",
    "title": "Neural Networks in R",
    "section": "TensorFlow",
    "text": "TensorFlow\nTensorflow is an open-source machine learning platform developed by Google. Tensorflow is capable of completing the following tasks:\n\nImage Classification\nText Classification\nRegression\nTime-Series"
  },
  {
    "objectID": "lectures/lecture_30.html#keras",
    "href": "lectures/lecture_30.html#keras",
    "title": "Neural Networks in R",
    "section": "Keras",
    "text": "Keras\nKeras is the API that will talk to Tensorflow via different platforms."
  },
  {
    "objectID": "lectures/lecture_30.html#more-information",
    "href": "lectures/lecture_30.html#more-information",
    "title": "Neural Networks in R",
    "section": "More Information",
    "text": "More Information\nTensorFlow for R"
  },
  {
    "objectID": "lectures/lecture_30.html#torch-1",
    "href": "lectures/lecture_30.html#torch-1",
    "title": "Neural Networks in R",
    "section": "Torch",
    "text": "Torch\nTorch is a scientific computing framework designed to support machine learning in CPU/GPU computing. Torch is capable of computing:\n\nMatrix Operations\nLinear Algebra\nNeural Networks\nNumerical Optimization\nand so much more!"
  },
  {
    "objectID": "lectures/lecture_30.html#torch-2",
    "href": "lectures/lecture_30.html#torch-2",
    "title": "Neural Networks in R",
    "section": "Torch",
    "text": "Torch\nTorch can be accessed in both:\n\nPytorch\nR Torch"
  },
  {
    "objectID": "lectures/lecture_30.html#r-torch",
    "href": "lectures/lecture_30.html#r-torch",
    "title": "Neural Networks in R",
    "section": "R Torch",
    "text": "R Torch\nR Torch is capable of handling:\n\nImage Recognition\nTabular Data\nTime Series Forecasting\nAudio Processing"
  },
  {
    "objectID": "lectures/lecture_30.html#more-information-1",
    "href": "lectures/lecture_30.html#more-information-1",
    "title": "Neural Networks in R",
    "section": "More Information",
    "text": "More Information"
  },
  {
    "objectID": "lectures/lecture_30.html#mnist",
    "href": "lectures/lecture_30.html#mnist",
    "title": "Neural Networks in R",
    "section": "MNIST",
    "text": "MNIST\nThis is a database of handwritten digits.\nWe will use to construct neural networks that will classify images."
  },
  {
    "objectID": "lectures/lecture_30.html#tensorflow-installation-r",
    "href": "lectures/lecture_30.html#tensorflow-installation-r",
    "title": "Neural Networks in R",
    "section": "Tensorflow Installation R",
    "text": "Tensorflow Installation R\n\n# install.packages(\"reticulate\")\n# install.packages(\"tensorflow\")\nlibrary(reticulate)\npath_to_python &lt;- install_python()\nvirtualenv_create(\"r-reticulate\", python = path_to_python)\n# install.packages(\"keras\")\nlibrary(keras)\ninstall_keras(envname = \"r-reticulate\")"
  },
  {
    "objectID": "lectures/lecture_30.html#application-of-tensorflow",
    "href": "lectures/lecture_30.html#application-of-tensorflow",
    "title": "Neural Networks in R",
    "section": "Application of TensorFlow",
    "text": "Application of TensorFlow\n\n#Set up python environment\nlibrary(keras)\n# install_keras()\nlibrary(reticulate)\npath_to_python &lt;- install_python()\nuse_python(path_to_python)\nlibrary(tensorflow)\ntf$constant(\"Hello Tensorflow!\")\n###\nmnist &lt;- dataset_mnist()\nx_train &lt;- mnist$train$x\ng_train &lt;- mnist$train$y\nx_test &lt;- mnist$test$x\ng_test &lt;- mnist$test$y\ndim(x_train)\ndim(x_test)\n###\nx_train &lt;- array_reshape(x_train, c(nrow(x_train), 784))\nx_test &lt;- array_reshape(x_test, c(nrow(x_test), 784))\ny_train &lt;- to_categorical(g_train, 10)\ny_test &lt;- to_categorical(g_test, 10)\n###\nx_train &lt;- x_train / 255\nx_test &lt;- x_test / 255\n###\nmodelnn &lt;- keras_model_sequential()\nmodelnn %&gt;%\n  layer_dense(units = 256, activation = \"relu\",\n       input_shape = c(784)) %&gt;%\n  layer_dropout(rate = 0.4) %&gt;%\n  layer_dense(units = 128, activation = \"relu\") %&gt;%\n  layer_dropout(rate = 0.3) %&gt;%\n  layer_dense(units = 10, activation = \"softmax\")\n###\nsummary(modelnn)\n###\nmodelnn %&gt;% compile(loss = \"categorical_crossentropy\",\n    optimizer = optimizer_rmsprop(), metrics = c(\"accuracy\")\n  )\n###\nsystem.time(\n  history &lt;- modelnn %&gt;%\n    fit(x_train, y_train, epochs = 30, batch_size = 128,\n        validation_split = 0.2)\n)\nplot(history, smooth = FALSE)\n###\naccuracy &lt;- function(pred, truth)\n  mean(drop(as.numeric(pred)) == drop(truth))\nmodelnn %&gt;% predict(x_test) %&gt;% k_argmax() %&gt;% accuracy(g_test)\n###\nmodellr &lt;- keras_model_sequential() %&gt;%\n  layer_dense(input_shape = 784, units = 10,\n       activation = \"softmax\")\nsummary(modellr)\n###\nmodellr %&gt;% compile(loss = \"categorical_crossentropy\",\n     optimizer = optimizer_rmsprop(), metrics = c(\"accuracy\"))\nmodellr %&gt;% fit(x_train, y_train, epochs = 30,\n      batch_size = 128, validation_split = 0.2)\nmodellr %&gt;% predict(x_test) %&gt;% k_argmax() %&gt;% accuracy(g_test)"
  },
  {
    "objectID": "lectures/lecture_30.html#installation-of-torch",
    "href": "lectures/lecture_30.html#installation-of-torch",
    "title": "Neural Networks in R",
    "section": "Installation of Torch",
    "text": "Installation of Torch\n\ninstall.packages(\"torch\")\ninstall.packages(\"luz\")\ninstall.packages(\"torchvision\")\ninstall.packages(\"torchdatasets\")\ninstall.packages(\"zeallot\")"
  },
  {
    "objectID": "lectures/lecture_30.html#torch-in-r",
    "href": "lectures/lecture_30.html#torch-in-r",
    "title": "Neural Networks in R",
    "section": "Torch in R",
    "text": "Torch in R\n\nlibrary(torch)\nlibrary(luz) # high-level interface for torch\nlibrary(torchvision) # for datasets and image transformation\nlibrary(torchdatasets) # for datasets we are going to use\nlibrary(zeallot)\ntorch_manual_seed(13)\n\n###\ntrain_ds &lt;- mnist_dataset(root = \".\", train = TRUE, download = TRUE)\ntest_ds &lt;- mnist_dataset(root = \".\", train = FALSE, download = TRUE)\n\nstr(train_ds[1])\nstr(test_ds[2])\n\nlength(train_ds)\nlength(test_ds)\n###\n\n\n###\ntransform &lt;- function(x) {\n  x %&gt;%\n    torch_tensor() %&gt;%\n    torch_flatten() %&gt;%\n    torch_div(255)\n}\ntrain_ds &lt;- mnist_dataset(\n  root = \".\",\n  train = TRUE,\n  download = TRUE,\n  transform = transform\n)\ntest_ds &lt;- mnist_dataset(\n  root = \".\",\n  train = FALSE,\n  download = TRUE,\n  transform = transform\n)\n###\n\n\n###\nmodelnn &lt;- nn_module(\n  initialize = function() {\n    self$linear1 &lt;- nn_linear(in_features = 28*28, out_features = 256)\n    self$linear2 &lt;- nn_linear(in_features = 256, out_features = 128)\n    self$linear3 &lt;- nn_linear(in_features = 128, out_features = 10)\n\n    self$drop1 &lt;- nn_dropout(p = 0.4)\n    self$drop2 &lt;- nn_dropout(p = 0.3)\n\n    self$activation &lt;- nn_relu()\n  },\n  forward = function(x) {\n    x %&gt;%\n\n      self$linear1() %&gt;%\n      self$activation() %&gt;%\n      self$drop1() %&gt;%\n\n      self$linear2() %&gt;%\n      self$activation() %&gt;%\n      self$drop2() %&gt;%\n\n      self$linear3()\n  }\n)\n###\n\n\n###\nprint(modelnn())\n###\n\n\n###\nmodelnn &lt;- modelnn %&gt;%\n  setup(\n    loss = nn_cross_entropy_loss(),\n    optimizer = optim_rmsprop,\n    metrics = list(luz_metric_accuracy())\n  )\n###\n\n###\nsystem.time(\n   fitted &lt;- modelnn %&gt;%\n      fit(\n        data = train_ds,\n        epochs = 5,\n        valid_data = 0.2,\n        dataloader_options = list(batch_size = 256),\n        verbose = FALSE\n      )\n )\nplot(fitted)\n###\n\n###\naccuracy &lt;- function(pred, truth) {\n   mean(pred == truth) }\n\n# gets the true classes from all observations in test_ds.\ntruth &lt;- sapply(seq_along(test_ds), function(x) test_ds[x][[2]])\n\nfitted %&gt;%\n  predict(test_ds) %&gt;%\n  torch_argmax(dim = 2) %&gt;%  # the predicted class is the one with higher 'logit'.\n  as_array() %&gt;% # we convert to an R object\n  accuracy(truth)\n###\n\n###\nmodellr &lt;- nn_module(\n  initialize = function() {\n    self$linear &lt;- nn_linear(784, 10)\n  },\n  forward = function(x) {\n    self$linear(x)\n  }\n)\nprint(modellr())\n###\n\n###\nfit_modellr &lt;- modellr %&gt;%\n  setup(\n    loss = nn_cross_entropy_loss(),\n    optimizer = optim_rmsprop,\n    metrics = list(luz_metric_accuracy())\n  ) %&gt;%\n  fit(\n    data = train_ds,\n    epochs = 5,\n    valid_data = 0.2,\n    dataloader_options = list(batch_size = 128)\n  )\n\nfit_modellr %&gt;%\n  predict(test_ds) %&gt;%\n  torch_argmax(dim = 2) %&gt;%  # the predicted class is the one with higher 'logit'.\n  as_array() %&gt;% # we convert to an R object\n  accuracy(truth)\n\n\n# alternatively one can use the `evaluate` function to get the results\n# on the test_ds\nevaluate(fit_modellr, test_ds)\n###"
  },
  {
    "objectID": "lectures/lecture_30.html#my-opinion",
    "href": "lectures/lecture_30.html#my-opinion",
    "title": "Neural Networks in R",
    "section": "My Opinion",
    "text": "My Opinion"
  },
  {
    "objectID": "lectures/lecture_29.html#learning-outcomes",
    "href": "lectures/lecture_29.html#learning-outcomes",
    "title": "Neural Networks",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nNeural Networks"
  },
  {
    "objectID": "lectures/lecture_29.html#neural-networks",
    "href": "lectures/lecture_29.html#neural-networks",
    "title": "Neural Networks",
    "section": "Neural Networks",
    "text": "Neural Networks\nNeural networks are a type of machine learning algorithm that are designed to mimic the function of the human brain. They consist of interconnected nodes or “neurons” that process information and generate outputs based on the inputs they receive."
  },
  {
    "objectID": "lectures/lecture_29.html#uses",
    "href": "lectures/lecture_29.html#uses",
    "title": "Neural Networks",
    "section": "Uses",
    "text": "Uses\nNeural networks are typically used for tasks such as image recognition, natural language processing, and prediction. They are capable of learning from data and improving their performance over time, which makes them well-suited for complex and dynamic problems."
  },
  {
    "objectID": "lectures/lecture_29.html#single-layer-neural-networks",
    "href": "lectures/lecture_29.html#single-layer-neural-networks",
    "title": "Neural Networks",
    "section": "Single Layer Neural Networks",
    "text": "Single Layer Neural Networks\nA single layer neural networks can be formulated as linear function:\n\\[\nf(X) = \\beta_0 + \\sum^K_{k=1}\\beta_kh_k(X)\n\\]\nWhere \\(X\\) is a vector of inputs of length \\(p\\) and \\(K\\) is the number of activations, \\(\\beta_j\\) are the regression coefficients and\n\\[\nh_k(X) = A_k = g(w_{k0} + \\sum^p_{l1}w_{kl}X_{l})\n\\]\nwith \\(g(\\cdot)\\) being a nonlinear function and \\(w_{kl}\\) are the weights."
  },
  {
    "objectID": "lectures/lecture_29.html#nonlinear-function-gcdot",
    "href": "lectures/lecture_29.html#nonlinear-function-gcdot",
    "title": "Neural Networks",
    "section": "Nonlinear Function \\(g(\\cdot)\\)",
    "text": "Nonlinear Function \\(g(\\cdot)\\)\n\n\\(g(z) = \\frac{e^z}{1+e^z}\\)\n\\(g(z) = (z)_+ = zI(z\\geq0)\\)"
  },
  {
    "objectID": "lectures/lecture_29.html#single-layer-neural-network",
    "href": "lectures/lecture_29.html#single-layer-neural-network",
    "title": "Neural Networks",
    "section": "Single Layer Neural Network",
    "text": "Single Layer Neural Network"
  },
  {
    "objectID": "lectures/lecture_29.html#multilayer-neural-network",
    "href": "lectures/lecture_29.html#multilayer-neural-network",
    "title": "Neural Networks",
    "section": "Multilayer Neural Network",
    "text": "Multilayer Neural Network\nMultilayer Neural Networks create multiple hidden layers where each layer feeds into each other which will create a final outcome."
  },
  {
    "objectID": "lectures/lecture_29.html#multilayer-neural-network-1",
    "href": "lectures/lecture_29.html#multilayer-neural-network-1",
    "title": "Neural Networks",
    "section": "Multilayer Neural Network",
    "text": "Multilayer Neural Network"
  },
  {
    "objectID": "lectures/lecture_27.html#learning-outcomes",
    "href": "lectures/lecture_27.html#learning-outcomes",
    "title": "Support Vector Machines",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nMaximal Margin Classifier\nSupport Vector Classifier\nR Code"
  },
  {
    "objectID": "lectures/lecture_27.html#motivating-example",
    "href": "lectures/lecture_27.html#motivating-example",
    "title": "Support Vector Machines",
    "section": "Motivating Example",
    "text": "Motivating Example"
  },
  {
    "objectID": "lectures/lecture_27.html#maximal-margin-classifier-1",
    "href": "lectures/lecture_27.html#maximal-margin-classifier-1",
    "title": "Support Vector Machines",
    "section": "Maximal Margin Classifier",
    "text": "Maximal Margin Classifier\nThe Maximal Margin Classifier will impose a hyperplane on a graph that will classify the data given a vector of predictor variables."
  },
  {
    "objectID": "lectures/lecture_27.html#hyperplane",
    "href": "lectures/lecture_27.html#hyperplane",
    "title": "Support Vector Machines",
    "section": "Hyperplane",
    "text": "Hyperplane\nGiven a p-dimensional space, a hyperplane is flat affine subspace of p-1 dimensions. It is mathematically defined as:\n\\[\n\\beta_0 + \\beta_1X_1 + \\beta_2 X_2 + \\cdots+\\beta_pX_p = 0\n\\]"
  },
  {
    "objectID": "lectures/lecture_27.html#hyperplane-1",
    "href": "lectures/lecture_27.html#hyperplane-1",
    "title": "Support Vector Machines",
    "section": "Hyperplane",
    "text": "Hyperplane"
  },
  {
    "objectID": "lectures/lecture_27.html#constructing-the-hyperplane",
    "href": "lectures/lecture_27.html#constructing-the-hyperplane",
    "title": "Support Vector Machines",
    "section": "Constructing the Hyperplane",
    "text": "Constructing the Hyperplane\nA hyperplane is constructed by maximizing the margin \\(M\\) of the data points that are farthest from the theoretical margin. The data points that define the outer edge of the margins are known as support vectors."
  },
  {
    "objectID": "lectures/lecture_27.html#optimization-problem",
    "href": "lectures/lecture_27.html#optimization-problem",
    "title": "Support Vector Machines",
    "section": "Optimization Problem",
    "text": "Optimization Problem\n\n\\(\\overset{\\mathrm{maximize}}{\\tiny\\beta_0, \\beta_1,\\ldots,\\beta_p,M}\\ \\large M\\)\nsubject to \\(\\sum^p_{j=1}\\beta_j^2 = 1\\)\n\\(y_i(\\beta_0 + \\beta_1X_{1i} + \\beta_2 X_{2i} + \\cdots+\\beta_pX_{pi})\\geq M \\ \\forall \\ i=1,\\ldots,n\\)"
  },
  {
    "objectID": "lectures/lecture_27.html#maximal-margin-classifier-2",
    "href": "lectures/lecture_27.html#maximal-margin-classifier-2",
    "title": "Support Vector Machines",
    "section": "Maximal Margin Classifier",
    "text": "Maximal Margin Classifier"
  },
  {
    "objectID": "lectures/lecture_27.html#support-vector-classifier-1",
    "href": "lectures/lecture_27.html#support-vector-classifier-1",
    "title": "Support Vector Machines",
    "section": "Support Vector Classifier",
    "text": "Support Vector Classifier\nMaximal Margin Classifiers have one fatal defect, the data points must be completely on one side of the margin. This does not allow room for error.\n\nA Support Vector Classifier allows for data points to be misclassified if need be.\n\n\nIt achieves this by implementing a Cost mechanism, denoted as \\(C\\), to account for any errors for data points."
  },
  {
    "objectID": "lectures/lecture_27.html#support-vector-classifiers",
    "href": "lectures/lecture_27.html#support-vector-classifiers",
    "title": "Support Vector Machines",
    "section": "Support Vector Classifiers",
    "text": "Support Vector Classifiers"
  },
  {
    "objectID": "lectures/lecture_27.html#optimization-problem-1",
    "href": "lectures/lecture_27.html#optimization-problem-1",
    "title": "Support Vector Machines",
    "section": "Optimization Problem",
    "text": "Optimization Problem\n\n\\(\\overset{\\mathrm{maximize}}{\\tiny\\beta_0, \\beta_1,\\ldots,\\beta_p,\\epsilon_1,\\ldots, \\epsilon_n,M}\\ \\large M\\)\nsubject to \\(\\sum^p_{j=1}\\beta_j^2 = 1\\)\n\\(y_i(\\beta_0 + \\beta_1X_{1i} + \\beta_2 X_{2i} + \\cdots+\\beta_pX_{pi})\\geq M (1-\\epsilon_i) \\ \\forall \\ i=1,\\ldots,n\\)\n\\(\\epsilon_i\\geq 0\\)\n\\(\\sum^n_{i=1} \\epsilon_i \\leq C\\)"
  },
  {
    "objectID": "lectures/lecture_27.html#budget-c",
    "href": "lectures/lecture_27.html#budget-c",
    "title": "Support Vector Machines",
    "section": "Budget C",
    "text": "Budget C\nThe tuning parameter \\(C\\) is known as the budget parameter for error. When the data point is on the correct side of the margin, then it has an error of \\(0\\). When a data point in on the wrong side the margin, it has a bit of error. When the data point is on the opposite side of the hyperplane, then it has an error greater than \\(1\\). This is allowed as long as the sum of errors are less than or equal to \\(C\\)."
  },
  {
    "objectID": "lectures/lecture_27.html#support-vector-classifier-2",
    "href": "lectures/lecture_27.html#support-vector-classifier-2",
    "title": "Support Vector Machines",
    "section": "Support Vector Classifier",
    "text": "Support Vector Classifier\n\nSimulationR CodeSupport VectorsSummary\n\n\n\nx &lt;- matrix ( rnorm (20 * 2) , ncol = 2)\ny &lt;- c( rep (-1, 10) , rep (1, 10) )\nx[y == 1, ] &lt;- x[y == 1, ] + 1\nplot (x, col = (3 - y))\n\n\n\n\n\n\n\n\n\n\n\ndat &lt;- data.frame (x = x, y = as.factor(y))\nlibrary(e1071)\nsvmfit &lt;- svm (y ~ ., data = dat , kernel = \"linear\",\ncost = 10, scale = FALSE )\nplot(svmfit, dat)\n\n\n\n\n\n\n\n\n\n\n\nsvmfit$index\n\n#&gt;  [1]  1  2  3  6  7  8 10 11 13 14 16 19 20\n\n\n\n\n\nsummary(svmfit)\n\n#&gt; \n#&gt; Call:\n#&gt; svm(formula = y ~ ., data = dat, kernel = \"linear\", cost = 10, scale = FALSE)\n#&gt; \n#&gt; \n#&gt; Parameters:\n#&gt;    SVM-Type:  C-classification \n#&gt;  SVM-Kernel:  linear \n#&gt;        cost:  10 \n#&gt; \n#&gt; Number of Support Vectors:  13\n#&gt; \n#&gt;  ( 7 6 )\n#&gt; \n#&gt; \n#&gt; Number of Classes:  2 \n#&gt; \n#&gt; Levels: \n#&gt;  -1 1"
  },
  {
    "objectID": "lectures/lecture_27.html#cross-validation-approach-for-c",
    "href": "lectures/lecture_27.html#cross-validation-approach-for-c",
    "title": "Support Vector Machines",
    "section": "Cross-Validation Approach for C",
    "text": "Cross-Validation Approach for C\n\nR CodeSummaryBest Model\n\n\n\nset.seed(2434)\ntune.out &lt;- tune(svm, y ~ ., data = dat, kernel = \"linear\",\n                 ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))\n\n\n\n\nsummary(tune.out)\n\n#&gt; \n#&gt; Parameter tuning of 'svm':\n#&gt; \n#&gt; - sampling method: 10-fold cross validation \n#&gt; \n#&gt; - best parameters:\n#&gt;  cost\n#&gt;   0.1\n#&gt; \n#&gt; - best performance: 0.45 \n#&gt; \n#&gt; - Detailed performance results:\n#&gt;    cost error dispersion\n#&gt; 1 1e-03  0.55  0.4972145\n#&gt; 2 1e-02  0.55  0.4972145\n#&gt; 3 1e-01  0.45  0.4377975\n#&gt; 4 1e+00  0.45  0.3689324\n#&gt; 5 5e+00  0.45  0.3689324\n#&gt; 6 1e+01  0.45  0.3689324\n#&gt; 7 1e+02  0.45  0.3689324\n\n\n\n\n\nbestmod &lt;- tune.out$best.model\nsummary(bestmod)\n\n#&gt; \n#&gt; Call:\n#&gt; best.tune(METHOD = svm, train.x = y ~ ., data = dat, ranges = list(cost = c(0.001, \n#&gt;     0.01, 0.1, 1, 5, 10, 100)), kernel = \"linear\")\n#&gt; \n#&gt; \n#&gt; Parameters:\n#&gt;    SVM-Type:  C-classification \n#&gt;  SVM-Kernel:  linear \n#&gt;        cost:  0.1 \n#&gt; \n#&gt; Number of Support Vectors:  18\n#&gt; \n#&gt;  ( 9 9 )\n#&gt; \n#&gt; \n#&gt; Number of Classes:  2 \n#&gt; \n#&gt; Levels: \n#&gt;  -1 1"
  },
  {
    "objectID": "lectures/lecture_27.html#prediction",
    "href": "lectures/lecture_27.html#prediction",
    "title": "Support Vector Machines",
    "section": "Prediction",
    "text": "Prediction\n\nSet UpPrediction\n\n\n\nxtest &lt;- matrix(rnorm(20 * 2), ncol = 2)\nytest &lt;- sample(c(-1, 1), 20, rep = TRUE)\nxtest[ytest == 1,] &lt;- xtest[ytest == 1, ] + 1\ntestdat &lt;- data.frame (x = xtest , y = as.factor(ytest))\n\n\n\n\nypred &lt;- predict(bestmod, testdat)\ntable(pred = ypred, truth = testdat$y)\n\n#&gt;     truth\n#&gt; pred -1  1\n#&gt;   -1  9  0\n#&gt;   1   1 10"
  },
  {
    "objectID": "lectures/lecture_25.html#learning-outcomes",
    "href": "lectures/lecture_25.html#learning-outcomes",
    "title": "Tree-Based Methods",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nTrees\nPruning\nClassification Trees\nRegression Trees\nR Examples"
  },
  {
    "objectID": "lectures/lecture_25.html#trees-1",
    "href": "lectures/lecture_25.html#trees-1",
    "title": "Tree-Based Methods",
    "section": "Trees",
    "text": "Trees\nA Statistical tree will partition a region from a set of predictor variables that will predict an outcome of interest.\n\nTrees will split a region based on a predictors ability to reduce the overall mean squared error.\n\n\nTrees are sometimes preferred to linear models due to the visual explanation of the model."
  },
  {
    "objectID": "lectures/lecture_25.html#trees-2",
    "href": "lectures/lecture_25.html#trees-2",
    "title": "Tree-Based Methods",
    "section": "Trees",
    "text": "Trees"
  },
  {
    "objectID": "lectures/lecture_25.html#fitting-a-tree",
    "href": "lectures/lecture_25.html#fitting-a-tree",
    "title": "Tree-Based Methods",
    "section": "Fitting a Tree",
    "text": "Fitting a Tree\n\nStart with the entire dataset and define the maximum number of regions or number of observations per region of the tree.\nCalculate the MSE of the dataset.\nFor each potential split, calculate the MSE. Choose the split that results in the lowest overall MSE.\nCreate a node in the tree with the selected split as the split criterion.\nRepeat steps 2-4 for each subset, stopping if the maximum number of regions has been reached or if the subset size is too small."
  },
  {
    "objectID": "lectures/lecture_25.html#pruning-1",
    "href": "lectures/lecture_25.html#pruning-1",
    "title": "Tree-Based Methods",
    "section": "Pruning",
    "text": "Pruning\nPruning is the process that will remove branches from a regression tree in order to prevent overfitting.\n\nThis will result in a subtree that has high predictive power with no overfitting.\n\n\nDue to the computational burden of pruning, it is recommended to implement Cost Complexity Pruning."
  },
  {
    "objectID": "lectures/lecture_25.html#cost-complexity-pruning",
    "href": "lectures/lecture_25.html#cost-complexity-pruning",
    "title": "Tree-Based Methods",
    "section": "Cost Complexity Pruning",
    "text": "Cost Complexity Pruning\nLet \\(\\alpha\\) be nonnegative tuning parameter that indexes a sequence of trees. Identify the tree that reduces:\n\\[\n\\sum^{|T|}_{m=1}\\sum_{i:\\ x_i \\in R_m}(y_i-\\hat y_{R_m})^2 +\\alpha|T|\n\\]\n\n\\(|T|\\): Number of terminal nodes\n\\(R_m\\): rectangular region containing data\n\\(y_i\\): observed value\n\\(\\hat y_{R_m}\\): predicted value in rectangular region."
  },
  {
    "objectID": "lectures/lecture_25.html#pruning-algorithm",
    "href": "lectures/lecture_25.html#pruning-algorithm",
    "title": "Tree-Based Methods",
    "section": "Pruning Algorithm",
    "text": "Pruning Algorithm\n\nConduct a fitting algorithm to find the largest tree from the training data. Stop once every region has a small number of observations.\nApply the cost complexity pruning algorithm to identify the best subset of trees.\nUse a K-fold cross-validation approach to choose the proper \\(\\alpha\\). For each kth fold:\n\nRepeat steps 1 and 2.\nEvaluate the mean squared prediction error as a function of \\(\\alpha\\).\n\nAverage the results for each value of \\(\\alpha\\). Pick the \\(\\alpha\\) that minimizes the error.\nReturn the subtree with the selected \\(\\alpha\\) from step 2"
  },
  {
    "objectID": "lectures/lecture_25.html#classification-trees-1",
    "href": "lectures/lecture_25.html#classification-trees-1",
    "title": "Tree-Based Methods",
    "section": "Classification Trees",
    "text": "Classification Trees\nClassification Trees will construct a tree that will classify data based on the region (leaf) you land. The class majority is what is classified."
  },
  {
    "objectID": "lectures/lecture_25.html#criterion-gini-index",
    "href": "lectures/lecture_25.html#criterion-gini-index",
    "title": "Tree-Based Methods",
    "section": "Criterion: Gini Index",
    "text": "Criterion: Gini Index\nThe Gini Index is used to determine the error rate in classification trees:\n\\[\nG = \\sum^K_{k=1} \\hat p_{mk}(1-\\hat p_{mk})\n\\]"
  },
  {
    "objectID": "lectures/lecture_25.html#regression-trees-1",
    "href": "lectures/lecture_25.html#regression-trees-1",
    "title": "Tree-Based Methods",
    "section": "Regression Trees",
    "text": "Regression Trees\nRegression trees will construct a tree and predict the value of the outcome based on the average value of the region (leaf).\nTrees are constructed by minimizing the residual sums of square."
  },
  {
    "objectID": "lectures/lecture_25.html#regression-trees-2",
    "href": "lectures/lecture_25.html#regression-trees-2",
    "title": "Tree-Based Methods",
    "section": "Regression Trees",
    "text": "Regression Trees\n\nCodePlot\n\n\n\nlibrary(tree)\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(magrittr)\ndf &lt;- penguins %&gt;% drop_na()\ntrain &lt;- sample(1:nrow(df), nrow(df)/2)\ntree_penguin &lt;- penguins %$% tree(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm, subset = train)\n\n\n\n\nplot(tree_penguin)\ntext(tree_penguin, pretty = 0)"
  },
  {
    "objectID": "lectures/lecture_25.html#classification-trees-2",
    "href": "lectures/lecture_25.html#classification-trees-2",
    "title": "Tree-Based Methods",
    "section": "Classification Trees",
    "text": "Classification Trees\n\nCodePlot\n\n\n\nlibrary(tree)\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(magrittr)\ndf &lt;- penguins %&gt;% drop_na()\ntrain &lt;- sample(1:nrow(df), nrow(df)/2)\ntree_penguin_class &lt;- df %$% tree(species ~ body_mass_g + flipper_length_mm + bill_length_mm + bill_depth_mm, subset = train)\n\n\n\n\nplot(tree_penguin_class)\ntext(tree_penguin_class, pretty = 0)"
  },
  {
    "objectID": "lectures/lecture_25.html#pruning-2",
    "href": "lectures/lecture_25.html#pruning-2",
    "title": "Tree-Based Methods",
    "section": "Pruning",
    "text": "Pruning\n\nCodeSummaryPlotPruning\n\n\n\nattach(df)\ntree_penguin_cv &lt;- cv.tree(tree_penguin)\n\n\n\n\ntree_penguin_cv\n\n#&gt; $size\n#&gt; [1] 8 7 6 5 4 3 2 1\n#&gt; \n#&gt; $dev\n#&gt; [1] 29082655 29153810 29153810 29392818 29382033 36684481 35584478 35584478\n#&gt; \n#&gt; $k\n#&gt; [1]     -Inf  1120613  1141260  2532512  2599318  5547556  9660309 67364500\n#&gt; \n#&gt; $method\n#&gt; [1] \"deviance\"\n#&gt; \n#&gt; attr(,\"class\")\n#&gt; [1] \"prune\"         \"tree.sequence\"\n\n\n\n\n\nplot(tree_penguin_cv$size,\n     tree_penguin_cv$dev, type = \"b\")\n\n\n\n\n\n\n\n\n\n\n\nprune_best &lt;- prune.tree(tree_penguin, best = 7)\nplot(prune_best)\ntext(prune_best, pretty = 0)"
  },
  {
    "objectID": "lectures/lecture_23.html#learning-outcomes",
    "href": "lectures/lecture_23.html#learning-outcomes",
    "title": "Bayes Classifier",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nBayes Classifier\nLinear Discriminant Analysis\nQuadratic Discriminant Analysis\nNaive Bayes\nR Code"
  },
  {
    "objectID": "lectures/lecture_23.html#bayes-classifier-1",
    "href": "lectures/lecture_23.html#bayes-classifier-1",
    "title": "Bayes Classifier",
    "section": "Bayes Classifier",
    "text": "Bayes Classifier\nBayes Classifier is used to classify a data point to a category \\(c\\), given a set of predictors \\(\\boldsymbol x\\)\n\\[\n\\Delta(\\boldsymbol x) = argmax_{k \\in K} f_k(c_k|\\boldsymbol X)\n\\]\n\n\\(c_1, c_2,\\ldots, c_K\\): Categories\n\\(f_k(c_k|\\boldsymbol X)\\): conditional density function of \\(c_k\\) given \\(\\boldsymbol X\\)"
  },
  {
    "objectID": "lectures/lecture_23.html#probability",
    "href": "lectures/lecture_23.html#probability",
    "title": "Bayes Classifier",
    "section": "Probability",
    "text": "Probability\n\\[\nf_k(c_k|\\boldsymbol X ) = \\frac{f_k(\\boldsymbol X)\\pi_c}{f(\\boldsymbol X)}\n\\]\n\n\\(f_k(\\boldsymbol X)\\): conditional density of \\(\\boldsymbol X\\) given \\(c_k\\)\n\\(\\pi_k\\): probability of observing \\(c_k\\)\n\\(f(\\boldsymbol X)\\): probability density function of \\(\\boldsymbol X\\)"
  },
  {
    "objectID": "lectures/lecture_23.html#lda",
    "href": "lectures/lecture_23.html#lda",
    "title": "Bayes Classifier",
    "section": "LDA",
    "text": "LDA\nLinear Discriminant Analysis is used to classify a new data point, from a set of classifications, given information from a set of predictors.\nLDA classifies data using a Bayes classifier and imposing a normal distribution to the model."
  },
  {
    "objectID": "lectures/lecture_23.html#lda-p1",
    "href": "lectures/lecture_23.html#lda-p1",
    "title": "Bayes Classifier",
    "section": "LDA (p=1)",
    "text": "LDA (p=1)\n\\[\nf_k(\\boldsymbol X) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{\\frac{(x-\\mu_k)^2}{2\\sigma^2}\\right\\}\n\\]\n\\[\nf(X) = \\sum^K_{l=1} \\pi_l f_l(X)\n\\]"
  },
  {
    "objectID": "lectures/lecture_23.html#lda-p1-1",
    "href": "lectures/lecture_23.html#lda-p1-1",
    "title": "Bayes Classifier",
    "section": "LDA (p=1)",
    "text": "LDA (p=1)\n\\[\n\\delta_k = f_k(c_k|\\boldsymbol X ) = \\frac{f_k(\\boldsymbol X)\\pi_c}{f(\\boldsymbol X)}\n\\]\n\\[\n\\delta_k(x) = x\\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{\\sigma^2} + \\ln(\\pi_k)\n\\]"
  },
  {
    "objectID": "lectures/lecture_23.html#lda-p1-estimates",
    "href": "lectures/lecture_23.html#lda-p1-estimates",
    "title": "Bayes Classifier",
    "section": "LDA (p=1) Estimates",
    "text": "LDA (p=1) Estimates\nLet \\(Y_i=c_l\\), \\(l=1\\ldots, K\\), and \\(X_i=x_i\\) bet the data from n observations:\n\\[\n\\hat\\mu_k = \\frac{1}{n_k}\\sum^n_{i=1(Y_i=c_k)} x_i\n\\]\n\\[\n\\hat\\sigma^2=\\frac{1}{n-K}\\sum^K_{l=1}\\sum_{i=1(Y_i=c_l)}^n(x_i-\\hat\\mu_l)^2\n\\]\n\n\\(n_k\\): number of observations in class \\(k\\)"
  },
  {
    "objectID": "lectures/lecture_23.html#lda-p1-2",
    "href": "lectures/lecture_23.html#lda-p1-2",
    "title": "Bayes Classifier",
    "section": "LDA (p>1)",
    "text": "LDA (p&gt;1)\n\\[\nf_k(\\boldsymbol X) = \\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}\\exp\\left\\{(\\boldsymbol x-\\boldsymbol{\\mu_k})^{\\mathrm T}\\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu_k)\\right\\}\n\\]\n\\[\nf(\\boldsymbol X) = \\sum^K_{l=1} \\pi_l f_l(\\boldsymbol X)\n\\]"
  },
  {
    "objectID": "lectures/lecture_23.html#lda-p1-3",
    "href": "lectures/lecture_23.html#lda-p1-3",
    "title": "Bayes Classifier",
    "section": "LDA (p>1)",
    "text": "LDA (p&gt;1)\n\\[\n\\delta_k(\\boldsymbol x) = \\boldsymbol x^{\\mathrm T}\\Sigma^{-1}\\boldsymbol \\mu_k-\\frac{1}{2}\\boldsymbol \\mu_k^{\\mathrm T}\\Sigma^{-1}\\boldsymbol \\mu_k + \\ln(\\pi_k)\n\\]"
  },
  {
    "objectID": "lectures/lecture_23.html#lda-classification",
    "href": "lectures/lecture_23.html#lda-classification",
    "title": "Bayes Classifier",
    "section": "LDA Classification",
    "text": "LDA Classification\nClassify each new data point as class \\(c_k\\) based on the largest \\(\\delta_k(\\boldsymbol X)\\)."
  },
  {
    "objectID": "lectures/lecture_23.html#qda",
    "href": "lectures/lecture_23.html#qda",
    "title": "Bayes Classifier",
    "section": "QDA",
    "text": "QDA\nIn LDA, it is assumed that \\(\\Sigma\\) from \\(\\boldsymbol X\\) is the same for all classification groups. In Quadratic Discriminant Analysis, this assumption is relaxed, resulting in \\(\\Sigma_k\\) for each classification."
  },
  {
    "objectID": "lectures/lecture_23.html#qda-1",
    "href": "lectures/lecture_23.html#qda-1",
    "title": "Bayes Classifier",
    "section": "QDA",
    "text": "QDA\n\\[\nf_k(\\boldsymbol X) = \\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}\\exp\\left\\{(\\boldsymbol x-\\boldsymbol{\\mu_k})^{\\mathrm T}\\Sigma_k^{-1}(\\boldsymbol x-\\boldsymbol \\mu_k)\\right\\}\n\\]"
  },
  {
    "objectID": "lectures/lecture_23.html#qda-2",
    "href": "lectures/lecture_23.html#qda-2",
    "title": "Bayes Classifier",
    "section": "QDA",
    "text": "QDA\n\\[\n\\delta_k(\\boldsymbol x) = -\\frac{1}{2}\\boldsymbol x^{\\mathrm T}\\Sigma^{-1}\\boldsymbol x + \\boldsymbol x^{\\mathrm T}\\Sigma_k^{-1}\\boldsymbol \\mu_k-\\frac{1}{2}\\boldsymbol \\mu_k^{\\mathrm T}\\Sigma_k^{-1}\\boldsymbol \\mu_k - \\frac{1}{2}\\ln|\\Sigma_k| + \\ln(\\pi_k)\n\\]"
  },
  {
    "objectID": "lectures/lecture_23.html#naive-bayes-1",
    "href": "lectures/lecture_23.html#naive-bayes-1",
    "title": "Bayes Classifier",
    "section": "Naive Bayes",
    "text": "Naive Bayes\nA Naive Bayes classifier, assumes the predictors in \\(\\boldsymbol X\\) are independent of each other."
  },
  {
    "objectID": "lectures/lecture_23.html#naive-bayes-2",
    "href": "lectures/lecture_23.html#naive-bayes-2",
    "title": "Bayes Classifier",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\\[\nf_k(\\boldsymbol X) = \\prod^p_{j} f_{jk}(x_j|c_k)\n\\]"
  },
  {
    "objectID": "lectures/lecture_23.html#naive-bayes-3",
    "href": "lectures/lecture_23.html#naive-bayes-3",
    "title": "Bayes Classifier",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\n\nQuantitative\n\nNormal: \\(N(\\mu_{jk}, \\sigma^2_{jk})\\)\nNonparametric\n\nKernel Density\n\n\n\nQualitative\n\nNonparametric"
  },
  {
    "objectID": "lectures/lecture_23.html#lda-1",
    "href": "lectures/lecture_23.html#lda-1",
    "title": "Bayes Classifier",
    "section": "LDA",
    "text": "LDA\n\nlibrary(palmerpenguins)\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(magrittr)\n\npenguins_df &lt;- penguins %&gt;% drop_na\nx_lda &lt;- penguins_df %$% lda(species ~ bill_length_mm + \n                                       bill_depth_mm + \n                                       flipper_length_mm + \n                                       body_mass_g)"
  },
  {
    "objectID": "lectures/lecture_23.html#lda-prediction",
    "href": "lectures/lecture_23.html#lda-prediction",
    "title": "Bayes Classifier",
    "section": "LDA Prediction",
    "text": "LDA Prediction\n\nnew_df &lt;- penguins_df %&gt;% select(bill_length_mm, \n                                 bill_depth_mm,\n                                 flipper_length_mm,\n                                 body_mass_g)\nx_lda_predict &lt;- x_lda %&gt;% predict(new_df)"
  },
  {
    "objectID": "lectures/lecture_23.html#lda-confusion-matrix",
    "href": "lectures/lecture_23.html#lda-confusion-matrix",
    "title": "Bayes Classifier",
    "section": "LDA Confusion Matrix",
    "text": "LDA Confusion Matrix\n\ntable(penguins_df$species, x_lda_predict$class)\n\n#&gt;            \n#&gt;             Adelie Chinstrap Gentoo\n#&gt;   Adelie       145         1      0\n#&gt;   Chinstrap      3        65      0\n#&gt;   Gentoo         0         0    119"
  },
  {
    "objectID": "lectures/lecture_23.html#qda-3",
    "href": "lectures/lecture_23.html#qda-3",
    "title": "Bayes Classifier",
    "section": "QDA",
    "text": "QDA\n\nx_qda &lt;- penguins_df %$% qda(species ~ bill_length_mm + \n                                       bill_depth_mm + \n                                       flipper_length_mm + \n                                       body_mass_g)"
  },
  {
    "objectID": "lectures/lecture_23.html#qda-prediction",
    "href": "lectures/lecture_23.html#qda-prediction",
    "title": "Bayes Classifier",
    "section": "QDA Prediction",
    "text": "QDA Prediction\n\nx_qda_predict &lt;- x_qda %&gt;% predict(penguins_df %&gt;% \n                                     select(bill_length_mm, \n                                            bill_depth_mm,\n                                            flipper_length_mm,\n                                            body_mass_g))"
  },
  {
    "objectID": "lectures/lecture_23.html#qda-confusion-matrix",
    "href": "lectures/lecture_23.html#qda-confusion-matrix",
    "title": "Bayes Classifier",
    "section": "QDA Confusion Matrix",
    "text": "QDA Confusion Matrix\n\ntable(penguins_df$species, x_qda_predict$class)\n\n#&gt;            \n#&gt;             Adelie Chinstrap Gentoo\n#&gt;   Adelie       144         2      0\n#&gt;   Chinstrap      2        66      0\n#&gt;   Gentoo         0         0    119"
  },
  {
    "objectID": "lectures/lecture_23.html#naive-bayes-4",
    "href": "lectures/lecture_23.html#naive-bayes-4",
    "title": "Bayes Classifier",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nlibrary(e1071)\nx_nb &lt;- penguins_df %&gt;% naiveBayes(species ~ bill_length_mm +\n                                             bill_depth_mm + \n                                             flipper_length_mm + \n                                             body_mass_g, .)"
  },
  {
    "objectID": "lectures/lecture_23.html#naive-bayes-prediction",
    "href": "lectures/lecture_23.html#naive-bayes-prediction",
    "title": "Bayes Classifier",
    "section": "Naive Bayes Prediction",
    "text": "Naive Bayes Prediction\n\nx_nb_predict &lt;- x_nb %&gt;% predict(penguins_df %&gt;% \n                                     select(bill_length_mm, \n                                            bill_depth_mm,\n                                            flipper_length_mm,\n                                            body_mass_g))"
  },
  {
    "objectID": "lectures/lecture_23.html#naive-bayes-confusion-matrix",
    "href": "lectures/lecture_23.html#naive-bayes-confusion-matrix",
    "title": "Bayes Classifier",
    "section": "Naive Bayes Confusion Matrix",
    "text": "Naive Bayes Confusion Matrix\n\ntable(penguins_df$species, x_nb_predict)\n\n#&gt;            x_nb_predict\n#&gt;             Adelie Chinstrap Gentoo\n#&gt;   Adelie       141         5      0\n#&gt;   Chinstrap      5        63      0\n#&gt;   Gentoo         0         0    119"
  },
  {
    "objectID": "lectures/lecture_21.html#learning-outcomes",
    "href": "lectures/lecture_21.html#learning-outcomes",
    "title": "Introduction to Statistical Learning",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nIntroduction to Statistical Learning\nClassification vs Regression\nSupervised vs Unsupervised Machine Learning\nModel Adequacy"
  },
  {
    "objectID": "lectures/lecture_21.html#announcements",
    "href": "lectures/lecture_21.html#announcements",
    "title": "Introduction to Statistical Learning",
    "section": "Announcements",
    "text": "Announcements\nGraduate Seminar:\nSurvival Analysis: The Life and Death of Statistics\n6-7 PM Del Norte 1550"
  },
  {
    "objectID": "lectures/lecture_21.html#introduction-to-statistical-learning-1",
    "href": "lectures/lecture_21.html#introduction-to-statistical-learning-1",
    "title": "Introduction to Statistical Learning",
    "section": "Introduction to Statistical Learning",
    "text": "Introduction to Statistical Learning\nWhat is Statistical Learning?\n\nStatistical learning is the task of predicting an outcome of interest given a set of predictor variables."
  },
  {
    "objectID": "lectures/lecture_21.html#statistical-learning-model",
    "href": "lectures/lecture_21.html#statistical-learning-model",
    "title": "Introduction to Statistical Learning",
    "section": "Statistical Learning Model",
    "text": "Statistical Learning Model\n\\[\nY = f(\\boldsymbol X) + \\varepsilon\n\\]\n\n\\(Y\\): Outcome variable\n\\(f(\\cdot)\\): systematic component explaining \\(Y\\)\n\\(\\boldsymbol X\\): vector of predictor variables\n\\(\\varepsilon\\): error term"
  },
  {
    "objectID": "lectures/lecture_21.html#modeling-fcdot-parametric",
    "href": "lectures/lecture_21.html#modeling-fcdot-parametric",
    "title": "Introduction to Statistical Learning",
    "section": "Modeling \\(f(\\cdot)\\): Parametric",
    "text": "Modeling \\(f(\\cdot)\\): Parametric\n\nLinear Models\nGeneralized Linear Models (GLM)"
  },
  {
    "objectID": "lectures/lecture_21.html#modeling-fcdot-nonparametric",
    "href": "lectures/lecture_21.html#modeling-fcdot-nonparametric",
    "title": "Introduction to Statistical Learning",
    "section": "Modeling \\(f(\\cdot)\\): Nonparametric",
    "text": "Modeling \\(f(\\cdot)\\): Nonparametric\n\nGeneralized Additive Models\nLocal-Linear Models\nSmoothing Splines"
  },
  {
    "objectID": "lectures/lecture_21.html#prediction",
    "href": "lectures/lecture_21.html#prediction",
    "title": "Introduction to Statistical Learning",
    "section": "Prediction",
    "text": "Prediction\n\nStatistical Learning is only concerned with an accurate \\(Y\\)\n\\(f(\\cdot)\\) is considered a black box\nWe will not know how \\(\\boldsymbol X\\) explains \\(Y\\)\nWe choose flexible (nonparametric) models"
  },
  {
    "objectID": "lectures/lecture_21.html#model-interpretability",
    "href": "lectures/lecture_21.html#model-interpretability",
    "title": "Introduction to Statistical Learning",
    "section": "Model Interpretability",
    "text": "Model Interpretability\n\nWith a focus on prediction, model interpretability declines\nWe will not know how changes in \\(\\boldsymbol X\\) will affect \\(Y\\)"
  },
  {
    "objectID": "lectures/lecture_21.html#regression",
    "href": "lectures/lecture_21.html#regression",
    "title": "Introduction to Statistical Learning",
    "section": "Regression",
    "text": "Regression\nRegression in statistical learning terms indicates predicting a continuous random variable.\n\nWhat are the methods that we learned to model continuous random variables?"
  },
  {
    "objectID": "lectures/lecture_21.html#example",
    "href": "lectures/lecture_21.html#example",
    "title": "Introduction to Statistical Learning",
    "section": "Example",
    "text": "Example\n\nScatter PlotRegressionGAMCombined"
  },
  {
    "objectID": "lectures/lecture_21.html#classification",
    "href": "lectures/lecture_21.html#classification",
    "title": "Introduction to Statistical Learning",
    "section": "Classification",
    "text": "Classification\nClassification in statistical learning terms indicates predicting a categorical random variable.\n\nWhat are the methods that we learned to model categorical random variables?"
  },
  {
    "objectID": "lectures/lecture_21.html#example-1",
    "href": "lectures/lecture_21.html#example-1",
    "title": "Introduction to Statistical Learning",
    "section": "Example",
    "text": "Example\n\nDataKNNSVMNN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#&gt;             iris_pred\n#&gt;              setosa versicolor virginica\n#&gt;   setosa         20          0         0\n#&gt;   versicolor      0         19         1\n#&gt;   virginica       0          0        20\n\n\n\n\n\n\n#&gt;             Actual\n#&gt; Predicted    setosa versicolor virginica\n#&gt;   setosa         50          0         0\n#&gt;   versicolor      0         48         2\n#&gt;   virginica       0          2        48\n\n\n\n\n\n\n#&gt;             \n#&gt; predicted    setosa versicolor virginica\n#&gt;   setosa         25          0         0\n#&gt;   versicolor      0         25         3\n#&gt;   virginica       0          2        20"
  },
  {
    "objectID": "lectures/lecture_21.html#machine-learning",
    "href": "lectures/lecture_21.html#machine-learning",
    "title": "Introduction to Statistical Learning",
    "section": "Machine Learning",
    "text": "Machine Learning\nMachine learning is a set of methods used for predicting and classifying data. Several statistical methods are considered machine learning techniques.\n\nCommon Methods\n\nRegression\nMixed-Effects\nNonparametric Regression\nNeural Networks\nTree-based methods\nBayesian Methods"
  },
  {
    "objectID": "lectures/lecture_21.html#training-data",
    "href": "lectures/lecture_21.html#training-data",
    "title": "Introduction to Statistical Learning",
    "section": "Training Data",
    "text": "Training Data\nTraining Data is the data set used to construct a model."
  },
  {
    "objectID": "lectures/lecture_21.html#supervised",
    "href": "lectures/lecture_21.html#supervised",
    "title": "Introduction to Statistical Learning",
    "section": "Supervised",
    "text": "Supervised\nSupervised Machine Learning techniques are techniques where the training data contains the outcome."
  },
  {
    "objectID": "lectures/lecture_21.html#unsupervised",
    "href": "lectures/lecture_21.html#unsupervised",
    "title": "Introduction to Statistical Learning",
    "section": "Unsupervised",
    "text": "Unsupervised\nUnsupervised Machine Learning techniques are techniques where the training data does not contains the outcome."
  },
  {
    "objectID": "lectures/lecture_21.html#quality-of-fit-regression",
    "href": "lectures/lecture_21.html#quality-of-fit-regression",
    "title": "Introduction to Statistical Learning",
    "section": "Quality of Fit: Regression",
    "text": "Quality of Fit: Regression\n\\[\nMSE = \\frac{1}{n}\\sum^n_{i=1}\\{y_i - f(\\boldsymbol x_i)\\}^2\n\\]"
  },
  {
    "objectID": "lectures/lecture_21.html#quality-of-fit-classification",
    "href": "lectures/lecture_21.html#quality-of-fit-classification",
    "title": "Introduction to Statistical Learning",
    "section": "Quality of Fit: Classification",
    "text": "Quality of Fit: Classification\n\\[\nER = \\frac{1}{n}\\sum^n_{i=1}I(y_i \\ne \\hat y_i)\n\\]"
  },
  {
    "objectID": "lectures/lecture_21.html#bias-variance-tradeoff",
    "href": "lectures/lecture_21.html#bias-variance-tradeoff",
    "title": "Introduction to Statistical Learning",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff"
  },
  {
    "objectID": "lectures/lecture_21.html#bias-variance-tradeoff-1",
    "href": "lectures/lecture_21.html#bias-variance-tradeoff-1",
    "title": "Introduction to Statistical Learning",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff"
  },
  {
    "objectID": "lectures/lecture_2.html#r-packages",
    "href": "lectures/lecture_2.html#r-packages",
    "title": "R Basics",
    "section": "R Packages",
    "text": "R Packages\n\nTidyverse\ncsucistats\n\n\ninstall.packages('csucistats', \n  repos = c('https://inqs909.r-universe.dev', \n  'https://cloud.r-project.org'))"
  },
  {
    "objectID": "lectures/lecture_2.html#r-as-a-calculator",
    "href": "lectures/lecture_2.html#r-as-a-calculator",
    "title": "R Basics",
    "section": "R as a calculator",
    "text": "R as a calculator\nR can evaluate different expressions in the console tab.\nTry the following:\n\n\\(4(4+2)/34\\)\n\\(6^3\\)\n\\(3-1\\)\n\\(4+4/3+45(32*34-54)\\)"
  },
  {
    "objectID": "lectures/lecture_2.html#r-functions",
    "href": "lectures/lecture_2.html#r-functions",
    "title": "R Basics",
    "section": "R Functions",
    "text": "R Functions\nR functions performs tasks to specific data values.\nEvaluate the following values in R:\n\n\\(\\sqrt{3}\\)\n\\(e^3\\)\n\\(\\ln(53)\\)\n\\(\\log(324)\\)\n\\(\\sin(3)\\)\n\\(\\sin(3\\pi)\\)"
  },
  {
    "objectID": "lectures/lecture_2.html#types-of-data",
    "href": "lectures/lecture_2.html#types-of-data",
    "title": "R Basics",
    "section": "Types of Data",
    "text": "Types of Data\n\nNumeric\nCharacter\nLogical\nMissing\n\nEvaluate the following code:\n\nis.numeric(1)\nis.numeric(\"1\")\nis.numeric(T)\nis.numeric(NA)"
  },
  {
    "objectID": "lectures/lecture_2.html#types-of-objects",
    "href": "lectures/lecture_2.html#types-of-objects",
    "title": "R Basics",
    "section": "Types of Objects",
    "text": "Types of Objects\nIn R, an object contains a set of data. The most common types are vectors and matrix.\nRun this code and print out the objects in the console:\n\nx &lt;- 3:34\ny &lt;- matrix(1:20, nrow = 4)"
  },
  {
    "objectID": "lectures/lecture_2.html#data-frames",
    "href": "lectures/lecture_2.html#data-frames",
    "title": "R Basics",
    "section": "Data Frames",
    "text": "Data Frames\nData frames can be thought of as R’s version of a data set.\nPlay around with mtcars:\n\nmtcars \n\n#&gt;                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n#&gt; Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n#&gt; Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n#&gt; Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\n#&gt; Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\n#&gt; Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\n#&gt; Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\n#&gt; Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\n#&gt; Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n#&gt; Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n#&gt; Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\n#&gt; Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n#&gt; Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\n#&gt; Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n#&gt; Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\n#&gt; Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\n#&gt; Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n#&gt; Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n#&gt; Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n#&gt; Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n#&gt; Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n#&gt; Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\n#&gt; Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\n#&gt; AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\n#&gt; Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\n#&gt; Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\n#&gt; Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n#&gt; Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n#&gt; Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n#&gt; Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\n#&gt; Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n#&gt; Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n#&gt; Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2"
  },
  {
    "objectID": "lectures/lecture_2.html#lists",
    "href": "lectures/lecture_2.html#lists",
    "title": "R Basics",
    "section": "Lists",
    "text": "Lists\nList can be thought as an extended vector, but each element is a different R object.\nTry playing with this R object:\n\nlist_one &lt;- list(mtcars, rep(0, 4),\n                 diag(rep(1, 3)))"
  },
  {
    "objectID": "lectures/lecture_18.html#learning-outcomes",
    "href": "lectures/lecture_18.html#learning-outcomes",
    "title": "Generalized Linear Models",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nEstimation Procedures\n\nRegression Coefficients\nDispersion Parameter\n\nNewton-Raphson Algorithm"
  },
  {
    "objectID": "lectures/lecture_18.html#estimating-boldsymbolbeta",
    "href": "lectures/lecture_18.html#estimating-boldsymbolbeta",
    "title": "Generalized Linear Models",
    "section": "Estimating \\(\\boldsymbol\\beta\\)",
    "text": "Estimating \\(\\boldsymbol\\beta\\)\nTo obtain the estimates of \\(\\boldsymbol \\beta\\) we can use the maximum log-likelihood approach to obtain \\(\\hat{\\boldsymbol\\beta}\\).\n\\[\nL(\\boldsymbol \\beta) =  \\prod^n_{i=1}f\\left(y_i|\\boldsymbol X_i;\\boldsymbol \\beta,\\phi\\right)\n\\]"
  },
  {
    "objectID": "lectures/lecture_18.html#maximum-likelihood-approach",
    "href": "lectures/lecture_18.html#maximum-likelihood-approach",
    "title": "Generalized Linear Models",
    "section": "Maximum Likelihood Approach",
    "text": "Maximum Likelihood Approach\n\\[\n\\ell(\\boldsymbol \\beta) =  \\sum^n_{i=1}\\log\\left\\{f\\left(y_i|\\boldsymbol X_i;\\boldsymbol \\beta,\\phi\\right)\\right\\}\n\\]"
  },
  {
    "objectID": "lectures/lecture_18.html#numerical-approaches",
    "href": "lectures/lecture_18.html#numerical-approaches",
    "title": "Generalized Linear Models",
    "section": "Numerical Approaches",
    "text": "Numerical Approaches\n\nNewton-Rhapson Algorithm\nFisher-Scoring Algorithm\nNelder-Mead\nBFGS"
  },
  {
    "objectID": "lectures/lecture_18.html#estimating-phi-1",
    "href": "lectures/lecture_18.html#estimating-phi-1",
    "title": "Generalized Linear Models",
    "section": "Estimating \\(\\phi\\)",
    "text": "Estimating \\(\\phi\\)\nDepending on the random variable, the dispersion parameter will need to be estimated to conduct inference procedures. There are 4 methods to estimate the dispersion parameter:\n\nMaximum Likelihood\nMaximum (Modified) Profile Likelihood Approach\nMean Deviance Estimator\nPearson Estimator"
  },
  {
    "objectID": "lectures/lecture_18.html#maximum-likelihood-approach-1",
    "href": "lectures/lecture_18.html#maximum-likelihood-approach-1",
    "title": "Generalized Linear Models",
    "section": "Maximum Likelihood Approach",
    "text": "Maximum Likelihood Approach\n\\[\n\\ell(\\phi) =  \\sum^n_{i=1}\\log\\left\\{f\\left(y_i|\\boldsymbol X_i;\\boldsymbol \\beta,\\phi\\right)\\right\\}\n\\]"
  },
  {
    "objectID": "lectures/lecture_18.html#maximum-modified-profile-likelihood-approach",
    "href": "lectures/lecture_18.html#maximum-modified-profile-likelihood-approach",
    "title": "Generalized Linear Models",
    "section": "Maximum (Modified) Profile Likelihood Approach",
    "text": "Maximum (Modified) Profile Likelihood Approach\n\\[\n\\ell_p(\\phi) = \\frac{p}{2}\\log \\phi + \\sum^n_{i=1}\\log\\left\\{f\\left(y_i|\\boldsymbol X_i;\\hat{\\boldsymbol \\beta},\\phi\\right)\\right\\}\n\\]"
  },
  {
    "objectID": "lectures/lecture_18.html#mean-deviance-estimator",
    "href": "lectures/lecture_18.html#mean-deviance-estimator",
    "title": "Generalized Linear Models",
    "section": "Mean Deviance Estimator",
    "text": "Mean Deviance Estimator\n\\[\n\\tilde \\phi = \\frac{D(y,\\hat\\mu)}{n-p}\n\\]\n\n\\(D(y,\\hat\\mu)=2\\sum^n_{i=1}\\left\\{t(y,y) - t(y,\\mu) \\right\\}\\)\n\\(t(y,\\mu)=y\\theta-\\kappa(\\theta)\\)\n\\(p\\): number of regression coefficients"
  },
  {
    "objectID": "lectures/lecture_18.html#pearson-estimator",
    "href": "lectures/lecture_18.html#pearson-estimator",
    "title": "Generalized Linear Models",
    "section": "Pearson Estimator",
    "text": "Pearson Estimator\n\\[\n\\bar \\phi = \\frac{\\Lambda^2}{n-p}\n\\]\n\n\\(\\Lambda^2=\\sum^n_{i=1}\\frac{y_i-\\hat\\mu_i}{V(\\hat\\mu_i)}\\)\n\\(\\hat \\mu_i = g^{-1}(\\hat\\beta_0 + \\sum^n_{j=1}{X_{ij}\\hat\\beta_j})\\)\n\\(V(\\hat\\mu_i)=\\frac{d^2\\kappa(\\hat\\theta_i)}{d\\theta_i^2}\\)"
  },
  {
    "objectID": "lectures/lecture_16.html#learning-objective",
    "href": "lectures/lecture_16.html#learning-objective",
    "title": "Model Building",
    "section": "Learning Objective",
    "text": "Learning Objective\n\nFixing Assumptions\nModel Building"
  },
  {
    "objectID": "lectures/lecture_16.html#fixing-assumptions-1",
    "href": "lectures/lecture_16.html#fixing-assumptions-1",
    "title": "Model Building",
    "section": "Fixing Assumptions",
    "text": "Fixing Assumptions\n\nLinearity\nCollinearity\nUnequal Variances\nInfluential observations and Outliers"
  },
  {
    "objectID": "lectures/lecture_16.html#residual-function",
    "href": "lectures/lecture_16.html#residual-function",
    "title": "Model Building",
    "section": "Residual Function",
    "text": "Residual Function\n\ndf_resid &lt;- function(x){\n  res &lt;- data.frame(obs = 1:nrow(x$model),\n                    x$model, \n                    resid = resid(x),\n                    fitted = fitted(x),\n                    sresid = rstandard(x),\n                    hatvals = hatvalues(x),\n                    jackknife =  rstudent(x),\n                    cooks = cooks.distance(x)\n                    )\n  return(res)\n}"
  },
  {
    "objectID": "lectures/lecture_16.html#linearity-logx",
    "href": "lectures/lecture_16.html#linearity-logx",
    "title": "Model Building",
    "section": "Linearity \\(\\log(x)\\)",
    "text": "Linearity \\(\\log(x)\\)\n\nx &lt;- rnorm(50, 6, 2)\ny &lt;- 3 - 2 * log(x) + rnorm(50, sd = 0.25)\nx_lm &lt;- lm(y ~ x)\nresid_df &lt;- df_resid(x_lm)\nggplot(resid_df, aes(x, resid)) + geom_point() +\n  stat_smooth(se = F) +\n  geom_hline(yintercept = 0)+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nx_lm &lt;- lm(y ~ log(x))\nresid_df &lt;- df_resid(x_lm)\nggplot(resid_df, aes(x, resid)) + geom_point() +\n  stat_smooth(se = F) +\n  geom_hline(yintercept = 0)+\n  theme_bw()"
  },
  {
    "objectID": "lectures/lecture_16.html#linearity-1x",
    "href": "lectures/lecture_16.html#linearity-1x",
    "title": "Model Building",
    "section": "Linearity \\(1/x\\)",
    "text": "Linearity \\(1/x\\)\n\nx &lt;- rnorm(50, 6, 1.5)\ny &lt;- 3 - 2 / x + rnorm(50, sd = 0.2)\nx_lm &lt;- lm(y ~ x)\nresid_df &lt;- df_resid(x_lm)\nggplot(resid_df, aes(x, resid)) + geom_point() +\n  stat_smooth(se = F) +\n  geom_hline(yintercept = 0)+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nx_lm &lt;- lm(y ~ I(1/x))\nresid_df &lt;- df_resid(x_lm)\nggplot(resid_df, aes(x, resid)) + geom_point() +\n  stat_smooth(se = F) +\n  geom_hline(yintercept = 0)+\n  theme_bw()"
  },
  {
    "objectID": "lectures/lecture_16.html#other-common-covariate-function-transformations",
    "href": "lectures/lecture_16.html#other-common-covariate-function-transformations",
    "title": "Model Building",
    "section": "Other Common Covariate Function Transformations",
    "text": "Other Common Covariate Function Transformations\n\n\\(\\log_{10}(x)\\)\n\\(x^2\\)\n\\(\\sqrt x\\)\n\\(x^3\\)"
  },
  {
    "objectID": "lectures/lecture_16.html#unequal-variance",
    "href": "lectures/lecture_16.html#unequal-variance",
    "title": "Model Building",
    "section": "Unequal Variance",
    "text": "Unequal Variance\nThere are a couple of methods to adjust for unequal variances\n\nGeneralized Least Squares\nMixed-Effects Models"
  },
  {
    "objectID": "lectures/lecture_16.html#collinearity",
    "href": "lectures/lecture_16.html#collinearity",
    "title": "Model Building",
    "section": "Collinearity",
    "text": "Collinearity\nThere are couple of methods to fix collinearity:\n\nPrincipal Components Analysis\nRidge Regression\nLasso Regression"
  },
  {
    "objectID": "lectures/lecture_16.html#model-building-1",
    "href": "lectures/lecture_16.html#model-building-1",
    "title": "Model Building",
    "section": "Model Building",
    "text": "Model Building\nWhen given a set of predictors, we want to build a model that only contains predictors that best fits the data, without overfitting.\nIdeally, we always want to choose a parsimonious model that best describes the outcome variable. The more predictors into the model, the less parsimonious and less powerful.\nChoosing the best model can be done based on selection criteria such as Mallow’s \\(C_p\\), AIC, AICc, BIC, and adjusted \\(R^2\\)."
  },
  {
    "objectID": "lectures/lecture_16.html#model-building-2",
    "href": "lectures/lecture_16.html#model-building-2",
    "title": "Model Building",
    "section": "Model Building",
    "text": "Model Building\n\nBest Subset Model\n\nFit all models and select the best model based criteria\n\nForward Stepwise Model Building\n\nBegin with the null model (\\(Y\\sim 1\\)) and add variables until a final model is chosen.\n\nBackward Stepwise Model Building\n\nBegin with the full model, and remove variable until the final model is chosen.\n\nHybrid Stepwise Regression\n\nA hybrid approach between the forward and backward building approach."
  },
  {
    "objectID": "lectures/lecture_16.html#best-subset-model-building",
    "href": "lectures/lecture_16.html#best-subset-model-building",
    "title": "Model Building",
    "section": "Best Subset Model Building",
    "text": "Best Subset Model Building\n\nBegin with the null model, no predictors\nFor \\(k=1,\\ldots, p\\) (number of predictors):\n\nFit all \\(\\left(^p_k\\right)\\) models that contain \\(k\\) predictors\nDefine \\(M_k\\) as the model with the largest \\(R²\\)\n\nThe final model is the model \\(M_k\\) based on selection criteria"
  },
  {
    "objectID": "lectures/lecture_16.html#forward-stepwise-model-building",
    "href": "lectures/lecture_16.html#forward-stepwise-model-building",
    "title": "Model Building",
    "section": "Forward Stepwise Model Building",
    "text": "Forward Stepwise Model Building\n\nBegin with the null model, no predictors\nFor \\(k=0,\\ldots, p-1\\) (number of predictors):\n\nFit all \\(p-k\\) models that adds one new predictor to the orginal model containing \\(k\\) predictors\nDefine \\(M_{k+1}\\) as the model with the largest \\(R²\\) among \\(p-k\\) models\n\nThe final model is the model \\(M_(k+1)\\) based on selection criteria"
  },
  {
    "objectID": "lectures/lecture_16.html#backward-stepwise-model-building",
    "href": "lectures/lecture_16.html#backward-stepwise-model-building",
    "title": "Model Building",
    "section": "Backward Stepwise Model Building",
    "text": "Backward Stepwise Model Building\n\nBegin with the full model \\(M_p\\), with all predictors\nFor \\(k=p,p-1, \\ldots, 1\\) (number of predictors):\n\nFit all models that contain \\(k-1\\) predictors\nDefine \\(M_{k-1}\\) as the model with the largest \\(R²\\)\n\nThe final model is the model \\(M_k\\) based on selection criteria"
  },
  {
    "objectID": "lectures/lecture_16.html#ridge-regression",
    "href": "lectures/lecture_16.html#ridge-regression",
    "title": "Model Building",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nMinimizes the following function\n\\[\n\\sum^n_{i=1}(y_i-\\boldsymbol X_i^\\mathrm T \\boldsymbol \\beta)^2 +\\lambda \\boldsymbol \\beta ^\\mathrm T\\boldsymbol \\beta\n\\]\n\n\\(\\boldsymbol X_i\\): Design matrix for \\(i\\)th observation\n\\(\\boldsymbol \\beta\\): regression coefficients\n\\(\\lambda &gt; 0\\): tuning parameter"
  },
  {
    "objectID": "lectures/lecture_16.html#lasso-regression",
    "href": "lectures/lecture_16.html#lasso-regression",
    "title": "Model Building",
    "section": "Lasso Regression",
    "text": "Lasso Regression\nMinimize the following function:\n\\[\n\\sum^n_{i=1}(y_i-\\boldsymbol X_i^\\mathrm T \\boldsymbol \\beta)^2 +\\lambda \\sum^p_{j=0}|\\beta_j|\n\\]"
  },
  {
    "objectID": "lectures/lecture_16.html#selecting-the-ideal-penalty-term",
    "href": "lectures/lecture_16.html#selecting-the-ideal-penalty-term",
    "title": "Model Building",
    "section": "Selecting the Ideal penalty term",
    "text": "Selecting the Ideal penalty term\nSelecting the correct penalty term is essential to ensure an ideal bias-variance trade-off. The best approach is to use a cross-validation approach, more specifically, the LOOCV (leave-one-out cross-validation)."
  },
  {
    "objectID": "lectures/lecture_16.html#cross-validation",
    "href": "lectures/lecture_16.html#cross-validation",
    "title": "Model Building",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nChoose a set of tuning parameters to test.\nFor each \\(k\\)th turning parameter Calculate the tuning parameter error for each value\n\nUtilize the leave-one-out approach\n\nFor each observation fit and compute:\n\\[\nMSE_i = (y_i - \\hat y_{i(i)})^2\n\\]\nCompute the following error:\n\\[\nCVE_k = \\frac{1}{n}\\sum^n_{i=1}MSE_i\n\\]\n\n\nIdentify the \\(k\\)th tuning parameter with the lowest \\(CVE_k\\)"
  },
  {
    "objectID": "lectures/lecture_16.html#mallows-c_p",
    "href": "lectures/lecture_16.html#mallows-c_p",
    "title": "Model Building",
    "section": "Mallow’s \\(C_p\\)",
    "text": "Mallow’s \\(C_p\\)\n\\[\nC_p = \\frac{1}{n}(RSS + 2 d \\hat \\sigma^2)\n\\]\n\n\\(RSS\\): Residual Sum of Squares\n\\(\\hat \\sigma^2\\): Mean Square Error\n\\(d\\): number of predictors\nLower is better"
  },
  {
    "objectID": "lectures/lecture_16.html#aikaike-information-criteria-aic",
    "href": "lectures/lecture_16.html#aikaike-information-criteria-aic",
    "title": "Model Building",
    "section": "Aikaike Information Criteria (AIC)",
    "text": "Aikaike Information Criteria (AIC)\n\\[\n\\frac{1}{n\\hat\\sigma^2}(RSS+2d\\hat\\sigma^2)\n\\]\n\nLower is Better"
  },
  {
    "objectID": "lectures/lecture_16.html#bayesian-information-criteria-bic",
    "href": "lectures/lecture_16.html#bayesian-information-criteria-bic",
    "title": "Model Building",
    "section": "Bayesian Information Criteria (BIC)",
    "text": "Bayesian Information Criteria (BIC)\n\\[\n\\frac{1}{n\\hat\\sigma^2}\\{RSS+\\log(n)d\\hat\\sigma^2\\}\n\\]\n\nLower is better"
  },
  {
    "objectID": "lectures/lecture_16.html#r2",
    "href": "lectures/lecture_16.html#r2",
    "title": "Model Building",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\n\\[\n1-\\frac{RSS}{TSS}\n\\]\n\n\\(RSS=\\sum^n_{i=1}(y_i-\\hat y_i)^2\\)\n\\(TSS=\\sum^n_{i=1}(y_i-\\bar y)^2\\)\nHigher is Better"
  },
  {
    "objectID": "lectures/lecture_16.html#adjusted-r2",
    "href": "lectures/lecture_16.html#adjusted-r2",
    "title": "Model Building",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\\[\n1-\\frac{RSS/(n-d-1)}{\nTSS/(n-1)}\n\\]\n\nHigher is Better"
  },
  {
    "objectID": "lectures/lecture_16.html#r-code",
    "href": "lectures/lecture_16.html#r-code",
    "title": "Model Building",
    "section": "R Code",
    "text": "R Code\n\nlibrary(leaps)\nregsubsets(y ~ ., data)"
  },
  {
    "objectID": "lectures/lecture_16.html#full-subset",
    "href": "lectures/lecture_16.html#full-subset",
    "title": "Model Building",
    "section": "Full Subset",
    "text": "Full Subset\n\nCodeSummaryPlotModel\n\n\n\nlm_full &lt;- regsubsets(mpg ~ ., data = mtcars)\n\n\n\n\nsum_lm_full &lt;- summary(lm_full)\nprint(sum_lm_full)\n\n#&gt; Subset selection object\n#&gt; Call: regsubsets.formula(mpg ~ ., data = mtcars)\n#&gt; 10 Variables  (and intercept)\n#&gt;      Forced in Forced out\n#&gt; cyl      FALSE      FALSE\n#&gt; disp     FALSE      FALSE\n#&gt; hp       FALSE      FALSE\n#&gt; drat     FALSE      FALSE\n#&gt; wt       FALSE      FALSE\n#&gt; qsec     FALSE      FALSE\n#&gt; vs       FALSE      FALSE\n#&gt; am       FALSE      FALSE\n#&gt; gear     FALSE      FALSE\n#&gt; carb     FALSE      FALSE\n#&gt; 1 subsets of each size up to 8\n#&gt; Selection Algorithm: exhaustive\n#&gt;          cyl disp hp  drat wt  qsec vs  am  gear carb\n#&gt; 1  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n#&gt; 2  ( 1 ) \"*\" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n#&gt; 3  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#&gt; 4  ( 1 ) \" \" \" \"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#&gt; 5  ( 1 ) \" \" \"*\"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#&gt; 6  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#&gt; 7  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \"*\"  \" \" \n#&gt; 8  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \"*\"  \"*\"\n\n\n\n\n\nplot(sum_lm_full$cp)\n\n\n\n\n\n\n\n\n\n\n\ncoef(lm_full,3)\n\n#&gt; (Intercept)          wt        qsec          am \n#&gt;    9.617781   -3.916504    1.225886    2.935837"
  },
  {
    "objectID": "lectures/lecture_16.html#forward-model-building",
    "href": "lectures/lecture_16.html#forward-model-building",
    "title": "Model Building",
    "section": "Forward Model Building",
    "text": "Forward Model Building\n\nCodeSummaryPlotModel\n\n\n\nlm_full &lt;- regsubsets(mpg ~ ., data = mtcars,\n                      method =  \"forward\")\n\n\n\n\nsum_lm_full &lt;- summary(lm_full)\nprint(sum_lm_full)\n\n#&gt; Subset selection object\n#&gt; Call: regsubsets.formula(mpg ~ ., data = mtcars, method = \"forward\")\n#&gt; 10 Variables  (and intercept)\n#&gt;      Forced in Forced out\n#&gt; cyl      FALSE      FALSE\n#&gt; disp     FALSE      FALSE\n#&gt; hp       FALSE      FALSE\n#&gt; drat     FALSE      FALSE\n#&gt; wt       FALSE      FALSE\n#&gt; qsec     FALSE      FALSE\n#&gt; vs       FALSE      FALSE\n#&gt; am       FALSE      FALSE\n#&gt; gear     FALSE      FALSE\n#&gt; carb     FALSE      FALSE\n#&gt; 1 subsets of each size up to 8\n#&gt; Selection Algorithm: forward\n#&gt;          cyl disp hp  drat wt  qsec vs  am  gear carb\n#&gt; 1  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n#&gt; 2  ( 1 ) \"*\" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n#&gt; 3  ( 1 ) \"*\" \" \"  \"*\" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n#&gt; 4  ( 1 ) \"*\" \" \"  \"*\" \" \"  \"*\" \" \"  \" \" \"*\" \" \"  \" \" \n#&gt; 5  ( 1 ) \"*\" \" \"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#&gt; 6  ( 1 ) \"*\" \"*\"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#&gt; 7  ( 1 ) \"*\" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#&gt; 8  ( 1 ) \"*\" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \"*\"  \" \"\n\n\n\n\n\nplot(sum_lm_full$cp)\n\n\n\n\n\n\n\n\n\n\n\ncoef(lm_full,3)\n\n#&gt; (Intercept)         cyl          hp          wt \n#&gt;  38.7517874  -0.9416168  -0.0180381  -3.1669731"
  },
  {
    "objectID": "lectures/lecture_16.html#backward-model-building",
    "href": "lectures/lecture_16.html#backward-model-building",
    "title": "Model Building",
    "section": "Backward Model Building",
    "text": "Backward Model Building\n\nCodeSummaryPlotModel\n\n\n\nlm_full &lt;- regsubsets(mpg ~ ., data = mtcars,\n                      method =  \"backward\")\n\n\n\n\nsum_lm_full &lt;- summary(lm_full)\nprint(sum_lm_full)\n\n#&gt; Subset selection object\n#&gt; Call: regsubsets.formula(mpg ~ ., data = mtcars, method = \"backward\")\n#&gt; 10 Variables  (and intercept)\n#&gt;      Forced in Forced out\n#&gt; cyl      FALSE      FALSE\n#&gt; disp     FALSE      FALSE\n#&gt; hp       FALSE      FALSE\n#&gt; drat     FALSE      FALSE\n#&gt; wt       FALSE      FALSE\n#&gt; qsec     FALSE      FALSE\n#&gt; vs       FALSE      FALSE\n#&gt; am       FALSE      FALSE\n#&gt; gear     FALSE      FALSE\n#&gt; carb     FALSE      FALSE\n#&gt; 1 subsets of each size up to 8\n#&gt; Selection Algorithm: backward\n#&gt;          cyl disp hp  drat wt  qsec vs  am  gear carb\n#&gt; 1  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n#&gt; 2  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \"*\"  \" \" \" \" \" \"  \" \" \n#&gt; 3  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#&gt; 4  ( 1 ) \" \" \" \"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#&gt; 5  ( 1 ) \" \" \"*\"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#&gt; 6  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n#&gt; 7  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \"*\"  \" \" \n#&gt; 8  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \"*\"  \"*\"\n\n\n\n\n\nplot(sum_lm_full$cp)\n\n\n\n\n\n\n\n\n\n\n\ncoef(lm_full,3)\n\n#&gt; (Intercept)          wt        qsec          am \n#&gt;    9.617781   -3.916504    1.225886    2.935837"
  },
  {
    "objectID": "lectures/lecture_16.html#ridge-and-lasso-regression",
    "href": "lectures/lecture_16.html#ridge-and-lasso-regression",
    "title": "Model Building",
    "section": "Ridge and Lasso Regression",
    "text": "Ridge and Lasso Regression\n\nlibrary(glmnet)\n# Ridge Regression\nglmnet(x, y, \n       alpha = 0, \n       lambda = grid)\n# Lasso Regression\nglmnet(x, y, \n       alpha = 1, \n       lambda = grid)"
  },
  {
    "objectID": "lectures/lecture_16.html#ridge-regression-1",
    "href": "lectures/lecture_16.html#ridge-regression-1",
    "title": "Model Building",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nPrepCVModel\n\n\n\nx &lt;- model.matrix(mpg ~ ., data = mtcars)[,-1]\ny &lt;- mtcars$mpg\n\ngrid_lambda &lt;- seq(0, 100, by = 0.1)\n\n\n\n\nridge_reg_cv &lt;- cv.glmnet(x, y,\n                    alpha = 0,\n                    lambda = grid_lambda)\nridge_reg_cv\n\n#&gt; \n#&gt; Call:  cv.glmnet(x = x, y = y, lambda = grid_lambda, alpha = 0) \n#&gt; \n#&gt; Measure: Mean-Squared Error \n#&gt; \n#&gt;     Lambda Index Measure    SE Nonzero\n#&gt; min    2.5   976   7.176 1.637      10\n#&gt; 1se   10.9   892   8.804 2.423      10\n\n\n\n\n\nridge_reg &lt;- glmnet(x, y,\n                    alpha = 0,\n                    lambda = grid_lambda)\n\ncoef(ridge_reg)[,974]\n\n#&gt;  (Intercept)          cyl         disp           hp         drat           wt \n#&gt; 21.143813495 -0.371451745 -0.005255145 -0.011639875  1.054826728 -1.238823689 \n#&gt;         qsec           vs           am         gear         carb \n#&gt;  0.162434931  0.767311761  1.628364774  0.544002049 -0.549534073"
  },
  {
    "objectID": "lectures/lecture_16.html#lasso-regression-1",
    "href": "lectures/lecture_16.html#lasso-regression-1",
    "title": "Model Building",
    "section": "Lasso Regression",
    "text": "Lasso Regression\n\nPrepCVModel\n\n\n\nx &lt;- model.matrix(mpg ~ ., data = mtcars)[,-1]\ny &lt;- mtcars$mpg\n\ngrid_lambda &lt;- seq(0, 100, by = 0.1)\n\n\n\n\nlasso_reg_cv &lt;- cv.glmnet(x, y,\n                    alpha = 1,\n                    lambda = grid_lambda)\nlasso_reg_cv\n\n#&gt; \n#&gt; Call:  cv.glmnet(x = x, y = y, lambda = grid_lambda, alpha = 1) \n#&gt; \n#&gt; Measure: Mean-Squared Error \n#&gt; \n#&gt;     Lambda Index Measure    SE Nonzero\n#&gt; min    0.7   994   8.101 2.347       3\n#&gt; 1se    1.5   986  10.089 3.572       3\n\n\n\n\n\nlasso_reg &lt;- glmnet(x, y,\n                    alpha = 1,\n                    lambda = grid_lambda)\n\ncoef(lasso_reg)[,993]\n\n#&gt; (Intercept)         cyl        disp          hp        drat          wt \n#&gt; 36.00374206 -0.88697325  0.00000000 -0.01170578  0.00000000 -2.70662163 \n#&gt;        qsec          vs          am        gear        carb \n#&gt;  0.00000000  0.00000000  0.00000000  0.00000000  0.00000000"
  },
  {
    "objectID": "lectures/lecture_14.html#learning-objectives",
    "href": "lectures/lecture_14.html#learning-objectives",
    "title": "Correlation, Confounding, and Interaction Terms",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nCorrelation vs Causation\nConfounding Variables\nInteraction Terms"
  },
  {
    "objectID": "lectures/lecture_14.html#correlation-vs-causation-1",
    "href": "lectures/lecture_14.html#correlation-vs-causation-1",
    "title": "Correlation, Confounding, and Interaction Terms",
    "section": "Correlation vs Causation",
    "text": "Correlation vs Causation\nWhen 2 variables are associated with each other, this does not necessarily mean that one causes the other component."
  },
  {
    "objectID": "lectures/lecture_14.html#correlation-vs-causation-2",
    "href": "lectures/lecture_14.html#correlation-vs-causation-2",
    "title": "Correlation, Confounding, and Interaction Terms",
    "section": "Correlation vs Causation",
    "text": "Correlation vs Causation\nLooking at body_mass_g and flipper_length_mm\n\nlibrary(palmerpenguins)\npenguins %&lt;&gt;% drop_na\npenguins %$% lm(body_mass_g ~ flipper_length_mm)\npenguins %$% lm(flipper_length_mm ~ body_mass_g)"
  },
  {
    "objectID": "lectures/lecture_14.html#correlation-vs-causation-3",
    "href": "lectures/lecture_14.html#correlation-vs-causation-3",
    "title": "Correlation, Confounding, and Interaction Terms",
    "section": "Correlation vs Causation",
    "text": "Correlation vs Causation\nLooking at body_mass_g and flipper_length_mm\n\nlibrary(palmerpenguins)\npenguins %&lt;&gt;% drop_na\npenguins %$% lm(body_mass_g ~ flipper_length_mm)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ flipper_length_mm)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)  flipper_length_mm  \n#&gt;          -5872.09              50.15\n\npenguins %$% lm(flipper_length_mm ~ body_mass_g)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = flipper_length_mm ~ body_mass_g)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)  body_mass_g  \n#&gt;    137.0396       0.0152"
  },
  {
    "objectID": "lectures/lecture_14.html#correlation-vs-causation-4",
    "href": "lectures/lecture_14.html#correlation-vs-causation-4",
    "title": "Correlation, Confounding, and Interaction Terms",
    "section": "Correlation vs Causation",
    "text": "Correlation vs Causation\nIn observation studies, we cannot claim any one factor causes an outcome. At best we can claim things are associated with each other. To determine causation, a criteria is followed, randomized-controlled trials, or causal inference methods."
  },
  {
    "objectID": "lectures/lecture_14.html#confounding-variables-1",
    "href": "lectures/lecture_14.html#confounding-variables-1",
    "title": "Correlation, Confounding, and Interaction Terms",
    "section": "Confounding Variables",
    "text": "Confounding Variables\nConfounding variables are tertiary variables that may cause an association between two variables when one does not exist."
  },
  {
    "objectID": "lectures/lecture_14.html#confounding-variable-example",
    "href": "lectures/lecture_14.html#confounding-variable-example",
    "title": "Correlation, Confounding, and Interaction Terms",
    "section": "Confounding Variable Example",
    "text": "Confounding Variable Example\nAs ice cream sales increase, the number of shark attacks increase."
  },
  {
    "objectID": "lectures/lecture_14.html#confounding-variables-2",
    "href": "lectures/lecture_14.html#confounding-variables-2",
    "title": "Correlation, Confounding, and Interaction Terms",
    "section": "Confounding Variables",
    "text": "Confounding Variables\n\nSource: Statology"
  },
  {
    "objectID": "lectures/lecture_14.html#simulation-example",
    "href": "lectures/lecture_14.html#simulation-example",
    "title": "Correlation, Confounding, and Interaction Terms",
    "section": "Simulation example",
    "text": "Simulation example\n\nz &lt;- rnorm(1000, mean = 8)\nxy &lt;- sapply(z, \\(.) mvtnorm::rmvnorm(1000, .*c(1,1), \n                                      matrix(c(1,0,0,1), nrow = 2)))\nlm(xy[1,]~xy[2,]) %&gt;% summary\nlm(xy[1,]~xy[2,] + z) %&gt;% summary"
  },
  {
    "objectID": "lectures/lecture_14.html#simulation-example-1",
    "href": "lectures/lecture_14.html#simulation-example-1",
    "title": "Correlation, Confounding, and Interaction Terms",
    "section": "Simulation Example",
    "text": "Simulation Example\n\nz &lt;- rnorm(1000, mean = 8)\nxy &lt;- sapply(z, \\(.) mvtnorm::rmvnorm(1000, .*c(1,1), \n                                      matrix(c(1,0,0,1), nrow = 2)))\nlm(xy[1,]~xy[2,]) %&gt;% summary\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = xy[1, ] ~ xy[2, ])\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.6923 -0.7576 -0.0307  0.7586  3.6455 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  3.86132    0.21625   17.86   &lt;2e-16 ***\n#&gt; xy[2, ]      0.51747    0.02651   19.52   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.178 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.2763, Adjusted R-squared:  0.2755 \n#&gt; F-statistic:   381 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\nlm(xy[1,]~xy[2,] + z) %&gt;% summary\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = xy[1, ] ~ xy[2, ] + z)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -2.7773 -0.6219  0.0408  0.6494  3.1882 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 0.088791   0.244134   0.364    0.716    \n#&gt; xy[2, ]     0.009761   0.031361   0.311    0.756    \n#&gt; z           0.979817   0.043798  22.371   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.9614 on 997 degrees of freedom\n#&gt; Multiple R-squared:  0.5181, Adjusted R-squared:  0.5172 \n#&gt; F-statistic: 536.1 on 2 and 997 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/lecture_14.html#activity",
    "href": "lectures/lecture_14.html#activity",
    "title": "Correlation, Confounding, and Interaction Terms",
    "section": "Activity",
    "text": "Activity\nCome up with examples that may have confounding variables."
  },
  {
    "objectID": "lectures/lecture_14.html#confounding-variables-3",
    "href": "lectures/lecture_14.html#confounding-variables-3",
    "title": "Correlation, Confounding, and Interaction Terms",
    "section": "Confounding Variables",
    "text": "Confounding Variables\nHow would we adjust for confounding variables?"
  },
  {
    "objectID": "lectures/lecture_14.html#interaction-terms-1",
    "href": "lectures/lecture_14.html#interaction-terms-1",
    "title": "Correlation, Confounding, and Interaction Terms",
    "section": "Interaction Terms",
    "text": "Interaction Terms\nAn interaction occurs when the independent variable effect on an outcome variable varies based on a value of a third variable."
  },
  {
    "objectID": "lectures/lecture_14.html#interaction-terms-2",
    "href": "lectures/lecture_14.html#interaction-terms-2",
    "title": "Correlation, Confounding, and Interaction Terms",
    "section": "Interaction Terms",
    "text": "Interaction Terms\n\nx &lt;- rnorm(1000, mean = 8)\nz &lt;- sample(c(0,1), 1000, replace = T)\ny &lt;- -4 + 3 * x - 5 * z + x*z + rnorm(1000, 2)\ndf &lt;- tibble(x, y, z)\nggplot(df, aes(x,y)) + geom_point()+\n  geom_smooth(method = \"lm\", se = F) +\n  theme_bw()"
  },
  {
    "objectID": "lectures/lecture_14.html#interaction-terms-3",
    "href": "lectures/lecture_14.html#interaction-terms-3",
    "title": "Correlation, Confounding, and Interaction Terms",
    "section": "Interaction Terms",
    "text": "Interaction Terms\n\nx &lt;- rnorm(1000, mean = 8)\nz &lt;- sample(c(0,1), 1000, replace = T)\ny &lt;- -4 + 3 * x -5 * z + x*z + rnorm(1000, 2)\ndf &lt;- tibble(x, y, z)\nggplot(df, aes(x,y, color = z)) + geom_point()+\n  geom_smooth(method = \"lm\", se = F) +\n  geom_smooth(aes(group=z), method = \"lm\", se =F) +\n  theme_bw()"
  },
  {
    "objectID": "lectures/lecture_14.html#interaction-terms-4",
    "href": "lectures/lecture_14.html#interaction-terms-4",
    "title": "Correlation, Confounding, and Interaction Terms",
    "section": "Interaction Terms",
    "text": "Interaction Terms\n\nx &lt;- rnorm(1000, mean = 8)\nz &lt;- sample(c(0,1), 1000, replace = T)\ny &lt;- -4 + 3 * x -5 * z + x*z + rnorm(1000, 2)\ndf &lt;- tibble(x, y, z)\ndf %$% lm(y ~ x*z)"
  },
  {
    "objectID": "lectures/lecture_12.html#learning-objectives",
    "href": "lectures/lecture_12.html#learning-objectives",
    "title": "Linear Regression",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nEstimation\nOrdinary Least Squares\nMatrix Formulation\nStandard Errors\nConduct in R"
  },
  {
    "objectID": "lectures/lecture_12.html#estimation-1",
    "href": "lectures/lecture_12.html#estimation-1",
    "title": "Linear Regression",
    "section": "Estimation",
    "text": "Estimation\n\nOrdinary Least Squares\nMaximum Likelihood Approach\nMethod of Moments"
  },
  {
    "objectID": "lectures/lecture_12.html#standard-errors",
    "href": "lectures/lecture_12.html#standard-errors",
    "title": "Linear Regression",
    "section": "Standard Errors",
    "text": "Standard Errors\n\nFind the variance of the estimate\nFind the information matrix\nUse for Inference"
  },
  {
    "objectID": "lectures/lecture_12.html#ordinary-least-squares-1",
    "href": "lectures/lecture_12.html#ordinary-least-squares-1",
    "title": "Linear Regression",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\nFor a data pair \\((X_i,Y_i)_{i=1}^n\\), the ordinary least squares estimator will find the estimates of \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) that minimize the following function:\n\\[\n\\sum^n_{i=1}\\{y_i-(\\beta_0+\\beta_1x_i)\\}^2\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#estimates",
    "href": "lectures/lecture_12.html#estimates",
    "title": "Linear Regression",
    "section": "Estimates",
    "text": "Estimates\n\\[\n\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\bar x\n\\] \\[\n\\hat\\beta_1 = \\frac{\\sum^n_{i=1}(y_i-\\bar y)(x_i-\\bar x)}{\\sum^n_{i=1}(x_i-\\bar x)^2}\n\\] \\[\n\\hat\\sigma^2 = \\frac{1}{n-2}\\sum^n_{i=1}(y_i-\\hat y_i)^2\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#matrix-version-of-model",
    "href": "lectures/lecture_12.html#matrix-version-of-model",
    "title": "Linear Regression",
    "section": "Matrix Version of Model",
    "text": "Matrix Version of Model\n\\[\nY_i = \\boldsymbol X_i^\\mathrm T \\boldsymbol \\beta + \\epsilon_i\n\\]\n\n\\(Y_i\\): Outcome Variable\n\\(\\boldsymbol X_i=(1, X_i)^\\mathrm T\\): Predictors\n\\(\\boldsymbol \\beta = (\\beta_0, \\beta_1)^\\mathrm T\\): Coefficients\n\\(\\epsilon_i\\): error term"
  },
  {
    "objectID": "lectures/lecture_12.html#data-matrix-formulation",
    "href": "lectures/lecture_12.html#data-matrix-formulation",
    "title": "Linear Regression",
    "section": "Data Matrix Formulation",
    "text": "Data Matrix Formulation\nFor \\(n\\) data points\n\\[\n\\boldsymbol Y = \\boldsymbol X^\\mathrm T\\boldsymbol \\beta + \\boldsymbol \\epsilon\n\\]\n\n\\(\\boldsymbol Y = (Y_1, \\cdots, Y_n)^\\mathrm T\\): Outcome Variable\n\\(\\boldsymbol X=(\\boldsymbol X_1, \\cdots, \\boldsymbol X_n)^\\mathrm T\\): Predictors\n\\(\\boldsymbol \\beta = (\\beta_0, \\beta_1)^\\mathrm T\\): Coefficients\n\\(\\boldsymbol \\epsilon = (\\epsilon_1, \\cdots, \\epsilon_n)^\\mathrm T\\): Error terms"
  },
  {
    "objectID": "lectures/lecture_12.html#least-squares-formula",
    "href": "lectures/lecture_12.html#least-squares-formula",
    "title": "Linear Regression",
    "section": "Least Squares Formula",
    "text": "Least Squares Formula\n\\[\n(Y - \\boldsymbol X ^\\mathrm T\\boldsymbol \\beta)^\\mathrm T(Y - \\boldsymbol X ^\\mathrm T\\boldsymbol \\beta)\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#estimates-1",
    "href": "lectures/lecture_12.html#estimates-1",
    "title": "Linear Regression",
    "section": "Estimates",
    "text": "Estimates\n\\[\n\\hat{\\boldsymbol \\beta} = (\\boldsymbol X ^\\mathrm T\\boldsymbol X)^{-1}\\boldsymbol X ^\\mathrm T\\boldsymbol Y\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#estimate-for-sigma2",
    "href": "lectures/lecture_12.html#estimate-for-sigma2",
    "title": "Linear Regression",
    "section": "Estimate for \\(\\sigma^2\\)",
    "text": "Estimate for \\(\\sigma^2\\)\n\\[\n\\hat \\sigma^2 = \\frac{1}{n-2} \\sum^n_{i=1} (Y_i-\\boldsymbol X_i^\\mathrm T\\hat{\\boldsymbol \\beta})^2\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#standard-errors-of-betas",
    "href": "lectures/lecture_12.html#standard-errors-of-betas",
    "title": "Linear Regression",
    "section": "Standard Errors of \\(\\beta\\)’s",
    "text": "Standard Errors of \\(\\beta\\)’s\n\\[\nSE(\\hat\\beta_0)=\\sqrt{\\frac{\\sum^n_{i=1}x_i^2\\hat\\sigma^2}{n\\sum^n_{i=1}(x_i-\\bar x)^2}}\n\\]\n\\[\nSE(\\hat\\beta_1)=\\sqrt\\frac{\\hat\\sigma^2}{\\sum^n_{i=1}(x_i-\\bar x)^2}\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#standard-errors-matrix-form",
    "href": "lectures/lecture_12.html#standard-errors-matrix-form",
    "title": "Linear Regression",
    "section": "Standard Errors Matrix Form",
    "text": "Standard Errors Matrix Form\n\\[\nVar(\\hat {\\boldsymbol \\beta}) = (\\boldsymbol X ^\\mathrm T\\boldsymbol X)^{-1} \\hat \\sigma^2\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#built-in-functions",
    "href": "lectures/lecture_12.html#built-in-functions",
    "title": "Linear Regression",
    "section": "Built in Functions",
    "text": "Built in Functions\nYou can use the lm to fit a linear model and extract the estimated values and standard errors"
  },
  {
    "objectID": "lectures/lecture_12.html#matrix-formulation-1",
    "href": "lectures/lecture_12.html#matrix-formulation-1",
    "title": "Linear Regression",
    "section": "Matrix Formulation",
    "text": "Matrix Formulation\nR is capable of conducting matrix operations with the following functions:\n\n%*%: matrix multiplication\nt(): transpose a matrix\nsolve(): computes the inverse matrix"
  },
  {
    "objectID": "lectures/lecture_12.html#minimization-problem",
    "href": "lectures/lecture_12.html#minimization-problem",
    "title": "Linear Regression",
    "section": "Minimization Problem",
    "text": "Minimization Problem\nMinimize the least squares using a numerical methods in R. The optim() function will minimize a function for set of parameters. We can minimize a function, least squares function, and supply initial values (0) for the parameters of interest."
  },
  {
    "objectID": "lectures/lecture_12.html#fit-a-line-using-lm-for-the-following-data",
    "href": "lectures/lecture_12.html#fit-a-line-using-lm-for-the-following-data",
    "title": "Linear Regression",
    "section": "Fit a Line using lm for the following data",
    "text": "Fit a Line using lm for the following data"
  },
  {
    "objectID": "lectures/lecture_12.html#fit-a-linear-model-using-matrix-operation",
    "href": "lectures/lecture_12.html#fit-a-linear-model-using-matrix-operation",
    "title": "Linear Regression",
    "section": "Fit a linear model using matrix operation",
    "text": "Fit a linear model using matrix operation"
  },
  {
    "objectID": "lectures/lecture_12.html#minimizing-function-using-optim",
    "href": "lectures/lecture_12.html#minimizing-function-using-optim",
    "title": "Linear Regression",
    "section": "Minimizing function using optim",
    "text": "Minimizing function using optim\nFind the value of x and y that will minimize the following function for any value a and b.\n\\[\nf(x,y) = \\frac{(x-3)^2}{a^2} + \\frac{(y+4)^2}{b^2}\n\\]"
  },
  {
    "objectID": "lectures/lecture_12.html#fit-a-linear-model-using-optim",
    "href": "lectures/lecture_12.html#fit-a-linear-model-using-optim",
    "title": "Linear Regression",
    "section": "Fit a linear model using optim",
    "text": "Fit a linear model using optim"
  },
  {
    "objectID": "lectures/lecture_10.html#learning-objectives",
    "href": "lectures/lecture_10.html#learning-objectives",
    "title": "Data Manipulation",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\ntidyr Functions\nWide to Long Example"
  },
  {
    "objectID": "lectures/lecture_10.html#tidyr-functions",
    "href": "lectures/lecture_10.html#tidyr-functions",
    "title": "Data Manipulation",
    "section": "tidyr Functions",
    "text": "tidyr Functions\nA set of functions that will tidy up a data set such that:\n\nEvery Column is a variable\nEvery Row is an observation\nEvery Cell is a single value"
  },
  {
    "objectID": "lectures/lecture_10.html#pivot_longer",
    "href": "lectures/lecture_10.html#pivot_longer",
    "title": "Data Manipulation",
    "section": "pivot_longer()",
    "text": "pivot_longer()\n\nThe pivot_longer() function grabs the variables that repeated in an observation places them in one variable"
  },
  {
    "objectID": "lectures/lecture_10.html#pivot_wider",
    "href": "lectures/lecture_10.html#pivot_wider",
    "title": "Data Manipulation",
    "section": "pivot_wider()",
    "text": "pivot_wider()\n\nThe pivot_wider() function then converts long data to wide data."
  },
  {
    "objectID": "lectures/lecture_10.html#separate",
    "href": "lectures/lecture_10.html#separate",
    "title": "Data Manipulation",
    "section": "separate()",
    "text": "separate()\n\nThe separate() function will separate a variable to multiple variables:"
  },
  {
    "objectID": "lectures/lecture_10.html#wide-to-long-data-example",
    "href": "lectures/lecture_10.html#wide-to-long-data-example",
    "title": "Data Manipulation",
    "section": "Wide to Long Data Example",
    "text": "Wide to Long Data Example\nWe work on converting data from wide to long using the functions in the tidyr package. For many statistical analysis, long data is necessary."
  },
  {
    "objectID": "lectures/lecture_10.html#load-data",
    "href": "lectures/lecture_10.html#load-data",
    "title": "Data Manipulation",
    "section": "Load Data",
    "text": "Load Data\nUse the read_csv() to read data_3_4.csv into an object called data1;\n\ndata1 &lt;- read_csv(file=\"http://www.inqs.info/files/hiss_3/data_3_4.csv\")"
  },
  {
    "objectID": "lectures/lecture_10.html#wide-data",
    "href": "lectures/lecture_10.html#wide-data",
    "title": "Data Manipulation",
    "section": "Wide Data",
    "text": "Wide Data"
  },
  {
    "objectID": "lectures/lecture_10.html#long-data",
    "href": "lectures/lecture_10.html#long-data",
    "title": "Data Manipulation",
    "section": "Long Data",
    "text": "Long Data"
  },
  {
    "objectID": "lectures/lecture_10.html#pivot_longer-1",
    "href": "lectures/lecture_10.html#pivot_longer-1",
    "title": "Data Manipulation",
    "section": "pivot_longer()",
    "text": "pivot_longer()\n\nThe pivot_longer() function grabs the variables that repeated in an observation places them in one variable:\n\n\n\ndf1 &lt;- data1 %&gt;% \n  pivot_longer(cols=`v1/mean`:`v4/median`,\n               names_to = \"measurement\",\n               values_to = \"value\")\n\n#&gt; pivot_longer: reorganized (v1/mean, v1/sd, v1/median, v2/mean, v2/sd, …) into (measurement, value) [was 1000x13, now 12000x3]"
  },
  {
    "objectID": "lectures/lecture_10.html#separate-1",
    "href": "lectures/lecture_10.html#separate-1",
    "title": "Data Manipulation",
    "section": "separate()",
    "text": "separate()\n\nThe separate() function will separate a variable to multiple variables:\n\n\n\ndf2 &lt;- data1 %&gt;% \n  pivot_longer(cols=`v1/mean`:`v4/median`,\n               names_to = \"measurement\",\n               values_to = \"value\") %&gt;% \n  separate(col=measurement,into=c(\"time\",\"stat\"),sep=\"/\")\n\n#&gt; pivot_longer: reorganized (v1/mean, v1/sd, v1/median, v2/mean, v2/sd, …) into (measurement, value) [was 1000x13, now 12000x3]"
  },
  {
    "objectID": "lectures/lecture_10.html#pivot_wider-1",
    "href": "lectures/lecture_10.html#pivot_wider-1",
    "title": "Data Manipulation",
    "section": "pivot_wider()",
    "text": "pivot_wider()\n\nThe pivot_wider() function then converts long data to wide data.\n\n\n\ndf3 &lt;- data1 %&gt;% \n  pivot_longer(`v1/mean`:`v4/median`,\n               names_to = \"measurement\", \n               values_to = \"value\") %&gt;% \n  separate(measurement,c(\"time\",\"stat\"),sep=\"/\") %&gt;% \n  pivot_wider(names_from = stat,\n              values_from = value)\n\n#&gt; pivot_longer: reorganized (v1/mean, v1/sd, v1/median, v2/mean, v2/sd, …) into (measurement, value) [was 1000x13, now 12000x3]\n\n\n#&gt; pivot_wider: reorganized (stat, value) into (mean, sd, median) [was 12000x4, now 4000x5]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Advanced Data Analysis!",
    "section": "",
    "text": "Brief Introduction\n\n\n\n\n\nWelcome to the course! This is the home page of the course where I will provide a recap on what was covered in the week. Here I will post any documents or videos for your reference. If you have any questions, please email me at isaac.qs@csuci.edu.\n\n\n\n\n\n\n\n\n\nDiscord Server\n\n\n\n\n\nWill post if class creates one.\n\n\n\n\n\n\n\n\n\nIntroduction to Statistics Book\n\n\n\n\n\nhttps://openintro-ims.netlify.app/\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 1\n\n\n\n\n\nThis week is designed to be an introduction week. We will briefly discuss topics related to statistics and inference. Then we will look at installing R and RStudio as well as the basics of using R.\n\n\n\n\n\nJan 22, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "hw.html",
    "href": "hw.html",
    "title": "Homework",
    "section": "",
    "text": "Below are the different homework assignments for the course. Make sure to upload your assignment as a single file on Canvas.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "exc/exc3.html",
    "href": "exc/exc3.html",
    "title": "Extra Credit 3",
    "section": "",
    "text": "Provide a summary of one book below, an write a brief analysis in supporting or opposing the book,and connect key elements of the book to your every day life. The majority of these books are available through the Broome Library.\nBooks:\n\nHappiness: A Guide to Developing Life’s\n\nMatthieu Ricard\n\nGrit: The Power of Passion and Perserverance\n\nAngela Duckworth\n\nMindset: The New Psychology of Success\n\nCarol Dweck\n\nMake it Stick: The Science of Successful Learning\n\nPeter Brown\n\nLimitless Mind\n\nJo Boaler\n\n\nReport guidelines\n\n5-6 Pages\nMust include a title page (does not count toward page count)\nDouble Spaced\n12 point font\nProofread your work\nSubmit a pdf of your work to Canvas\nDue 5/13/2024\n\nWorth 3 final grade percentage points."
  },
  {
    "objectID": "exc/exc1.html",
    "href": "exc/exc1.html",
    "title": "Extra Credit 1",
    "section": "",
    "text": "Write a 2-page report practices related to self care. As you move on through college and your future careers, it is important to practice self care. For this extra credit assignments, write about what are some practices you will do to for your well being.\nWorth 1 percent points.\nReport guidelines\n\n2 Pages\nDouble Spaced\n12 point font\nDue 2/9/2024"
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books",
    "section": "",
    "text": "A list of recommended books to learn more about Statistics, the majority are freely available from the Broome Library:"
  },
  {
    "objectID": "books.html#basics",
    "href": "books.html#basics",
    "title": "Books",
    "section": "Basics",
    "text": "Basics\n\nIntroduction to Statistics and Data Analysis\n\nHeumann and Shalabh\n\nStatistical Foundations, Reasoning and Inference\n\nKauermann, Küchenhoff, and Heumann"
  },
  {
    "objectID": "books.html#regression",
    "href": "books.html#regression",
    "title": "Books",
    "section": "Regression",
    "text": "Regression\n\nGeneralized Linear Models With Examples in R\n\nDunn and Smyth\nGraduate\n\nLinear and Generalized Linear Mixed Models and Their Applications (2nd Edition)\n\nJiang and Nguyen\nGraudate\n\nRegression Modeling Strategies\n\nHarrell\nUndergraduate\n\nVector Generalized Linear and Additive Models\n\nYee\nGraduate"
  },
  {
    "objectID": "books.html#nonparametric",
    "href": "books.html#nonparametric",
    "title": "Books",
    "section": "Nonparametric",
    "text": "Nonparametric\n\nSemiparametric Regression with R\n\nHarezlak, Ruppert, and Wand\nGraduate"
  },
  {
    "objectID": "books.html#computational",
    "href": "books.html#computational",
    "title": "Books",
    "section": "Computational",
    "text": "Computational\n\nBootstrap Methods with applications in R\n\nDikta and Scheer\nGraduate\n\nModern Optimization with R (2nd Edition)\n\nCortez\nGraduate\n\nComputational Statistics\n\nGentle\n\nMonte Carlo and Quasi-Monte Carlo Sampling\n\nLemieux\n\nStatistics With Julia\n\nNazarathy andKlok\n\nIntroducing Monte Carlo Methods in R\n\nRobert and Casella\n\nPermutation Statistical Methods with R\n\nBerry, Kvamme, Johnston, and Mielke\n\nMonte Carlo Strategies in Scientific Computing\n\nLiu"
  },
  {
    "objectID": "books.html#bayesian",
    "href": "books.html#bayesian",
    "title": "Books",
    "section": "Bayesian",
    "text": "Bayesian\n\nIntroduction to Bayesian Inference, Methods and Computation\n\nHeard\n\nApplied Bayesian Statistics\n\nCowles\n\nBayesian Statistical Modeling with Stan, R, and Python\n\nMatsuura\n\nBayesian Essentials in R\n\nMarin and Robert"
  },
  {
    "objectID": "books.html#theoretical",
    "href": "books.html#theoretical",
    "title": "Books",
    "section": "Theoretical",
    "text": "Theoretical\n\nEssentials of Stochastic Processes (3rd Edition)\n\nDurrett\nGraduate\n\nA Concise Introduction to Measure Theory\n\nShirali\nGraduate\n\nLarge Sample Techniques for Statistics (2nd Edition)\n\nJiang\nGraduate\n\nA Course in Mathematical Statistics and Large Sample Theory\n\nBhattacharya, Lin, and Patrangenaru\nGraduate\n\nMixture and Hidden Markov Models with R\n\nVisser and Speekenbrink\nUndergraduate\n\nModern Mathematical Statistics (3rd Edition)\n\nDevore, Berk, and Carlton\nUndergraduate\n\nProbability Theory (3rd Edition)\n\nKlenke\nGraduate\n\nTesting Statistical Hypotheses (4th Edition)\n\nLehmann and Romano\nGraduate\n\nTheory of Point Estimation\n\nLehmann and Casella\nGraduate\nMay not be available"
  },
  {
    "objectID": "books.html#longitudinal-data-analysis",
    "href": "books.html#longitudinal-data-analysis",
    "title": "Books",
    "section": "Longitudinal Data Analysis",
    "text": "Longitudinal Data Analysis\n\nLongitudinal Categorical Data Analysis\n\nSutradhar"
  },
  {
    "objectID": "books.html#survival-analysis",
    "href": "books.html#survival-analysis",
    "title": "Books",
    "section": "Survival Analysis",
    "text": "Survival Analysis\n\nStatistical Modelling of Survival Data with Random Effects\n\nHa, Jeong, and Lee\n\nSurvival Analysis (3rd Edition)\n\nKleinbaum and Klein\n\nApplied Survival Analysis in R\n\nMoore\n\nBayesian Survival Analysis\n\nIbrahim, Chen, and Sinha\n\nSurvival Analysis Techniques for Censored and Truncated Data (2nd Edition)\n\nKlein and Moeschberger"
  },
  {
    "objectID": "books.html#machine-learning",
    "href": "books.html#machine-learning",
    "title": "Books",
    "section": "Machine Learning",
    "text": "Machine Learning\n\nFundamental of High-Dimensional Statistics\n\nLederer\n\nAn Introduction to Statistical Learning (2nd Edition)\n\nJames, Witten, Hastie and Tibshirani\n\nStatistical Learning from a Regression Perspective (2nd Edition)\n\nBerk\n\nElements of Statistical Learning\n\nHastie, Friedman, and Tibshirani\n\nStatistics for High Dimensional Data\n\nBühlmann and van der Geer\n\nProbability and Statistics for Machine Learning\n\nDas Gupta"
  },
  {
    "objectID": "books.html#time-series",
    "href": "books.html#time-series",
    "title": "Books",
    "section": "Time-Series",
    "text": "Time-Series\n\nIntroduction to Time Series and Forcasting (3rd Edition)\n\nBrockwell and Davis\n\nTime Series Analysis and Its Applications\n\nShumway and Stoffer\n\nTime Series Analysis for the State-Space Model with R/Stan\n\nHagiwara"
  },
  {
    "objectID": "books.html#study-desing-and-causal-inference",
    "href": "books.html#study-desing-and-causal-inference",
    "title": "Books",
    "section": "Study Desing and Causal Inference",
    "text": "Study Desing and Causal Inference\n\nCausal Inference What IF\n\nHernán and Robins\n\nDesign of Observational Studies\n\nRosenbaum\n\n\nBolded Titles, I have read thoroughly."
  },
  {
    "objectID": "ec.html",
    "href": "ec.html",
    "title": "Extra Credit",
    "section": "",
    "text": "Extra Credit is designed to expand on different topics that related to Probability and Statistics, but are not necessarily required for the course. Additionally, these opportunities provide students relief when unexpected situations occur during the semester. While it is not required, I encourage everyone to attempt each opportunity.\nBelow is more information on each assignment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtra Credit 1\n\n\n\n\n\nInstructions for extra credit one.\n\n\n\n\n\nJan 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nExtra Credit 2\n\n\n\n\n\nInstructions for extra credit two.\n\n\n\n\n\nJan 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nExtra Credit 3\n\n\n\n\n\nInstructions for extra credit four.\n\n\n\n\n\nJan 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nExtra Credit 4\n\n\n\n\n\nInstructions for extra credit five.\n\n\n\n\n\nJan 16, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "exc/exc2.html",
    "href": "exc/exc2.html",
    "title": "Extra Credit 2",
    "section": "",
    "text": "Write a report on one of the topics below. Provide a brief history and their purpose. Lastly, provide a toy example demonstrating the use of the topic in R.\n\nMonte Carlo Methods\nMonte Carlo Methods are a class of computational algorithms that rely on random sampling to obtain numerical results. These methods use statistical sampling techniques to approximate complex mathematical problems, particularly those with deterministic or probabilistic aspects. The name “Monte Carlo” is derived from the Monte Carlo Casino in Monaco, known for its games of chance and randomness.\nIn Monte Carlo Methods, random samples are generated to simulate the behavior of a system or process, and the results are analyzed to estimate desired quantities or solve problems. These methods find applications in various fields, such as physics, finance, engineering, and statistics. Monte Carlo simulations are particularly useful for solving problems with a large number of variables or complex interactions, where analytical solutions may be challenging or impossible to obtain.\n\nResources\n\nhttps://towardsdatascience.com/an-overview-of-monte-carlo-methods-675384eb1694\nhttps://towardsdatascience.com/monte-carlo-simulation-a-practical-guide-85da45597f0e\n\n\n\n\nSurvival Analysis\nSurvival analysis is a statistical method used to analyze the time until an event of interest occurs. This type of analysis is commonly employed in medical research, epidemiology, and other fields to study the duration until a specific event, often referred to as a “failure” or “survival” event. The event could be anything from the onset of a disease, death, relapse, or any other occurrence of interest.\nSurvival analysis is conducted using various statistical models, with the Cox proportional hazards model being one of the most widely used. These analyses help researchers understand factors influencing the time to an event, identify risk factors, and estimate survival probabilities over time.\nMake sure to provide a brief history of survival analysis and prominent methods such as Kaplan-Meier curves and Cox proportional hazard models.\n\nResources\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC2394262/\nhttps://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_survival/BS704_Survival_print.html\n\n\n\n\nBayesian Analysis\nBayesian analysis is a statistical approach that involves updating probabilities for hypotheses based on new evidence or data. It is rooted in Bayes’ theorem, which describes how beliefs about the probability of a hypothesis should change in light of new information. In Bayesian analysis, the prior probability (initial belief) is combined with the likelihood of observing the data given the hypothesis and results in the posterior probability (updated belief).\nBayesian methods are particularly valuable in situations with limited data or when incorporating prior knowledge is crucial. These methods provide a flexible framework for modeling uncertainty and updating beliefs as more information becomes available. Bayesian analysis is applied across various fields, including statistics, machine learning, physics, biology, and finance. Markov Chain Monte Carlo (MCMC) methods are often used to simulate samples from the posterior distribution in complex Bayesian models.\n\nResources\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6406060/\nhttps://www.statlect.com/fundamentals-of-statistics/Bayesian-inference\n\n\n\n\nCausal Inference\nCausal inference is a field within statistics and epidemiology that focuses on understanding and estimating the causal relationships between variables or events. The goal is to determine whether a particular factor or intervention has a causal impact on an outcome. Causal inference involves identifying and controlling for confounding factors, which are variables that may influence both the cause and the effect, leading to potential bias in estimating causal relationships.\nCausal inference is essential for making informed decisions in fields such as medicine, public health, economics, and social sciences. It helps researchers draw valid conclusions about the effectiveness of interventions, policies, or treatments by accounting for potential sources of bias and confounding. Advances in causal inference methodologies contribute to a more rigorous understanding of cause-and-effect relationships in complex systems.\n\nResources\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC2836213/\nhttps://web.stanford.edu/~swager/stats361.pdf\n\n\n\n\nReport Guideline\n\n3-4 Pages\nMust include a title page\nDouble Spaced\n12 point font\nProofread your work\nSubmit a pdf of your work to Canvas\nDue 5/13/2024\n\nWorth 3 percent points."
  },
  {
    "objectID": "exc/exc4.html",
    "href": "exc/exc4.html",
    "title": "Extra Credit 4",
    "section": "",
    "text": "Provide a summary of one book below, an write a brief analysis in supporting or opposing the book, and connect key elements of the book to your every day life. Some of these books are available through the Broome Library.\nBooks:\n\nThe Book of Why\n\nJudea Pearl\n\nAlgorithms of Oppression\n\nSafiya Umoja Noble\n\nData Feminism\n\nCatherine D’Ignazio and Lauren Klein\n\nWeapons of Math Destruction\n\nCathy O’Niel\n\nInvisible women : data bias in a world designed for men\n\nCaroline Criado Perez\n\nFactfulness: Ten Reasons We’re Wrong About the World… and Why Things are Better Than You Think\n\nHans Rosling\n\nArtificial Unintelligence: How Computers Misunderstand the World\n\nMerideth Broussard\n\nTechnically Wrong: Sexist Apps, Biased Algorithms, and Other Threats of Toxic Tech\n\nSara Wachter-Boettcher\n\n\nReport guidelines\n\n5-6 Pages\nMust include a title page (does not count toward page count)\nDouble Spaced\n12 point font\nProofread your work\nSubmit a pdf of your work to Canvas\nDue 5/13/2024\n\nWorth 3 final grade percentage points."
  },
  {
    "objectID": "lectures/lecture_1.html#introductions",
    "href": "lectures/lecture_1.html#introductions",
    "title": "Welcome to Math 408",
    "section": "Introductions",
    "text": "Introductions\n\nSan Bernardino, CA\nCSU Monterey Bay\n\nBS Biology\n\nSan Diego State University\n\nMaster’s in Public Health\n\nUC Riverside\n\nPhD in Applied Statistics"
  },
  {
    "objectID": "lectures/lecture_1.html#introductions-1",
    "href": "lectures/lecture_1.html#introductions-1",
    "title": "Welcome to Math 408",
    "section": "Introductions",
    "text": "Introductions\n\nName\nYear\nMajor\nFun Fact\nCareer Goal"
  },
  {
    "objectID": "lectures/lecture_1.html#goals-for-the-course",
    "href": "lectures/lecture_1.html#goals-for-the-course",
    "title": "Welcome to Math 408",
    "section": "Goals for the Course",
    "text": "Goals for the Course\n\nGain R Programming Skills\nApply different modeling techniques\n\nLinear Regression\nModel Building\n\nApply Different Machine Learning Techniques"
  },
  {
    "objectID": "lectures/lecture_1.html#oh-traditional",
    "href": "lectures/lecture_1.html#oh-traditional",
    "title": "Welcome to Math 408",
    "section": "OH: Traditional",
    "text": "OH: Traditional\n\nBTE 2840 MW 3-4 PM\nBTE 2840 T 10 AM -12 PM"
  },
  {
    "objectID": "lectures/lecture_1.html#syllabus-1",
    "href": "lectures/lecture_1.html#syllabus-1",
    "title": "Welcome to Math 408",
    "section": "Syllabus",
    "text": "Syllabus\nSyllabus"
  },
  {
    "objectID": "lectures/lecture_1.html#books",
    "href": "lectures/lecture_1.html#books",
    "title": "Welcome to Math 408",
    "section": "Books",
    "text": "Books\n\nIntroduction to Statistical Learning (SL)\n\nJames, Witten, Hastie, Tibshirani\nAvailable to Download from the Broome Library\n\nStatistical Computing (SC)\n\nIsaac Quintanilla Salinas\nhttps://www.inqs.info/stat_comp/\nhttps://hypothes.is/groups/xMmDdj2A/m408"
  },
  {
    "objectID": "lectures/lecture_1.html#r-programming",
    "href": "lectures/lecture_1.html#r-programming",
    "title": "Welcome to Math 408",
    "section": "R Programming",
    "text": "R Programming\nR is a statistical programming package that allows you to conduct different types of analysis.\nR"
  },
  {
    "objectID": "lectures/lecture_1.html#rstudio",
    "href": "lectures/lecture_1.html#rstudio",
    "title": "Welcome to Math 408",
    "section": "RStudio",
    "text": "RStudio\nA piece of software that organizes how you conduct statistical analysis in R.\nRStudio"
  },
  {
    "objectID": "lectures/lecture_1.html#posit-cloud",
    "href": "lectures/lecture_1.html#posit-cloud",
    "title": "Welcome to Math 408",
    "section": "Posit Cloud",
    "text": "Posit Cloud\nA web version of RStudio.\nPosit Cloud"
  },
  {
    "objectID": "lectures/lecture_1.html#r-packages",
    "href": "lectures/lecture_1.html#r-packages",
    "title": "Welcome to Math 408",
    "section": "R Packages",
    "text": "R Packages\n\nTidyverse\ncsucistats\n\n\ninstall.packages('csucistats', \n                 repos = c('https://inqs909.r-universe.dev', \n                           'https://cloud.r-project.org'))"
  },
  {
    "objectID": "lectures/lecture_1.html#r-as-a-calculator",
    "href": "lectures/lecture_1.html#r-as-a-calculator",
    "title": "Welcome to Math 408",
    "section": "R as a calculator",
    "text": "R as a calculator\nR can evaluate different expressions in the console tab."
  },
  {
    "objectID": "lectures/lecture_1.html#types-of-data",
    "href": "lectures/lecture_1.html#types-of-data",
    "title": "Welcome to Math 408",
    "section": "Types of Data",
    "text": "Types of Data\n\nNumeric\nCharacter\nLogical\nMissing"
  },
  {
    "objectID": "lectures/lecture_1.html#r-functions",
    "href": "lectures/lecture_1.html#r-functions",
    "title": "Welcome to Math 408",
    "section": "R Functions",
    "text": "R Functions\nR functions performs tasks to specific data values."
  },
  {
    "objectID": "lectures/lecture_11.html#learning-objectives",
    "href": "lectures/lecture_11.html#learning-objectives",
    "title": "Linear Regression",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nAssociations\nLinear Regression\nConducting it in R"
  },
  {
    "objectID": "lectures/lecture_11.html#types-of-associations",
    "href": "lectures/lecture_11.html#types-of-associations",
    "title": "Linear Regression",
    "section": "Types of Associations",
    "text": "Types of Associations\n\nCategorical vs Continuous\nCategorical vs Categorical\nContinuous vs Continuous"
  },
  {
    "objectID": "lectures/lecture_11.html#categorical-vs-continuous",
    "href": "lectures/lecture_11.html#categorical-vs-continuous",
    "title": "Linear Regression",
    "section": "Categorical vs Continuous",
    "text": "Categorical vs Continuous\nDescribing the relationship between categorical and continuous variables can be done by stratifying by the categorical variable and finding the mean or median, or conducting a statistical test:\n\nt-tests\nANOVA\n\nMeasuring the association between species and flipper length from palmerpenguins:\n\n# Stratification\npenguins %&gt;% drop_na %&gt;%\n  group_by(species) %&gt;% \n  summarise(mean = mean(flipper_length_mm),\n            median = median(flipper_length_mm))\n\n#&gt; # A tibble: 3 × 3\n#&gt;   species    mean median\n#&gt;   &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 Adelie     190.    190\n#&gt; 2 Chinstrap  196.    196\n#&gt; 3 Gentoo     217.    216\n\n# ANOVA\nspecies_aov &lt;- aov(flipper_length_mm ~ species, penguins)\ntidy(species_aov)\n\n#&gt; # A tibble: 2 × 6\n#&gt;   term         df  sumsq  meansq statistic    p.value\n#&gt;   &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 species       2 52473. 26237.       595.  1.35e-111\n#&gt; 2 Residuals   339 14953.    44.1       NA  NA"
  },
  {
    "objectID": "lectures/lecture_11.html#categorical-vs-categorical",
    "href": "lectures/lecture_11.html#categorical-vs-categorical",
    "title": "Linear Regression",
    "section": "Categorical vs Categorical",
    "text": "Categorical vs Categorical\nComparing 2 categorical variables can be done by computing proportions or conducting a chi-square test. Below is an example comparing\n\n# Count and Proportions\npenguins %&gt;% group_by(species, island) %&gt;% \n  summarise(count = n(), proportions = n() / nrow(penguins))\n\n#&gt; # A tibble: 5 × 4\n#&gt; # Groups:   species [3]\n#&gt;   species   island    count proportions\n#&gt;   &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt;       &lt;dbl&gt;\n#&gt; 1 Adelie    Biscoe       44       0.128\n#&gt; 2 Adelie    Dream        56       0.163\n#&gt; 3 Adelie    Torgersen    52       0.151\n#&gt; 4 Chinstrap Dream        68       0.198\n#&gt; 5 Gentoo    Biscoe      124       0.360\n\n# Chi-Square Test\npenguins %$% chisq.test(species, island) \n\n#&gt; \n#&gt;  Pearson's Chi-squared test\n#&gt; \n#&gt; data:  species and island\n#&gt; X-squared = 299.55, df = 4, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/lecture_11.html#continuous-vs-continuous",
    "href": "lectures/lecture_11.html#continuous-vs-continuous",
    "title": "Linear Regression",
    "section": "Continuous vs Continuous",
    "text": "Continuous vs Continuous\nTo understand the relationship between two continuous random variables, we can use the following:\n\nCorrelation\nLinear Regression"
  },
  {
    "objectID": "lectures/lecture_11.html#correlation",
    "href": "lectures/lecture_11.html#correlation",
    "title": "Linear Regression",
    "section": "Correlation",
    "text": "Correlation\nCorrelations measures the association between 2 continuous random variables.\n\nPearson’s Correlation\nSpearman’s Correlation\nKendall’s Tau\n\n\n# Pearson's Correlation\npenguins %&gt;% drop_na %$% cor.test(body_mass_g, flipper_length_mm)\n\n#&gt; \n#&gt;  Pearson's product-moment correlation\n#&gt; \n#&gt; data:  body_mass_g and flipper_length_mm\n#&gt; t = 32.562, df = 331, p-value &lt; 2.2e-16\n#&gt; alternative hypothesis: true correlation is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  0.8447622 0.8963550\n#&gt; sample estimates:\n#&gt;       cor \n#&gt; 0.8729789\n\n# Spearman's Correlation \npenguins %&gt;% drop_na %$% cor.test(body_mass_g, flipper_length_mm, method = \"spearman\")\n\n#&gt; \n#&gt;  Spearman's rank correlation rho\n#&gt; \n#&gt; data:  body_mass_g and flipper_length_mm\n#&gt; S = 982284, p-value &lt; 2.2e-16\n#&gt; alternative hypothesis: true rho is not equal to 0\n#&gt; sample estimates:\n#&gt;       rho \n#&gt; 0.8403902\n\n# Kendall's Tau\npenguins %&gt;% drop_na %$% cor.test(body_mass_g, flipper_length_mm, method = \"kendall\")\n\n#&gt; \n#&gt;  Kendall's rank correlation tau\n#&gt; \n#&gt; data:  body_mass_g and flipper_length_mm\n#&gt; z = 17.683, p-value &lt; 2.2e-16\n#&gt; alternative hypothesis: true tau is not equal to 0\n#&gt; sample estimates:\n#&gt;       tau \n#&gt; 0.6614597"
  },
  {
    "objectID": "lectures/lecture_11.html#scatter-plot",
    "href": "lectures/lecture_11.html#scatter-plot",
    "title": "Linear Regression",
    "section": "Scatter Plot",
    "text": "Scatter Plot"
  },
  {
    "objectID": "lectures/lecture_11.html#linear-regression-1",
    "href": "lectures/lecture_11.html#linear-regression-1",
    "title": "Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nLinear regression is used to model the association between a set of predictor variables (x’s) and an outcome variable (y). Linear regression will fit a line that best describes the data points."
  },
  {
    "objectID": "lectures/lecture_11.html#scatter-plot-1",
    "href": "lectures/lecture_11.html#scatter-plot-1",
    "title": "Linear Regression",
    "section": "Scatter Plot",
    "text": "Scatter Plot"
  },
  {
    "objectID": "lectures/lecture_11.html#simple-linear-regression",
    "href": "lectures/lecture_11.html#simple-linear-regression",
    "title": "Linear Regression",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nSimple linear regression will model the association between one predictor variable and an outcome:\n\\[\nY = \\beta_0 + \\beta_1 X + \\epsilon\n\\]\n\n\\(\\beta_0\\): Intercept term\n\\(\\beta_1\\): Slope term\n\\(\\epsilon\\sim N(0,\\sigma^2)\\)"
  },
  {
    "objectID": "lectures/lecture_11.html#estimated-model",
    "href": "lectures/lecture_11.html#estimated-model",
    "title": "Linear Regression",
    "section": "Estimated Model",
    "text": "Estimated Model\n\\[\n\\hat Y = \\hat \\beta_0 + \\hat \\beta_1 X\n\\]"
  },
  {
    "objectID": "lectures/lecture_11.html#r-formulas",
    "href": "lectures/lecture_11.html#r-formulas",
    "title": "Linear Regression",
    "section": "R Formulas",
    "text": "R Formulas\nIn R, we create a formula using the tilde\nOutcome Variable tilde Independent Variables\n\nooutcome_variable_name ~ independent_variable_name"
  },
  {
    "objectID": "lectures/lecture_11.html#r-example",
    "href": "lectures/lecture_11.html#r-example",
    "title": "Linear Regression",
    "section": "R Example",
    "text": "R Example\nApplying a linear regression model to body_mass_g (Predictor) and flipper_length_mm (Outcome) from penguins:\n\nR OutputScatter Plot\n\n\n\n# summary(lm(flipper_length_mm ~ body_mass_g, data = penguins))\n\n# OR\n\npenguins %$% lm(flipper_length_mm ~ body_mass_g) %&gt;% summary\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = flipper_length_mm ~ body_mass_g)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -23.7626  -4.9138   0.9891   5.1166  16.6392 \n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 1.367e+02  1.997e+00   68.47   &lt;2e-16 ***\n#&gt; body_mass_g 1.528e-02  4.668e-04   32.72   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 6.913 on 340 degrees of freedom\n#&gt;   (2 observations deleted due to missingness)\n#&gt; Multiple R-squared:  0.759,  Adjusted R-squared:  0.7583 \n#&gt; F-statistic:  1071 on 1 and 340 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nggplot(penguins, aes(y = flipper_length_mm, x = body_mass_g)) + geom_point() + \n  geom_smooth(method = \"lm\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nInterpretation\n\\[\n\\hat y = 136.73 + 0.015 x\n\\]"
  },
  {
    "objectID": "lectures/lecture_11.html#simulation-example",
    "href": "lectures/lecture_11.html#simulation-example",
    "title": "Linear Regression",
    "section": "Simulation Example",
    "text": "Simulation Example"
  },
  {
    "objectID": "lectures/lecture_11.html#generate-x-values",
    "href": "lectures/lecture_11.html#generate-x-values",
    "title": "Linear Regression",
    "section": "Generate X Values",
    "text": "Generate X Values\nSimulate 250 independent values (x) from \\(N(3, 1)\\) and plot a histogram\n\nx &lt;- rnorm(205, 1)"
  },
  {
    "objectID": "lectures/lecture_11.html#generate-y-values",
    "href": "lectures/lecture_11.html#generate-y-values",
    "title": "Linear Regression",
    "section": "Generate Y Values",
    "text": "Generate Y Values\nCreate a new variable with the following formula:\n\\[\nY_i = 3 X_i + 20\n\\]\nCreate a Scatter Plot"
  },
  {
    "objectID": "lectures/lecture_11.html#generate-error-term",
    "href": "lectures/lecture_11.html#generate-error-term",
    "title": "Linear Regression",
    "section": "Generate Error Term",
    "text": "Generate Error Term\nGenerate \\(\\epsilon_i\\sim N(0, 2)\\)\nCreate a histogram"
  },
  {
    "objectID": "lectures/lecture_11.html#add-error-term",
    "href": "lectures/lecture_11.html#add-error-term",
    "title": "Linear Regression",
    "section": "Add error term",
    "text": "Add error term\nCreate final form of \\(Y_i\\):\n\\[\nY_i = 3 X_i + 20 + \\epsilon_i\n\\]\nCreate Scatter Plot"
  },
  {
    "objectID": "lectures/lecture_11.html#fit-model",
    "href": "lectures/lecture_11.html#fit-model",
    "title": "Linear Regression",
    "section": "Fit Model",
    "text": "Fit Model"
  },
  {
    "objectID": "lectures/lecture_11.html#simulation-study",
    "href": "lectures/lecture_11.html#simulation-study",
    "title": "Linear Regression",
    "section": "Simulation Study",
    "text": "Simulation Study\nTo confirm that lm works, repeat the process 100 times and obtain the average \\(\\beta\\) estimates:\n\nGenerate 250 X values\nCreate Y Value\nAdd error term\nFit Model\nStore Values"
  },
  {
    "objectID": "lectures/lecture_13.html#learning-objectives",
    "href": "lectures/lecture_13.html#learning-objectives",
    "title": "Multivariable Linear Regression",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nMultivariable Linear Regression\nSimulation Study 1\nSimulation Study 2\nMatrix Formulation"
  },
  {
    "objectID": "lectures/lecture_13.html#mlr",
    "href": "lectures/lecture_13.html#mlr",
    "title": "Multivariable Linear Regression",
    "section": "MLR",
    "text": "MLR\nMultivariable linear regression models are used when more than one explanatory variable is used to explain the outcome of interest."
  },
  {
    "objectID": "lectures/lecture_13.html#continuous-variable",
    "href": "lectures/lecture_13.html#continuous-variable",
    "title": "Multivariable Linear Regression",
    "section": "Continuous Variable",
    "text": "Continuous Variable\nTo fit an additional continuous random variable to the model, we will only need to add it to the model:\n\\[\nY = \\beta_0 +\\beta_1 X_1 + \\beta_2 X_2\n\\]"
  },
  {
    "objectID": "lectures/lecture_13.html#example",
    "href": "lectures/lecture_13.html#example",
    "title": "Multivariable Linear Regression",
    "section": "Example",
    "text": "Example\nLooking at the penguins from palmerpenguins, fit a model between body_mass_g as an outcome variable and flipper_length_mm and bill_length_mm as predictor variables.\n\nlibrary(palmerpenguins)\npenguins %$% lm(body_mass_g ~ flipper_length_mm + bill_length_mm)"
  },
  {
    "objectID": "lectures/lecture_13.html#interpretation",
    "href": "lectures/lecture_13.html#interpretation",
    "title": "Multivariable Linear Regression",
    "section": "Interpretation",
    "text": "Interpretation\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ flipper_length_mm + bill_length_mm)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)  flipper_length_mm     bill_length_mm  \n#&gt;         -5736.897             48.145              6.047"
  },
  {
    "objectID": "lectures/lecture_13.html#categorical-variable",
    "href": "lectures/lecture_13.html#categorical-variable",
    "title": "Multivariable Linear Regression",
    "section": "Categorical Variable",
    "text": "Categorical Variable\nA categorical variable can be included in a model, but a reference category must be specified."
  },
  {
    "objectID": "lectures/lecture_13.html#fitting-a-model-with-categorical-variables",
    "href": "lectures/lecture_13.html#fitting-a-model-with-categorical-variables",
    "title": "Multivariable Linear Regression",
    "section": "Fitting a model with categorical variables",
    "text": "Fitting a model with categorical variables\nTo fit a model with categorical variables, we must utilize dummy (binary) variables that indicate which category is being referenced. We use \\(C-1\\) dummy variables where \\(C\\) indicates the number of categories. When coded correctly, each category will be represented by a combination of dummy variables."
  },
  {
    "objectID": "lectures/lecture_13.html#example-1",
    "href": "lectures/lecture_13.html#example-1",
    "title": "Multivariable Linear Regression",
    "section": "Example",
    "text": "Example\nIf we have 4 categories, we will need 3 dummy variables:\n\n\n\n\nCat 1\nCat 2\nCat 3\nCat 4\n\n\n\n\nDummy 1\n1\n0\n0\n0\n\n\nDummy 2\n0\n1\n0\n0\n\n\nDummy 3\n0\n0\n1\n0\n\n\n\nWhich one is the reference category?"
  },
  {
    "objectID": "lectures/lecture_13.html#fitting-a-model-with-categorical-variables-1",
    "href": "lectures/lecture_13.html#fitting-a-model-with-categorical-variables-1",
    "title": "Multivariable Linear Regression",
    "section": "Fitting a model with categorical variables",
    "text": "Fitting a model with categorical variables\nLooking at the penguins from palmerpenguins, fit a model between body_mass_g as an outcome variable and flipper_length_mm and islands as predictor variables.\n\nlibrary(palmerpenguins)\npenguins %$% lm(body_mass_g ~ flipper_length_mm + island)"
  },
  {
    "objectID": "lectures/lecture_13.html#interpreting-categorical-variable-results",
    "href": "lectures/lecture_13.html#interpreting-categorical-variable-results",
    "title": "Multivariable Linear Regression",
    "section": "Interpreting Categorical Variable Results",
    "text": "Interpreting Categorical Variable Results\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ flipper_length_mm + island)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)  flipper_length_mm        islandDream    islandTorgersen  \n#&gt;          -4624.98              44.54            -262.18            -185.13\n\n\n#&gt; [1] \"Biscoe\"    \"Dream\"     \"Torgersen\""
  },
  {
    "objectID": "lectures/lecture_13.html#relevel-a-factor-variable-in-r",
    "href": "lectures/lecture_13.html#relevel-a-factor-variable-in-r",
    "title": "Multivariable Linear Regression",
    "section": "Relevel a factor variable in R",
    "text": "Relevel a factor variable in R"
  },
  {
    "objectID": "lectures/lecture_13.html#simulation-study-1-1",
    "href": "lectures/lecture_13.html#simulation-study-1-1",
    "title": "Multivariable Linear Regression",
    "section": "Simulation Study 1",
    "text": "Simulation Study 1\nSimulate 1000 random variables from the following model:\n\\[\nY = 3 + 2X_1 + 4X_2 + \\epsilon\n\\]\n\n\\(X_1\\sim N(2,1)\\)\n\\(X_2\\sim N(-4,1)\\)\n\\(\\epsilon\\sim N(0, 2)\\)"
  },
  {
    "objectID": "lectures/lecture_13.html#fit-model",
    "href": "lectures/lecture_13.html#fit-model",
    "title": "Multivariable Linear Regression",
    "section": "Fit Model",
    "text": "Fit Model\nFit a model between \\(Y\\) and \\(X_1\\).\nRepeat the process 1000 times. and answer the following questions:\n\nOn average does \\(\\beta_1\\) get estimated correctly? Why?\nWhat is the average model variance?"
  },
  {
    "objectID": "lectures/lecture_13.html#mlr-model",
    "href": "lectures/lecture_13.html#mlr-model",
    "title": "Multivariable Linear Regression",
    "section": "MLR Model",
    "text": "MLR Model\nInstead of fitting a simple linear regression model. Fit a model that will include predictor \\(X_2\\). This can be done by adding \\(X_2\\) in R:\n\nlm(y ~ x1 + x2)\n\nModify your simulation study and see what happens to \\(\\beta_1\\) and the model variance."
  },
  {
    "objectID": "lectures/lecture_13.html#simulation-study-2-1",
    "href": "lectures/lecture_13.html#simulation-study-2-1",
    "title": "Multivariable Linear Regression",
    "section": "Simulation Study 2",
    "text": "Simulation Study 2\nSimulate 1000 random variables from the following model:\n\\[\nY = 3 + 2log(X_1) + \\epsilon\n\\]\n\n\\(X_1\\sim N(8,1)\\)\n\\(\\epsilon\\sim N(0, 2)\\)"
  },
  {
    "objectID": "lectures/lecture_13.html#fit-model-1",
    "href": "lectures/lecture_13.html#fit-model-1",
    "title": "Multivariable Linear Regression",
    "section": "Fit Model",
    "text": "Fit Model\nFit a model between \\(Y\\) and \\(X_1\\).\nRepeat the process 1000 times. and answer the following questions:\n\nOn average does \\(\\beta_1\\) get estimated correctly? Why?\nWhat is the average model variance?"
  },
  {
    "objectID": "lectures/lecture_13.html#slr-model",
    "href": "lectures/lecture_13.html#slr-model",
    "title": "Multivariable Linear Regression",
    "section": "SLR Model",
    "text": "SLR Model\nFit a simple linear regression model using \\(\\log(X_1)\\) instead.\n\nlm(y ~ log(x1))\n\nModify your simulation study and see what happens to \\(\\beta_1\\) and the model variance."
  },
  {
    "objectID": "lectures/lecture_13.html#matrix-formulation-1",
    "href": "lectures/lecture_13.html#matrix-formulation-1",
    "title": "Multivariable Linear Regression",
    "section": "Matrix Formulation",
    "text": "Matrix Formulation\n\\[\nY_i = \\boldsymbol X_i^\\mathrm T \\boldsymbol \\beta + \\epsilon_i\n\\]\n\n\\(Y_i\\): Outcome Variable\n\\(\\boldsymbol X_i\\): Predictors\n\\(\\boldsymbol \\beta\\): Coefficients\n\\(\\epsilon_i\\): error term"
  },
  {
    "objectID": "lectures/lecture_13.html#matrix-formulation-2",
    "href": "lectures/lecture_13.html#matrix-formulation-2",
    "title": "Multivariable Linear Regression",
    "section": "Matrix Formulation",
    "text": "Matrix Formulation\nFit the following models using matrix formulas instead of the lm function.\n\\[\nbody\\_mass\\_g = \\beta_0+\\beta_1flipper\\_length\\_mm + \\beta_2bill\\_length\\_mm\n\\]\n\\[\nbody\\_mass\\_g = \\beta_0 +\\beta_1 flipper\\_length\\_mm + \\beta_2 dream \\_island +\\beta_3 biscoe\\_island  \n\\]\n\\[\n\\boldsymbol\\beta = (\\boldsymbol X^\\mathrm T \\boldsymbol X)^{-1} \\boldsymbol X^\\mathrm T \\boldsymbol Y\n\\]\nDefine \\(\\boldsymbol X\\) in each case."
  },
  {
    "objectID": "lectures/lecture_15.html#learning-objective",
    "href": "lectures/lecture_15.html#learning-objective",
    "title": "Residual Analysis",
    "section": "Learning Objective",
    "text": "Learning Objective\n\nModel Assumptions\nResidual Analysis\nMulticollinearity"
  },
  {
    "objectID": "lectures/lecture_15.html#model",
    "href": "lectures/lecture_15.html#model",
    "title": "Residual Analysis",
    "section": "Model",
    "text": "Model\n\\[\ny = \\beta_0 + \\beta_1 x + \\epsilon\n\\]\n\n\\(\\epsilon \\sim N(0,\\sigma^2)\\)"
  },
  {
    "objectID": "lectures/lecture_15.html#model-scatter-plot",
    "href": "lectures/lecture_15.html#model-scatter-plot",
    "title": "Residual Analysis",
    "section": "Model Scatter Plot",
    "text": "Model Scatter Plot\n\nx &lt;- rnorm(1000, 8)\ny &lt;- -4 + 2*x + rnorm(1000, sd = 1)\ndf &lt;- tibble(x,y)\nggplot(df, aes(x,y)) + geom_point() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/lecture_15.html#model-assumptions-1",
    "href": "lectures/lecture_15.html#model-assumptions-1",
    "title": "Residual Analysis",
    "section": "Model Assumptions",
    "text": "Model Assumptions\n\nErrors are normally distributed\nConstant Variance\nLinearity\nIndependence\nNo outliers"
  },
  {
    "objectID": "lectures/lecture_15.html#errors-normally-distributed",
    "href": "lectures/lecture_15.html#errors-normally-distributed",
    "title": "Residual Analysis",
    "section": "Errors Normally Distributed",
    "text": "Errors Normally Distributed"
  },
  {
    "objectID": "lectures/lecture_15.html#constant-variance",
    "href": "lectures/lecture_15.html#constant-variance",
    "title": "Residual Analysis",
    "section": "Constant Variance",
    "text": "Constant Variance"
  },
  {
    "objectID": "lectures/lecture_15.html#constant-variance-1",
    "href": "lectures/lecture_15.html#constant-variance-1",
    "title": "Residual Analysis",
    "section": "Constant Variance",
    "text": "Constant Variance\n\nx &lt;- rnorm(1000, 8, sd = 0.5)\ny &lt;- sapply(x, \\(.) -4 + 5*. + rnorm(1, sd = ./2))\ndf &lt;- tibble(x,y)\nggplot(df, aes(x,y)) + geom_point() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/lecture_15.html#linearity",
    "href": "lectures/lecture_15.html#linearity",
    "title": "Residual Analysis",
    "section": "Linearity",
    "text": "Linearity\n\nx &lt;- rnorm(1000, 8)\ny &lt;- -4 + 2*x + rnorm(1000, sd = 1)\ndf &lt;- tibble(x,y)\nggplot(df, aes(x,y)) + geom_point() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/lecture_15.html#linearity-1",
    "href": "lectures/lecture_15.html#linearity-1",
    "title": "Residual Analysis",
    "section": "Linearity",
    "text": "Linearity\n\nx &lt;- rnorm(1000, 8, sd = 4)\ny &lt;- -4 - 1*x + -2*x^2 + rnorm(1000, sd = 16)\ndf &lt;- tibble(x,y)\nggplot(df, aes(x,y)) + geom_point() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/lecture_15.html#no-outliers",
    "href": "lectures/lecture_15.html#no-outliers",
    "title": "Residual Analysis",
    "section": "No Outliers",
    "text": "No Outliers\n\nx &lt;- rnorm(1000, 8)\ny &lt;- -4 + 2*x + rnorm(1000, sd = 1)\ndf &lt;- tibble(x,y)\nggplot(df, aes(x,y)) + geom_point() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/lecture_15.html#residuals",
    "href": "lectures/lecture_15.html#residuals",
    "title": "Residual Analysis",
    "section": "Residuals",
    "text": "Residuals\nResiduals are the errors between the observed value and the estimated model. Common residuals include\n\nRaw Residual\nStandardized Residual\nJackknife (studentized) Residuals"
  },
  {
    "objectID": "lectures/lecture_15.html#influential-measurements",
    "href": "lectures/lecture_15.html#influential-measurements",
    "title": "Residual Analysis",
    "section": "Influential Measurements",
    "text": "Influential Measurements\nInfluential measures are statistics that determine how much a data point affects the model. Common influential measures are\n\nLeverages\nCook’s Distance"
  },
  {
    "objectID": "lectures/lecture_15.html#raw-residuals",
    "href": "lectures/lecture_15.html#raw-residuals",
    "title": "Residual Analysis",
    "section": "Raw Residuals",
    "text": "Raw Residuals\n\\[\n\\hat r_i = y_i - \\hat y_i\n\\]"
  },
  {
    "objectID": "lectures/lecture_15.html#leverages",
    "href": "lectures/lecture_15.html#leverages",
    "title": "Residual Analysis",
    "section": "Leverages",
    "text": "Leverages\n\\[\nH = \\boldsymbol X (\\boldsymbol X^\\mathrm T\\boldsymbol X)^{-1}\\boldsymbol X ^\\mathrm T\n\\]\n\n\\(\\boldsymbol X\\): design matrix\n\\(h_i = H[i,i]\\): leverage for \\(i\\)th value"
  },
  {
    "objectID": "lectures/lecture_15.html#standardized-residuals",
    "href": "lectures/lecture_15.html#standardized-residuals",
    "title": "Residual Analysis",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\n\\[\n\\hat r^*_i = \\frac{\\hat r_i}{\\sqrt{\\hat\\sigma^2(1-h_{i})}}\n\\]\n\n\\(\\hat \\sigma^2\\): Mean square error\n\\(h_{ii}\\): leverage of \\(i\\)th data point"
  },
  {
    "objectID": "lectures/lecture_15.html#jackknife-residuals",
    "href": "lectures/lecture_15.html#jackknife-residuals",
    "title": "Residual Analysis",
    "section": "Jackknife Residuals",
    "text": "Jackknife Residuals\n\\[\n\\hat r ^\\prime_i = \\frac{y_i - \\hat y_{i(i)}}{\\sqrt{\\hat \\sigma^2_{(i)}(1-h_{i})}}\n\\]\n\n\\(\\hat y_{i(i)}\\): fitted value for \\(i\\)th value from model fitted without \\(i\\)th data point\n\\(\\hat\\sigma^2_{(i)}\\): mean square error from model fitted without \\(i\\)th data point"
  },
  {
    "objectID": "lectures/lecture_15.html#cooks-distance",
    "href": "lectures/lecture_15.html#cooks-distance",
    "title": "Residual Analysis",
    "section": "Cook’s Distance",
    "text": "Cook’s Distance\n\\[\n\\hat d_i = \\frac{(y_i - \\hat y_{i})^2}{(k+1)\\hat \\sigma^2}\\left\\{\\frac{h_i}{(1-h_i)^2}\\right\\}\n\\]\n\n\\(k\\): number of predictors"
  },
  {
    "objectID": "lectures/lecture_15.html#residual-analysis-1",
    "href": "lectures/lecture_15.html#residual-analysis-1",
    "title": "Residual Analysis",
    "section": "Residual Analysis",
    "text": "Residual Analysis\nA residual analysis is used to test the assumptions of linear regression."
  },
  {
    "objectID": "lectures/lecture_15.html#qq-plot",
    "href": "lectures/lecture_15.html#qq-plot",
    "title": "Residual Analysis",
    "section": "QQ Plot",
    "text": "QQ Plot\nA qq (quantile-quantile) plot will plot the estimated quantiles of the residuals against the theoretical quantiles from a normal distribution function. If the points from the qq-plot lie on \\(y=x\\), it is said that the residuals follow a normal distribution."
  },
  {
    "objectID": "lectures/lecture_15.html#residual-vs-fitted-plot",
    "href": "lectures/lecture_15.html#residual-vs-fitted-plot",
    "title": "Residual Analysis",
    "section": "Residual vs Fitted Plot",
    "text": "Residual vs Fitted Plot\nThis plot allows you to assess the linearity, constant variance, and identify potential outliers. Create a scatter plot between the fitted values (x-axis) and the raw/standardized residuals (y-axis)."
  },
  {
    "objectID": "lectures/lecture_15.html#residual-vs-x-plots",
    "href": "lectures/lecture_15.html#residual-vs-x-plots",
    "title": "Residual Analysis",
    "section": "Residual vs X Plots",
    "text": "Residual vs X Plots\nThis plot helps identify issues with linearity and suggests potential solution. Create a scatter plot between raw/standardized residuals (y-axis) and the predictor variables (x-axis)."
  },
  {
    "objectID": "lectures/lecture_15.html#outlier-plots",
    "href": "lectures/lecture_15.html#outlier-plots",
    "title": "Residual Analysis",
    "section": "Outlier Plots",
    "text": "Outlier Plots\nAn outlier plot can tell you if there are any outliers in the data. Create a scatter plot between the index number (x-axis) and standardized/studentized residuals (y-axis)"
  },
  {
    "objectID": "lectures/lecture_15.html#influential-observations-plots",
    "href": "lectures/lecture_15.html#influential-observations-plots",
    "title": "Residual Analysis",
    "section": "Influential Observations Plots",
    "text": "Influential Observations Plots\nWill identify outliers/observations that will have an affect on the model. Create a scatter plot between the index number (x-axis) and leverages/cook’s distance (y-axis)."
  },
  {
    "objectID": "lectures/lecture_15.html#mulitcollinearity",
    "href": "lectures/lecture_15.html#mulitcollinearity",
    "title": "Residual Analysis",
    "section": "Mulitcollinearity",
    "text": "Mulitcollinearity\nMulticolinearity occurs when predictor variable have a correlation between each other. Collinearity between predictor variables with inflate the standard errors and cause problems with inference."
  },
  {
    "objectID": "lectures/lecture_15.html#variance-inflation-factor",
    "href": "lectures/lecture_15.html#variance-inflation-factor",
    "title": "Residual Analysis",
    "section": "Variance Inflation Factor",
    "text": "Variance Inflation Factor\nThe variance inflation factor is a measurement on how much variables are collinear with each other. A value greater than 10 is a cause for concern and action should be taken."
  },
  {
    "objectID": "lectures/lecture_15.html#fitting-a-model",
    "href": "lectures/lecture_15.html#fitting-a-model",
    "title": "Residual Analysis",
    "section": "Fitting a model",
    "text": "Fitting a model\n\nx_lm &lt;- iris %$% lm(Petal.Length ~ Sepal.Length + Sepal.Width) %T&gt;% \n  (\\(.) print(summary(.)))\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Petal.Length ~ Sepal.Length + Sepal.Width)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -1.25582 -0.46922 -0.05741  0.45530  1.75599 \n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  -2.52476    0.56344  -4.481 1.48e-05 ***\n#&gt; Sepal.Length  1.77559    0.06441  27.569  &lt; 2e-16 ***\n#&gt; Sepal.Width  -1.33862    0.12236 -10.940  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.6465 on 147 degrees of freedom\n#&gt; Multiple R-squared:  0.8677, Adjusted R-squared:  0.8659 \n#&gt; F-statistic:   482 on 2 and 147 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/lecture_15.html#data-prep",
    "href": "lectures/lecture_15.html#data-prep",
    "title": "Residual Analysis",
    "section": "Data Prep",
    "text": "Data Prep\n\ndf_resid &lt;- tibble(obs = 1:nrow(x_lm$model),\n                   x_lm$model, \n                   resid = resid(x_lm),\n                   fitted = fitted(x_lm),\n                   sresid = rstandard(x_lm),\n                   hatvals = hatvalues(x_lm),\n                   jackknife =  rstudent(x_lm),\n                   cooks = cooks.distance(x_lm)\n                   )"
  },
  {
    "objectID": "lectures/lecture_15.html#residual-vs-fitted",
    "href": "lectures/lecture_15.html#residual-vs-fitted",
    "title": "Residual Analysis",
    "section": "Residual vs Fitted",
    "text": "Residual vs Fitted\n\nggplot(df_resid, aes(fitted, resid)) + geom_point() +\n  geom_hline(yintercept = 0) +\n  geom_smooth(se = F) +\n  theme_bw()"
  },
  {
    "objectID": "lectures/lecture_15.html#qq-plot-1",
    "href": "lectures/lecture_15.html#qq-plot-1",
    "title": "Residual Analysis",
    "section": "QQ Plot",
    "text": "QQ Plot\n\nggplot(df_resid, aes(sample = resid)) + stat_qq() +\n  stat_qq_line() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/lecture_15.html#residuals-vs-x",
    "href": "lectures/lecture_15.html#residuals-vs-x",
    "title": "Residual Analysis",
    "section": "Residuals vs X",
    "text": "Residuals vs X\n\nggplot(df_resid, aes(Sepal.Length, resid)) + geom_point() +\n  geom_hline(yintercept = 0) +\n  stat_smooth(se = F) +\n  theme_bw()\n\n\n\n\n\n\n\nggplot(df_resid, aes(Sepal.Width, resid)) + geom_point() +\n  geom_hline(yintercept = 0) +\n  stat_smooth(se = F) +\n  theme_bw()"
  },
  {
    "objectID": "lectures/lecture_15.html#jackknife-residuals-1",
    "href": "lectures/lecture_15.html#jackknife-residuals-1",
    "title": "Residual Analysis",
    "section": "Jackknife Residuals",
    "text": "Jackknife Residuals\n\nggplot(df_resid, aes(obs, jackknife)) + geom_point() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/lecture_15.html#levarages",
    "href": "lectures/lecture_15.html#levarages",
    "title": "Residual Analysis",
    "section": "Levarages",
    "text": "Levarages\n\nggplot(df_resid, aes(obs, hatvals)) + geom_point() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/lecture_15.html#cooks-distance-1",
    "href": "lectures/lecture_15.html#cooks-distance-1",
    "title": "Residual Analysis",
    "section": "Cook’s Distance",
    "text": "Cook’s Distance\n\nggplot(df_resid, aes(obs, cooks)) + geom_point() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/lecture_15.html#multicolinearity",
    "href": "lectures/lecture_15.html#multicolinearity",
    "title": "Residual Analysis",
    "section": "Multicolinearity",
    "text": "Multicolinearity\n\nlibrary(car)\niris %$% cor(Sepal.Length, Sepal.Width)\n\n#&gt; [1] -0.1175698\n\nvif(x_lm)\n\n#&gt; Sepal.Length  Sepal.Width \n#&gt;     1.014016     1.014016"
  },
  {
    "objectID": "lectures/lecture_17.html#learning-outcomes",
    "href": "lectures/lecture_17.html#learning-outcomes",
    "title": "Generalized Linear Models",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nExponential Family of Distributions\nGeneralized Linear Models\nR Code"
  },
  {
    "objectID": "lectures/lecture_17.html#exponential-family-of-distributions-1",
    "href": "lectures/lecture_17.html#exponential-family-of-distributions-1",
    "title": "Generalized Linear Models",
    "section": "Exponential Family of Distributions",
    "text": "Exponential Family of Distributions\nAn exponential family of distributions are random variables that allow their probability density function to have the following form:\n\\[\nf(y; \\theta,\\phi) = a(y,\\phi)\\exp\\left\\{\\frac{y\\theta-\\kappa(\\theta)}{\\phi}\\right\\}\n\\]\n\n\\(\\theta\\): is the canonical parameter (also a function of other parameters)\n\\(\\kappa(\\theta)\\): is a known cumulant function\n\\(\\phi&gt;0\\): dispersion parameter function\n\\(a(y,\\phi)\\): normalizing constant"
  },
  {
    "objectID": "lectures/lecture_17.html#canonical-parameter",
    "href": "lectures/lecture_17.html#canonical-parameter",
    "title": "Generalized Linear Models",
    "section": "Canonical Parameter",
    "text": "Canonical Parameter\nThe canonical parameter represents the relationship between the random variable and the \\(E(Y)=\\mu\\)"
  },
  {
    "objectID": "lectures/lecture_17.html#normal-distribution",
    "href": "lectures/lecture_17.html#normal-distribution",
    "title": "Generalized Linear Models",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\\[\nf(x;\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\]"
  },
  {
    "objectID": "lectures/lecture_17.html#binomial-distribution",
    "href": "lectures/lecture_17.html#binomial-distribution",
    "title": "Generalized Linear Models",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\\[\nf(x;n,p)=\\big(^n_x\\big) p^x(1-p)^{n-x}\n\\]"
  },
  {
    "objectID": "lectures/lecture_17.html#poisson-distribution",
    "href": "lectures/lecture_17.html#poisson-distribution",
    "title": "Generalized Linear Models",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\\[\nf(x;\\lambda) = \\frac{e^{-\\lambda}\\lambda^x}{x!}\n\\]"
  },
  {
    "objectID": "lectures/lecture_17.html#common-distributions-and-canonical-parameters",
    "href": "lectures/lecture_17.html#common-distributions-and-canonical-parameters",
    "title": "Generalized Linear Models",
    "section": "Common Distributions and Canonical Parameters",
    "text": "Common Distributions and Canonical Parameters\n\n\n\nRandom Variable\nCanonical Parameter\n\n\n\n\nNormal\n\\(\\mu\\)\n\n\nBinomial\n\\(\\log\\left(\\frac{\\mu}{1-\\mu}\\right)\\)\n\n\nNegative Binomial\n\\(\\log\\left(\\frac{\\mu}{\\mu+k}\\right)\\)\n\n\nPoisson\n\\(\\log(\\mu)\\)\n\n\nGamma\n\\(-\\frac{1}{\\mu}\\)\n\n\nInverse Gaussian\n\\(-\\frac{1}{2\\mu^2}\\)"
  },
  {
    "objectID": "lectures/lecture_17.html#generalized-linear-models-1",
    "href": "lectures/lecture_17.html#generalized-linear-models-1",
    "title": "Generalized Linear Models",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nA generalized linear model (GLM) is used to model the association between an outcome variable (of any data type) and a set of predictor values. We estimate a set of regression coefficients \\(\\boldsymbol \\beta\\) to explain how each predictor is related to the expected value of the outcome."
  },
  {
    "objectID": "lectures/lecture_17.html#generalized-linear-models-2",
    "href": "lectures/lecture_17.html#generalized-linear-models-2",
    "title": "Generalized Linear Models",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nA GLM is composed of a systematic and random component."
  },
  {
    "objectID": "lectures/lecture_17.html#random-component",
    "href": "lectures/lecture_17.html#random-component",
    "title": "Generalized Linear Models",
    "section": "Random Component",
    "text": "Random Component\nThe random component is the random variable that defines the randomness and variation of the outcome variable."
  },
  {
    "objectID": "lectures/lecture_17.html#systematic-component",
    "href": "lectures/lecture_17.html#systematic-component",
    "title": "Generalized Linear Models",
    "section": "Systematic Component",
    "text": "Systematic Component\nThe systematic component is the linear model that models the association between a set of predictors and the expected value of Y:\n\\[\ng(\\mu)=\\eta=\\boldsymbol X_i^\\mathrm T \\boldsymbol \\beta\n\\]\n\n\\(\\boldsymbol\\beta\\): regression coefficients\n\\(\\boldsymbol X_i=(1, X_{i1}, \\ldots, X_{ip})^\\mathrm T\\): design vector\n\\(\\eta\\): linear model\n\\(\\mu=E(Y)\\)\n\\(g(\\cdot)\\): link function"
  },
  {
    "objectID": "lectures/lecture_17.html#general-r-code",
    "href": "lectures/lecture_17.html#general-r-code",
    "title": "Generalized Linear Models",
    "section": "General R Code",
    "text": "General R Code\n\n1glm(formula,\n2    data,\n3    family)\n\n\n1\n\nSupply a formula for R\n\n2\n\nSupply the data frame\n\n3\n\nWhich family and link function is used to model data"
  },
  {
    "objectID": "lectures/lecture_17.html#logistic-binomial-regression",
    "href": "lectures/lecture_17.html#logistic-binomial-regression",
    "title": "Generalized Linear Models",
    "section": "Logistic (Binomial) Regression",
    "text": "Logistic (Binomial) Regression\nLogistic Regression is used when your outcome is binary:\n\nglm(y~x, data, family = binomial())"
  },
  {
    "objectID": "lectures/lecture_17.html#poisson-regression",
    "href": "lectures/lecture_17.html#poisson-regression",
    "title": "Generalized Linear Models",
    "section": "Poisson Regression",
    "text": "Poisson Regression\nPoisson Regression is used when the outcome is count data:\n\nglm(y~x, data, family = poisson())"
  },
  {
    "objectID": "lectures/lecture_17.html#gamma-regression",
    "href": "lectures/lecture_17.html#gamma-regression",
    "title": "Generalized Linear Models",
    "section": "Gamma Regression",
    "text": "Gamma Regression\nGamma Regression is used when modeling the association between predictors and positive continuous values:\n\nglm(y~x, data, family = Gamma())"
  },
  {
    "objectID": "lectures/lecture_17.html#negative-binomial-regression",
    "href": "lectures/lecture_17.html#negative-binomial-regression",
    "title": "Generalized Linear Models",
    "section": "Negative Binomial Regression",
    "text": "Negative Binomial Regression\nNegative Binomial Regression is used four with overdispersed count data, where the variance is larger than expected.\n\nlibrary(MASS)\nglm.nb(y~x, data)"
  },
  {
    "objectID": "lectures/lecture_17.html#inverse-gaussian-regression",
    "href": "lectures/lecture_17.html#inverse-gaussian-regression",
    "title": "Generalized Linear Models",
    "section": "Inverse Gaussian Regression",
    "text": "Inverse Gaussian Regression\nInverse Gaussian Regression is used for overly dispersed positive continuous data where Gamma Regression is inappropriate:\n\nglm(y~x, data, family=inverse.gaussian())"
  },
  {
    "objectID": "lectures/lecture_17.html#fitting-models",
    "href": "lectures/lecture_17.html#fitting-models",
    "title": "Generalized Linear Models",
    "section": "Fitting Models",
    "text": "Fitting Models\nUse the survival, MASS and GLMsData R packages to fit the models for examples.\n\nlibrary(GLMsData)\nlibrary(survival)\nlibrary(MASS)\nlibrary(magrittr)"
  },
  {
    "objectID": "lectures/lecture_17.html#logistic-regression",
    "href": "lectures/lecture_17.html#logistic-regression",
    "title": "Generalized Linear Models",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nWe are going to fit y, the presence of bacteria, and the predictors ap, active or placebo drug, and hilo, high/low compliance, from the bacteria data set.\n\nbacteria %$% glm(y ~ ap + hilo, \n                 family = binomial(link = \"logit\")) %&gt;% \n  summary\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = y ~ ap + hilo, family = binomial(link = \"logit\"))\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)   1.3539     0.2855   4.743 2.11e-06 ***\n#&gt; app           0.7933     0.3748   2.116   0.0343 *  \n#&gt; hilolo       -0.4816     0.3480  -1.384   0.1664    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 217.38  on 219  degrees of freedom\n#&gt; Residual deviance: 209.87  on 217  degrees of freedom\n#&gt; AIC: 215.87\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "lectures/lecture_17.html#poisson-regression-1",
    "href": "lectures/lecture_17.html#poisson-regression-1",
    "title": "Generalized Linear Models",
    "section": "Poisson Regression",
    "text": "Poisson Regression\nFit a model between the outcome recur, number of reccurrence, and predictors treatment, drugs or placebo, and number, the initial number of tumors, from the bladder1 data set.\n\nbladder1 %$% glm(recur ~ treatment + number, \n                 family = poisson(link = \"log\")) %&gt;% \n  summary\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = recur ~ treatment + number, family = poisson(link = \"log\"))\n#&gt; \n#&gt; Coefficients:\n#&gt;                     Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)          1.00918    0.06057  16.661  &lt; 2e-16 ***\n#&gt; treatmentpyridoxine  0.25506    0.06889   3.702 0.000214 ***\n#&gt; treatmentthiotepa   -0.45167    0.08626  -5.236 1.64e-07 ***\n#&gt; number               0.11603    0.01620   7.164 7.82e-13 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for poisson family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 868.47  on 293  degrees of freedom\n#&gt; Residual deviance: 772.19  on 290  degrees of freedom\n#&gt; AIC: 1529.5\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 5"
  },
  {
    "objectID": "lectures/lecture_17.html#negative-binomial-regression-1",
    "href": "lectures/lecture_17.html#negative-binomial-regression-1",
    "title": "Generalized Linear Models",
    "section": "Negative Binomial Regression",
    "text": "Negative Binomial Regression\nFit a model between the outcome Count, viral activity (pock counts), and predictor Dilution factor from the pock data set.\n\ndata(\"pock\")\npock %$% glm.nb(Count ~ Dilution) %&gt;% summary\n\n#&gt; \n#&gt; Call:\n#&gt; glm.nb(formula = Count ~ Dilution, init.theta = 6.045172803, \n#&gt;     link = log)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)  5.05810    0.09622   52.57   &lt;2e-16 ***\n#&gt; Dilution    -0.19204    0.01322  -14.53   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for Negative Binomial(6.0452) family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 262.687  on 47  degrees of freedom\n#&gt; Residual deviance:  48.891  on 46  degrees of freedom\n#&gt; AIC: 427.26\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 1\n#&gt; \n#&gt; \n#&gt;               Theta:  6.05 \n#&gt;           Std. Err.:  1.43 \n#&gt; \n#&gt;  2 x log-likelihood:  -421.261"
  },
  {
    "objectID": "lectures/lecture_17.html#gamma-regression-1",
    "href": "lectures/lecture_17.html#gamma-regression-1",
    "title": "Generalized Linear Models",
    "section": "Gamma Regression",
    "text": "Gamma Regression\nFit a model between the outcome Foliage, foliage biomass, and predictor DBH, tree diameter at breast height, from the lime data set.\n\ndata(\"lime\")\nlime %$% glm(Foliage ~ DBH, \n             family = Gamma(link = \"log\")) %&gt;% \n  summary \n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = Foliage ~ DBH, family = Gamma(link = \"log\"))\n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -1.786052   0.085994  -20.77   &lt;2e-16 ***\n#&gt; DBH          0.122388   0.004736   25.84   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for Gamma family taken to be 0.5432673)\n#&gt; \n#&gt;     Null deviance: 508.48  on 384  degrees of freedom\n#&gt; Residual deviance: 189.57  on 383  degrees of freedom\n#&gt; AIC: 831.65\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 5"
  },
  {
    "objectID": "lectures/lecture_17.html#inverse-gaussian-regression-1",
    "href": "lectures/lecture_17.html#inverse-gaussian-regression-1",
    "title": "Generalized Linear Models",
    "section": "Inverse Gaussian Regression",
    "text": "Inverse Gaussian Regression\nFit a model between the outcome Foliage, foliage biomass, and predictor DBH, tree diameter at breast height, from the lime data set.\n\ndata(\"lime\")\nlime %$% glm(Foliage ~ DBH, \n             family = inverse.gaussian(link = \"log\")) %&gt;% \n  summary\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = Foliage ~ DBH, family = inverse.gaussian(link = \"log\"))\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  -2.7290     0.1113  -24.53   &lt;2e-16 ***\n#&gt; DBH           0.2077     0.0118   17.59   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for inverse.gaussian family taken to be 1.571078)\n#&gt; \n#&gt;     Null deviance: 873.60  on 384  degrees of freedom\n#&gt; Residual deviance: 542.83  on 383  degrees of freedom\n#&gt; AIC: 1192.6\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 16"
  },
  {
    "objectID": "lectures/lecture_19.html#learning-outcomes",
    "href": "lectures/lecture_19.html#learning-outcomes",
    "title": "Generalized Linear Models",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nModel Diagnostics\nResiduals\nResidual Analysis\nModel Selection"
  },
  {
    "objectID": "lectures/lecture_19.html#assumptions",
    "href": "lectures/lecture_19.html#assumptions",
    "title": "Generalized Linear Models",
    "section": "Assumptions",
    "text": "Assumptions\n\nLinearity\nCorrect Distribution\n\nRandom Component\nLink Function\n\nDispersion parameter constant\nIndependence\nNo Outliers"
  },
  {
    "objectID": "lectures/lecture_19.html#raw-residuals",
    "href": "lectures/lecture_19.html#raw-residuals",
    "title": "Generalized Linear Models",
    "section": "Raw Residuals",
    "text": "Raw Residuals\n\\[\nr = y - \\hat \\mu\n\\]\n\n\\(\\hat \\mu\\): fitted value"
  },
  {
    "objectID": "lectures/lecture_19.html#working-responses",
    "href": "lectures/lecture_19.html#working-responses",
    "title": "Generalized Linear Models",
    "section": "Working Responses",
    "text": "Working Responses\n\\[\nz_i = \\hat \\eta_i + \\frac{d\\eta_i}{d\\mu_i}(y_i-\\hat\\mu_i)\n\\]"
  },
  {
    "objectID": "lectures/lecture_19.html#partial-residuals",
    "href": "lectures/lecture_19.html#partial-residuals",
    "title": "Generalized Linear Models",
    "section": "Partial Residuals",
    "text": "Partial Residuals\n\\[\nu_{ij} = e_i + \\hat\\beta_j x_{ij}\n\\]\n\n\\(e_i = z_i - \\hat \\eta_i\\)"
  },
  {
    "objectID": "lectures/lecture_19.html#pearson-residuals",
    "href": "lectures/lecture_19.html#pearson-residuals",
    "title": "Generalized Linear Models",
    "section": "Pearson Residuals",
    "text": "Pearson Residuals\n\\[\nr^* = \\frac{y-\\hat \\mu}{\\sqrt{V(\\hat\\mu)}}\n\\]\n\n\\(\\hat\\mu\\): fitted value\n\\(V(\\mu)\\): variance function"
  },
  {
    "objectID": "lectures/lecture_19.html#deviance-residuals",
    "href": "lectures/lecture_19.html#deviance-residuals",
    "title": "Generalized Linear Models",
    "section": "Deviance Residuals",
    "text": "Deviance Residuals\n\\[\nr^\\prime = \\mathrm{sign}(y-\\hat\\mu)\\sqrt{d(y,\\hat\\mu)}\n\\]\n\n\\(d(y,\\mu)=2\\left\\{t(y,y) - t(y,\\mu)\\right\\}\\)\n\\(t(y, \\mu)=y\\theta - \\kappa(\\theta)\\)"
  },
  {
    "objectID": "lectures/lecture_19.html#quantile-residuals",
    "href": "lectures/lecture_19.html#quantile-residuals",
    "title": "Generalized Linear Models",
    "section": "Quantile Residuals",
    "text": "Quantile Residuals\n\\[\nr^+ = \\Phi^{-1}\\left\\{F(y;\\hat\\mu,\\phi)\\right\\}\n\\]\n\n\\(\\Phi^{-1}\\): Inverse CDF of Standard Normal Distribution\n\\(F(\\cdot)\\): CDF of random variable \\(y\\)"
  },
  {
    "objectID": "lectures/lecture_19.html#leverages",
    "href": "lectures/lecture_19.html#leverages",
    "title": "Generalized Linear Models",
    "section": "Leverages",
    "text": "Leverages\n\\[\nH = W^{1/2} X(X^{\\mathrm T}X)^{-1}X^{\\mathrm T}W^{1/2}\n\\]\n\n\\(X\\): Design Matrix\n\\(W = \\mathrm{diag}(W_1, \\cdots, W_n)\\)\n\\(W_i = \\frac{(d\\mu_i/d\\eta_i)^2}{V(\\hat\\mu_i)}\\)\n\\(h_i\\): diagonal element of \\(H\\) is the the leverage for observation \\(i\\)"
  },
  {
    "objectID": "lectures/lecture_19.html#standardized-residuals",
    "href": "lectures/lecture_19.html#standardized-residuals",
    "title": "Generalized Linear Models",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\n\n\n\\[\nr^*_s = \\frac{y -\\hat \\mu}{\\sqrt{\\hat\\phi V(\\hat\\mu)(1-h)}}\n\\]\n\n\\[\nr^\\prime_s = \\frac{\\mathrm{sign}(y-\\hat\\mu)\\sqrt{d(y,\\hat\\mu)}}{\\sqrt{\\hat\\phi(1-h)}}\n\\]"
  },
  {
    "objectID": "lectures/lecture_19.html#cooks-distance",
    "href": "lectures/lecture_19.html#cooks-distance",
    "title": "Generalized Linear Models",
    "section": "Cook’s Distance",
    "text": "Cook’s Distance\n\\[\nD \\approx\\frac{(r^*_s)^2}{p^\\prime}\\frac{h}{1-h}\n\\]\n\n\\(p^\\prime\\): number of predictors plus one"
  },
  {
    "objectID": "lectures/lecture_19.html#residual-plots",
    "href": "lectures/lecture_19.html#residual-plots",
    "title": "Generalized Linear Models",
    "section": "Residual Plots",
    "text": "Residual Plots\n\nResidual plots are used to test both the systematic and random component of the model\nIt can also be used to identify potential outliers in the data."
  },
  {
    "objectID": "lectures/lecture_19.html#fitting-a-model",
    "href": "lectures/lecture_19.html#fitting-a-model",
    "title": "Generalized Linear Models",
    "section": "Fitting a Model",
    "text": "Fitting a Model\n\nbin_model &lt;- bacteria %$% glm(y ~ ap + hilo, \n                 family = binomial(link = \"logit\"))"
  },
  {
    "objectID": "lectures/lecture_19.html#residual-plot-data-frame",
    "href": "lectures/lecture_19.html#residual-plot-data-frame",
    "title": "Generalized Linear Models",
    "section": "Residual Plot Data Frame",
    "text": "Residual Plot Data Frame\n\nlibrary(statmod)\n1resid_df &lt;- data.frame(obs = 1:nrow(bin_model$model),\n2                       bin_model$model,\n3                       fitted = fitted(bin_model),\n4                       eta = bin_model$linear.predictors,\n5                       raw = resid(bin_model, type = \"response\"),\n6                       pearson = resid(bin_model, type = \"pearson\"),\n7                       deviance = resid(bin_model),\n                       working = resid(bin_model, type = \"working\") +\n8                                 bin_model$linear.predictors,\n9                       partial = resid(bin_model, type = \"partial\"),\n                       std_pear = rstandard(bin_model, \n10                                            type = \"pearson\"),\n11                       std_dev = rstandard(bin_model),\n12                       stud_dev = rstudent(bin_model),\n13                       qresid = qresid(bin_model),\n14                       leverages = hatvalues(bin_model),\n15                       cooks = cooks.distance(bin_model))\n\n\n1\n\nIndex\n\n2\n\nData to fit model\n\n3\n\n\\(\\hat \\mu\\)\n\n4\n\n\\(\\hat \\eta\\)\n\n5\n\nRaw Residual\n\n6\n\nPearson Residual\n\n7\n\nDeviance Residual\n\n8\n\nWorking Responses\n\n9\n\nPartial Values: each individual X\n\n10\n\nStandardized Pearson Residuals\n\n11\n\nStandardized Deviance Residuals\n\n12\n\nStudentized Deviance Residuals\n\n13\n\nQuantile Residuals\n\n14\n\nLeverages\n\n15\n\nCook’s Distance"
  },
  {
    "objectID": "lectures/lecture_19.html#residuals-vs-fitted",
    "href": "lectures/lecture_19.html#residuals-vs-fitted",
    "title": "Generalized Linear Models",
    "section": "Residuals vs Fitted",
    "text": "Residuals vs Fitted\n\nlibrary(ggplot2)\nggplot(resid_df, aes(x = fitted, y = deviance)) + geom_point() +\n  stat_smooth() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/lecture_19.html#working-responses-vs-hateta",
    "href": "lectures/lecture_19.html#working-responses-vs-hateta",
    "title": "Generalized Linear Models",
    "section": "Working Responses vs \\(\\hat\\eta\\)",
    "text": "Working Responses vs \\(\\hat\\eta\\)\n\nggplot(resid_df, aes(x = eta, y = working)) + geom_point() +\n  stat_smooth() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/lecture_19.html#partial-residuals-vs-x",
    "href": "lectures/lecture_19.html#partial-residuals-vs-x",
    "title": "Generalized Linear Models",
    "section": "Partial Residuals vs X",
    "text": "Partial Residuals vs X\n\naphilo\n\n\n\nggplot(resid_df, aes(ap, partial.ap)) + geom_point() +\n  stat_smooth() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nggplot(resid_df, aes(hilo, partial.hilo)) + geom_point() +\n  stat_smooth() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/lecture_19.html#qq-plot-of-quantile-residuals",
    "href": "lectures/lecture_19.html#qq-plot-of-quantile-residuals",
    "title": "Generalized Linear Models",
    "section": "QQ Plot of Quantile Residuals",
    "text": "QQ Plot of Quantile Residuals\n\nggplot(resid_df, aes(sample = qresid)) + stat_qq() +\n  stat_qq_line() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/lecture_19.html#leverages-plot",
    "href": "lectures/lecture_19.html#leverages-plot",
    "title": "Generalized Linear Models",
    "section": "Leverages Plot",
    "text": "Leverages Plot\n\nggplot(resid_df, aes(obs, leverages)) + geom_point() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/lecture_19.html#cooks-distance-plot",
    "href": "lectures/lecture_19.html#cooks-distance-plot",
    "title": "Generalized Linear Models",
    "section": "Cook’s Distance Plot",
    "text": "Cook’s Distance Plot\n\nggplot(resid_df, aes(obs, cooks)) + geom_point() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/lecture_19.html#aic",
    "href": "lectures/lecture_19.html#aic",
    "title": "Generalized Linear Models",
    "section": "AIC",
    "text": "AIC\n\\[\n\\mathrm{AIC} = -2\\ell(\\hat{\\boldsymbol\\beta},\\hat\\phi,y) + 2 p^{\\prime}\n\\]\n\n\\(\\ell(\\cdot)\\): log-likelihood function\n\\(p^\\prime\\): number of predictors plus one\nLower is better"
  },
  {
    "objectID": "lectures/lecture_19.html#bic",
    "href": "lectures/lecture_19.html#bic",
    "title": "Generalized Linear Models",
    "section": "BIC",
    "text": "BIC\n\\[\n\\mathrm{BIC} = -2\\ell(\\hat{\\boldsymbol\\beta}, \\hat \\phi, y) + \\log (n) (p^\\prime+1)\n\\]\n\n\\(\\ell(\\cdot)\\): log-likelihood function\n\\(p^\\prime\\): number of predictors plus one\nLower is better"
  },
  {
    "objectID": "lectures/lecture_20.html#learning-outcomes",
    "href": "lectures/lecture_20.html#learning-outcomes",
    "title": "Generalized Linear Models",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nHypothesis Tests\nModel Inference"
  },
  {
    "objectID": "lectures/lecture_20.html#hypothesis-tests-1",
    "href": "lectures/lecture_20.html#hypothesis-tests-1",
    "title": "Generalized Linear Models",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nHypothesis tests are used to test whether claims are valid or not. This is conducted by collecting data, setting the Null and Alternative Hypothesis."
  },
  {
    "objectID": "lectures/lecture_20.html#null-hypothesis",
    "href": "lectures/lecture_20.html#null-hypothesis",
    "title": "Generalized Linear Models",
    "section": "Null Hypothesis",
    "text": "Null Hypothesis"
  },
  {
    "objectID": "lectures/lecture_20.html#alternative-hypothesis",
    "href": "lectures/lecture_20.html#alternative-hypothesis",
    "title": "Generalized Linear Models",
    "section": "Alternative Hypothesis",
    "text": "Alternative Hypothesis"
  },
  {
    "objectID": "lectures/lecture_20.html#common-hypothesis-tests",
    "href": "lectures/lecture_20.html#common-hypothesis-tests",
    "title": "Generalized Linear Models",
    "section": "Common Hypothesis Tests",
    "text": "Common Hypothesis Tests\n\nt-tests\nANOVA\n\\(\\chi^2\\)-tests"
  },
  {
    "objectID": "lectures/lecture_20.html#testing-beta_j",
    "href": "lectures/lecture_20.html#testing-beta_j",
    "title": "Generalized Linear Models",
    "section": "Testing \\(\\beta_j\\)",
    "text": "Testing \\(\\beta_j\\)\n\n\n\\(\\phi\\) known\n\\[\n\\frac{\\hat\\beta_j - \\theta}{\\mathrm{se}(\\hat\\beta_j)} \\sim N(0,1)\n\\]\n\n\\(\\phi\\) unknown\n\\[\n\\frac{\\hat\\beta_j-\\theta}{\\mathrm{se}(\\hat\\beta_j)} \\sim t_{n-p^\\prime}\n\\]"
  },
  {
    "objectID": "lectures/lecture_20.html#confidence-intervals",
    "href": "lectures/lecture_20.html#confidence-intervals",
    "title": "Generalized Linear Models",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\\[\nPE \\pm CV \\times SE\n\\]\n\nPE: Point Estimate\nCV: Critical Value \\(P(X&lt;CV) = 1-\\alpha/2\\)\n\\(\\alpha\\): significance level\nSE: Standard Error"
  },
  {
    "objectID": "lectures/lecture_20.html#model-inference",
    "href": "lectures/lecture_20.html#model-inference",
    "title": "Generalized Linear Models",
    "section": "Model Inference",
    "text": "Model Inference\n\\[\n\\hat\\eta \\pm CV\\times SE\n\\]\n\\[\n(LB = \\hat\\eta - CV\\times SE, UB = \\hat\\eta + CV\\times SE\n\\]"
  },
  {
    "objectID": "lectures/lecture_20.html#model-inference-for-mu",
    "href": "lectures/lecture_20.html#model-inference-for-mu",
    "title": "Generalized Linear Models",
    "section": "Model inference for \\(\\mu\\)",
    "text": "Model inference for \\(\\mu\\)\n\\[\n\\{g^{-1}(LB), g^{-1}(UB)\\}\n\\]"
  },
  {
    "objectID": "lectures/lecture_20.html#applications-in-r",
    "href": "lectures/lecture_20.html#applications-in-r",
    "title": "Generalized Linear Models",
    "section": "Applications in R",
    "text": "Applications in R\n\nlibrary(survival)\nlibrary(tidyverse)\nlibrary(magrittr)\npois_glm &lt;- bladder1 %$% glm(recur ~ treatment + number + size, \n                family = poisson)"
  },
  {
    "objectID": "lectures/lecture_20.html#individual-hypothesis-testing",
    "href": "lectures/lecture_20.html#individual-hypothesis-testing",
    "title": "Generalized Linear Models",
    "section": "Individual Hypothesis Testing",
    "text": "Individual Hypothesis Testing\n\npois_glm %&gt;% summary\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = recur ~ treatment + number + size, family = poisson)\n#&gt; \n#&gt; Coefficients:\n#&gt;                     Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)          1.10381    0.07532  14.656  &lt; 2e-16 ***\n#&gt; treatmentpyridoxine  0.26176    0.06892   3.798 0.000146 ***\n#&gt; treatmentthiotepa   -0.45650    0.08609  -5.303 1.14e-07 ***\n#&gt; number               0.11152    0.01627   6.853 7.22e-12 ***\n#&gt; size                -0.04337    0.02081  -2.084 0.037132 *  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for poisson family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 868.47  on 293  degrees of freedom\n#&gt; Residual deviance: 767.66  on 289  degrees of freedom\n#&gt; AIC: 1527\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 5"
  },
  {
    "objectID": "lectures/lecture_20.html#model-inference-1",
    "href": "lectures/lecture_20.html#model-inference-1",
    "title": "Generalized Linear Models",
    "section": "Model Inference",
    "text": "Model Inference\nExpected Reoccurrence for those on Pyridoxine and 2 tumors, largest tumor 4 cm.\n\nnewdata &lt;- data.frame(treatment = \"pyridoxine\", number = 2, size = 4)\nres &lt;- pois_glm %&gt;% predict(newdata, \n                     se.fit = T, \n                     type = \"response\") %T&gt;% print\n\n#&gt; $fit\n#&gt;        1 \n#&gt; 4.116953 \n#&gt; \n#&gt; $se.fit\n#&gt;         1 \n#&gt; 0.2683267 \n#&gt; \n#&gt; $residual.scale\n#&gt; [1] 1\n\nci &lt;- (res$fit + c(-1,1) * qnorm(0.975) * res$se.fit) %T&gt;% print\n\n#&gt; [1] 3.591042 4.642863\n\nexp(res$fit) # Expected occurrence\n\n#&gt;        1 \n#&gt; 61.37193\n\nexp(ci) # 95% C\n\n#&gt; [1]  36.27185 103.84125"
  },
  {
    "objectID": "lectures/lecture_20.html#model-inference-2",
    "href": "lectures/lecture_20.html#model-inference-2",
    "title": "Generalized Linear Models",
    "section": "Model Inference",
    "text": "Model Inference\nFor those who are on thiotepa, what is the difference in the average number of recuccrence for a patient who has 6 than a patient who has 0 tumors, keeping size constant."
  },
  {
    "objectID": "lectures/lecture_22.html#learning-outcomes",
    "href": "lectures/lecture_22.html#learning-outcomes",
    "title": "Introduction to Classification",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nK-Nearest Neighbors\nBayes’ Classifier\nLogistic Regression"
  },
  {
    "objectID": "lectures/lecture_22.html#classification",
    "href": "lectures/lecture_22.html#classification",
    "title": "Introduction to Classification",
    "section": "Classification",
    "text": "Classification\nThe practice of classifying data points into different categories."
  },
  {
    "objectID": "lectures/lecture_22.html#k-nearest-neighbors-1",
    "href": "lectures/lecture_22.html#k-nearest-neighbors-1",
    "title": "Introduction to Classification",
    "section": "K-Nearest Neighbors",
    "text": "K-Nearest Neighbors\nK-Nearest Neighbors will assign a category to a new data point based on the majority category of its K nearest neighbors."
  },
  {
    "objectID": "lectures/lecture_22.html#knn",
    "href": "lectures/lecture_22.html#knn",
    "title": "Introduction to Classification",
    "section": "KNN",
    "text": "KNN"
  },
  {
    "objectID": "lectures/lecture_22.html#distances",
    "href": "lectures/lecture_22.html#distances",
    "title": "Introduction to Classification",
    "section": "Distances",
    "text": "Distances\nThe distance between a given point and the training data must be computed. These are the most commonly used distances:\n\nManhattan\nEuclidean\nMinkowski"
  },
  {
    "objectID": "lectures/lecture_22.html#manhattan-distance",
    "href": "lectures/lecture_22.html#manhattan-distance",
    "title": "Introduction to Classification",
    "section": "Manhattan Distance",
    "text": "Manhattan Distance\n\\[\nd(x,y) = \\sum_{i=1}^{p} |x_i - y_i|\n\\]\n\n\\(x\\): vector values of individual point\n\\(y\\): vector values of new point\n\\(p\\): length of vector"
  },
  {
    "objectID": "lectures/lecture_22.html#euclidean-distance",
    "href": "lectures/lecture_22.html#euclidean-distance",
    "title": "Introduction to Classification",
    "section": "Euclidean Distance",
    "text": "Euclidean Distance\n\\[\nd(x,y) = \\sqrt{\\sum_{i=1}^{p} (x_i - y_i)^2}\n\\]"
  },
  {
    "objectID": "lectures/lecture_22.html#minkowski-distance",
    "href": "lectures/lecture_22.html#minkowski-distance",
    "title": "Introduction to Classification",
    "section": "Minkowski Distance",
    "text": "Minkowski Distance\n\\[\nd(x,y) = \\left( \\sum_{i=1}^{p} |x_i - y_i|^w \\right)^{\\frac{1}{w}}\n\\]"
  },
  {
    "objectID": "lectures/lecture_22.html#algorithm",
    "href": "lectures/lecture_22.html#algorithm",
    "title": "Introduction to Classification",
    "section": "Algorithm",
    "text": "Algorithm\nGiven a training data set, conduct the following steps:\n\nCompute the distance between a new data point and every point in the training data set.\nChoose the \\(K\\) nearest training data points to the new point using the smallest distance.\nCategorize the new data point based on the majority of category from the \\(K\\) nearest training data points."
  },
  {
    "objectID": "lectures/lecture_22.html#bayes-classifier-1",
    "href": "lectures/lecture_22.html#bayes-classifier-1",
    "title": "Introduction to Classification",
    "section": "Bayes Classifier",
    "text": "Bayes Classifier\nBayes Classifier is used to classify a data point to a category \\(c\\)\n\\[\nf(\\boldsymbol x) = argmax_{c \\in C} f(C|\\boldsymbol X)\n\\]"
  },
  {
    "objectID": "lectures/lecture_22.html#probability",
    "href": "lectures/lecture_22.html#probability",
    "title": "Introduction to Classification",
    "section": "Probability",
    "text": "Probability\n\\[\nf(C = c|\\boldsymbol X = x) = \\frac{f(\\boldsymbol X | C)\\pi_c}{f(\\boldsymbol X)}\n\\]\n\n\\(f(\\boldsymbol X| C)\\): conditional distribution of \\(\\boldsymbol X\\)\n\\(\\pi_c\\): probability of observing category \\(C\\)\n\\(f(\\boldsymbol X)\\): marginal distribution of \\(\\boldsymbol X\\)"
  },
  {
    "objectID": "lectures/lecture_22.html#distribution-of-fboldsymbol-xc-and-fboldsymbol-x",
    "href": "lectures/lecture_22.html#distribution-of-fboldsymbol-xc-and-fboldsymbol-x",
    "title": "Introduction to Classification",
    "section": "Distribution of \\(f(\\boldsymbol X|C)\\) and \\(f(\\boldsymbol X)\\)",
    "text": "Distribution of \\(f(\\boldsymbol X|C)\\) and \\(f(\\boldsymbol X)\\)\nTo apply Bayes classifier, we must specify the form of \\(f(\\boldsymbol X| C)\\) and \\(f(\\boldsymbol X)\\). Common distributions are:\n\nNormal\nBernoulli\nMultinomial"
  },
  {
    "objectID": "lectures/lecture_22.html#logistic-regression-1",
    "href": "lectures/lecture_22.html#logistic-regression-1",
    "title": "Introduction to Classification",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic regression can be used to classify an input of \\(\\boldsymbol X\\) to a binary category."
  },
  {
    "objectID": "lectures/lecture_22.html#multinomial-regression",
    "href": "lectures/lecture_22.html#multinomial-regression",
    "title": "Introduction to Classification",
    "section": "Multinomial Regression",
    "text": "Multinomial Regression\nAn extension of logistic regression, where there are more than two categories."
  },
  {
    "objectID": "lectures/lecture_22.html#fitting-a-model",
    "href": "lectures/lecture_22.html#fitting-a-model",
    "title": "Introduction to Classification",
    "section": "Fitting a model",
    "text": "Fitting a model\nUse a glm to fit a model with a Bernoulli or multinomial distribution."
  },
  {
    "objectID": "lectures/lecture_22.html#create-the-knn-algorithm-in-r",
    "href": "lectures/lecture_22.html#create-the-knn-algorithm-in-r",
    "title": "Introduction to Classification",
    "section": "Create the KNN algorithm in R",
    "text": "Create the KNN algorithm in R\n\nManhattan\nEuclidean\nMinkowski (\\(w=5\\))\n\nUse the penguins data set from palmerpenguins and categorize the following data: bill_depth = 19, bill_length = 40, flipper_length = 185, and body_mass = 3345."
  },
  {
    "objectID": "lectures/lecture_24.html#learning-outcomes",
    "href": "lectures/lecture_24.html#learning-outcomes",
    "title": "Resampling Methods",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nCross-Validation\n\nLeave-one-out\nK-Fold\n\nBootstrap Methods"
  },
  {
    "objectID": "lectures/lecture_24.html#cross-validation-1",
    "href": "lectures/lecture_24.html#cross-validation-1",
    "title": "Resampling Methods",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nA cross-validation approach is to obtain a good estimate of the error-rate of a machine learning algorithm. We split the data set into two categories: training and testing. The training data set is used to train the model, and the test data is used to test the model and compute the error rate."
  },
  {
    "objectID": "lectures/lecture_24.html#tuning-parameter",
    "href": "lectures/lecture_24.html#tuning-parameter",
    "title": "Resampling Methods",
    "section": "Tuning Parameter",
    "text": "Tuning Parameter\nA cross-validation approach is great when there is a tuning parameter. We can fit a model for different values of the tuning parameter, and we can choose which value results in the lowest error rate."
  },
  {
    "objectID": "lectures/lecture_24.html#loocv-cross-validation",
    "href": "lectures/lecture_24.html#loocv-cross-validation",
    "title": "Resampling Methods",
    "section": "LOOCV Cross-Validation",
    "text": "LOOCV Cross-Validation\n\nChoose a set of tuning parameters to test.\nFor each \\(k\\)th turning parameter Calculate the tuning parameter error for each value\n\nUtilize the leave-one-out approach\n\nFor each observation fit a model with the remaining observations and fit the excluded value\nCompute the following error:\n\\[\nCVE_k = \\frac{1}{n}\\sum^n_{i=1}e_i\n\\]\n\n\nIdentify the \\(k\\)th tuning parameter with the lowest \\(CVE_k\\)"
  },
  {
    "objectID": "lectures/lecture_24.html#k-fold-cross-validation",
    "href": "lectures/lecture_24.html#k-fold-cross-validation",
    "title": "Resampling Methods",
    "section": "K-Fold Cross-Validation",
    "text": "K-Fold Cross-Validation\n\nChoose a set of tuning parameters to test.\nCreate different K subsets of the data.\nFor each \\(j\\)th turning parameter Calculate the tuning parameter error for each value\n\nFor each K subset, fit a model using the data excluding the Kth subset\nPredict the values in the Kth subset using the fitted model\nRepeat the process for each K subset\nCompute the following error:\n\\[\nCVE_j = \\frac{1}{n}\\sum^n_{i=1}e_i\n\\]\n\nIdentify the \\(j\\)th tuning parameter with the lowest \\(CVE_j\\)"
  },
  {
    "objectID": "lectures/lecture_24.html#bootstrap-methods-1",
    "href": "lectures/lecture_24.html#bootstrap-methods-1",
    "title": "Resampling Methods",
    "section": "Bootstrap Methods",
    "text": "Bootstrap Methods\nBootstrapping methods are used when we cannot theoretically compute the standard errors. Bootstrap methods are computationally intensive but will compute accurate standard errors.\nWhen all else fails, a bootstrap approach will compute accurate standard errors."
  },
  {
    "objectID": "lectures/lecture_24.html#bootstrap-algorithm",
    "href": "lectures/lecture_24.html#bootstrap-algorithm",
    "title": "Resampling Methods",
    "section": "Bootstrap Algorithm",
    "text": "Bootstrap Algorithm\n\nDraw a sample \\(X*\\) of size \\(n\\) with replacement from the original data \\(X\\).\n\n\\(n\\) is the size of the data\n\nCompute the bootstrap replicate statistic \\(T* = g(X*)\\), where \\(g(\\cdot)\\) is the function that computes the statistic of interest.\nRepeat steps 1-2 \\(B\\) times to obtain \\(B\\) bootstrap replicates \\({T*_1, T*_2, ..., T*_B}\\).\nThe computed statistics from \\(B\\) samples are the empirical bootstrap distribution of the statistic, \\(g(X)\\).\nCalculate the bootstrap standard error of the statistic, \\(se_b(g(X))\\), as the standard deviation of the bootstrap replicates.\nCalculate the bootstrap confidence interval for the statistic, \\(CI(g(X))\\), with the \\(\\alpha\\) and \\((1-\\alpha)%\\) percentiles of the bootstrap replicates, where \\(\\alpha\\) is the desired level of significance."
  },
  {
    "objectID": "lectures/lecture_24.html#examples",
    "href": "lectures/lecture_24.html#examples",
    "title": "Resampling Methods",
    "section": "Examples",
    "text": "Examples\nFitting the following model:\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(magrittr)\npenguins &lt;- penguins %&gt;% drop_na()\npenguins %$% lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ flipper_length_mm + bill_length_mm + \n#&gt;     bill_depth_mm)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)  flipper_length_mm     bill_length_mm      bill_depth_mm  \n#&gt;         -6445.476             50.762              3.293             17.836\n\n\nObtain the Bootstrap-based Standard Errors. Use \\(B=1000\\) bootstrap samples."
  },
  {
    "objectID": "lectures/lecture_26.html#learning-outcomes",
    "href": "lectures/lecture_26.html#learning-outcomes",
    "title": "Bagging, Random Forests, and Boosting",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nBagging\nRandom Forests\nBoosting"
  },
  {
    "objectID": "lectures/lecture_26.html#bagging-1",
    "href": "lectures/lecture_26.html#bagging-1",
    "title": "Bagging, Random Forests, and Boosting",
    "section": "Bagging",
    "text": "Bagging\nWhen splitting the data to train and test data sets, the construction of the tree suffers from high variance.\n\nThis is due to splitting the data in a random way. One training data set will lead to different results from another training data set.\n\n\nTo improve performance, we implement a Bootstrap Aggregation (Bagging) technique.\n\n\nBagging will produce a forest of trees to classify a new observation."
  },
  {
    "objectID": "lectures/lecture_26.html#bagging-algorithm",
    "href": "lectures/lecture_26.html#bagging-algorithm",
    "title": "Bagging, Random Forests, and Boosting",
    "section": "Bagging Algorithm",
    "text": "Bagging Algorithm\nGiven a single training data set:\n\nSample from the data with replacement.\nBuild a tree from the sampled data:\n\\[\n\\hat f^{*b}(x)\n\\]\nRepeat the process B times (B=100)\nCompute the final average for all predictions:\n\\[\n\\hat f_{bag}(x)=\\frac{1}{B}\\sum^B_{b=1}\\hat f^{*b}(x)\n\\]"
  },
  {
    "objectID": "lectures/lecture_26.html#classification",
    "href": "lectures/lecture_26.html#classification",
    "title": "Bagging, Random Forests, and Boosting",
    "section": "Classification",
    "text": "Classification\nTo classify an observation, you can record the classification of each \\(b\\) tree. Then classify an observation by majority rule."
  },
  {
    "objectID": "lectures/lecture_26.html#variable-importance",
    "href": "lectures/lecture_26.html#variable-importance",
    "title": "Bagging, Random Forests, and Boosting",
    "section": "Variable Importance",
    "text": "Variable Importance\nWith the implementation of Bagging, you lose interpretability from the original tree due to the forest.\nHowever, we can compute which variables reduced the RSS or Gini Index for all the trees. The variables with the largest reduction are considered important."
  },
  {
    "objectID": "lectures/lecture_26.html#random-forests-1",
    "href": "lectures/lecture_26.html#random-forests-1",
    "title": "Bagging, Random Forests, and Boosting",
    "section": "Random Forests",
    "text": "Random Forests\nRandom Forests is an extension of Bagging, where a forest is generated from a bootstrap-based approach. However, when making a split, a random set of predictors (m&lt;p) are chosen for the split, instead of the full set p.\n\nThis will ensure that trees are unique, uncorrelated.\n\n\nIt ensures that no one predictor will have all the power and lower the variance."
  },
  {
    "objectID": "lectures/lecture_26.html#boosting-1",
    "href": "lectures/lecture_26.html#boosting-1",
    "title": "Bagging, Random Forests, and Boosting",
    "section": "Boosting",
    "text": "Boosting\nBoosting is a mechanism where a final tree is built slowly from smaller trees using the residuals.\n\nThis ensures a tree is built from a slow process and prevents overfitting.\n\n\nThis is done to improve prediction capabilities."
  },
  {
    "objectID": "lectures/lecture_26.html#algorithm",
    "href": "lectures/lecture_26.html#algorithm",
    "title": "Bagging, Random Forests, and Boosting",
    "section": "Algorithm",
    "text": "Algorithm\n\nSet \\(\\hat f(x) = 0\\) and \\(r_i = y_i\\) for all \\(i\\) in the training set\nFor \\(b=1, 2, \\ldots, B\\) repeat:\n\nFit tree \\(\\hat f^b\\) with \\(d\\) splits (\\(d+1\\) terminal nodes) to the training data \\((X,r)\\)\nUpdate \\(\\hat f\\)\n\\[\n\\hat f(x) \\leftarrow \\hat f(x) + \\lambda\\hat f^b(x)\n\\]\nUpdate residuals\n\\[\nr_i \\leftarrow r_i - \\lambda\\hat f^{b}(x_i)\n\\]\n\nOutput boosted model:\n\\[\n\\hat f(x) = \\sum^B_{b=1} \\lambda \\hat f^b(x)\n\\]"
  },
  {
    "objectID": "lectures/lecture_26.html#bagging-regression-trees",
    "href": "lectures/lecture_26.html#bagging-regression-trees",
    "title": "Bagging, Random Forests, and Boosting",
    "section": "Bagging Regression Trees",
    "text": "Bagging Regression Trees\n\nR CodePredictions\n\n\n\nlibrary(randomForest)\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(magrittr)\n\npenguins %&lt;&gt;% drop_na()\ntrain &lt;- sample(1:nrow(penguins), nrow(penguins)/2)\nbag_penguins &lt;- penguins %$% randomForest(body_mass_g ~ bill_depth_mm + bill_length_mm + flipper_length_mm, subset = train, mtry = 3, importance = T)\n\n\n\n\nyhat_bag &lt;- predict(bag_penguins, newdata = penguins[-train , ])\ntest &lt;- penguins[-train , ]$body_mass_g\nplot(yhat_bag, test)"
  },
  {
    "objectID": "lectures/lecture_26.html#bagging-classification-trees",
    "href": "lectures/lecture_26.html#bagging-classification-trees",
    "title": "Bagging, Random Forests, and Boosting",
    "section": "Bagging Classification Trees",
    "text": "Bagging Classification Trees\n\nR CodePrediction\n\n\n\nbag_penguins &lt;- penguins %$% randomForest(species ~ body_mass_g + bill_depth_mm + bill_length_mm + flipper_length_mm, subset = train, mtry = 4, importance = T)\n\n\n\n\nyhat_bag &lt;- predict(bag_penguins, newdata = penguins[-train , ])\ntest &lt;- penguins[-train , ]$species\ntable(yhat_bag, test)\n\n#&gt;            test\n#&gt; yhat_bag    Adelie Chinstrap Gentoo\n#&gt;   Adelie        67         1      1\n#&gt;   Chinstrap      5        25      0\n#&gt;   Gentoo         1         0     67"
  },
  {
    "objectID": "lectures/lecture_26.html#random-forests-regression-trees",
    "href": "lectures/lecture_26.html#random-forests-regression-trees",
    "title": "Bagging, Random Forests, and Boosting",
    "section": "Random Forests Regression Trees",
    "text": "Random Forests Regression Trees\n\nR CodePrediction\n\n\n\nbag_penguins &lt;- penguins %$% randomForest(body_mass_g ~ bill_depth_mm + bill_length_mm + flipper_length_mm, subset = train, mtry = 2, importance = T)\n\n\n\n\nyhat_bag &lt;- predict(bag_penguins, newdata = penguins[-train , ])\ntest &lt;- penguins[-train , ]$body_mass_g\nplot(yhat_bag, test)"
  },
  {
    "objectID": "lectures/lecture_26.html#random-forests-classification-trees",
    "href": "lectures/lecture_26.html#random-forests-classification-trees",
    "title": "Bagging, Random Forests, and Boosting",
    "section": "Random Forests Classification Trees",
    "text": "Random Forests Classification Trees\n\nR CodePrediction\n\n\n\nbag_penguins &lt;- penguins %$% randomForest(species ~ body_mass_g + bill_depth_mm + bill_length_mm + flipper_length_mm, subset = train, mtry = 2, importance = T)\n\n\n\n\nyhat_bag &lt;- predict(bag_penguins, newdata = penguins[-train , ])\ntest &lt;- penguins[-train , ]$species\ntable(yhat_bag, test)\n\n#&gt;            test\n#&gt; yhat_bag    Adelie Chinstrap Gentoo\n#&gt;   Adelie        69         1      0\n#&gt;   Chinstrap      3        25      0\n#&gt;   Gentoo         1         0     68"
  },
  {
    "objectID": "lectures/lecture_26.html#boosting-regression-trees",
    "href": "lectures/lecture_26.html#boosting-regression-trees",
    "title": "Bagging, Random Forests, and Boosting",
    "section": "Boosting Regression Trees",
    "text": "Boosting Regression Trees\n\nR CodePrediction\n\n\n\nlibrary(gbm)\nboost_penguin &lt;- gbm(body_mass_g ~ bill_depth_mm + bill_length_mm + flipper_length_mm, data = penguins[train , ],\ndistribution = \"gaussian\", n.trees = 5000,\ninteraction.depth = 4)\n\n\n\n\nyhat_boost &lt;- predict(boost_penguin, newdata = penguins[-train , ], n.trees = 5000)\ntest &lt;- penguins[-train , ]$body_mass_g\nplot(yhat_bag, test)"
  },
  {
    "objectID": "lectures/lecture_28.html#learning-outcomes",
    "href": "lectures/lecture_28.html#learning-outcomes",
    "title": "Support Vector Machines",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nSupport Vector Machine\nR Code"
  },
  {
    "objectID": "lectures/lecture_28.html#motivating-example",
    "href": "lectures/lecture_28.html#motivating-example",
    "title": "Support Vector Machines",
    "section": "Motivating Example",
    "text": "Motivating Example"
  },
  {
    "objectID": "lectures/lecture_28.html#motivating-example-1",
    "href": "lectures/lecture_28.html#motivating-example-1",
    "title": "Support Vector Machines",
    "section": "Motivating Example",
    "text": "Motivating Example"
  },
  {
    "objectID": "lectures/lecture_28.html#support-vector-machines-1",
    "href": "lectures/lecture_28.html#support-vector-machines-1",
    "title": "Support Vector Machines",
    "section": "Support Vector Machines",
    "text": "Support Vector Machines\nA Support Vector Machine will create a nonlinear boundary instead of a line.\n\nIt incorporates a kernel function that will compute the similarities between two support vectors.\n\n\nThe kernel function can be loosely claimed how the data is modeled."
  },
  {
    "objectID": "lectures/lecture_28.html#nonlinear-boundary",
    "href": "lectures/lecture_28.html#nonlinear-boundary",
    "title": "Support Vector Machines",
    "section": "Nonlinear Boundary",
    "text": "Nonlinear Boundary"
  },
  {
    "objectID": "lectures/lecture_28.html#nonlinear-boundary-1",
    "href": "lectures/lecture_28.html#nonlinear-boundary-1",
    "title": "Support Vector Machines",
    "section": "Nonlinear Boundary",
    "text": "Nonlinear Boundary"
  },
  {
    "objectID": "lectures/lecture_28.html#support-vector-machines-kernels",
    "href": "lectures/lecture_28.html#support-vector-machines-kernels",
    "title": "Support Vector Machines",
    "section": "Support Vector Machines Kernels",
    "text": "Support Vector Machines Kernels\n\nLinear\nPolynomial\nRadial"
  },
  {
    "objectID": "lectures/lecture_28.html#r-code-1",
    "href": "lectures/lecture_28.html#r-code-1",
    "title": "Support Vector Machines",
    "section": "R Code",
    "text": "R Code\n\nDataPlotSVM\n\n\n\nset.seed(1)\nx &lt;- matrix(rnorm(200 * 2), ncol = 2)\nx[1:100, ] &lt;- x[1:100, ] + 2\nx[101:150, ] &lt;- x[101:150 , ] - 2\ny &lt;- c(rep(1, 150), rep(2, 50))\ndat &lt;- data.frame (x = x, y = as.factor(y))\n\n\n\n\nplot(x,col=y)\n\n\n\n\n\n\n\n\n\n\n\nlibrary(e1071)\ntrain &lt;- sample(200, 100)\nsvmfit &lt;- svm(y ~ ., data = dat[train, ], kernel = \"radial\",\ngamma = 1, cost = 1)\nplot(svmfit, dat[train, ])"
  },
  {
    "objectID": "lectures/lecture_3.html#learning-objectives",
    "href": "lectures/lecture_3.html#learning-objectives",
    "title": "Control Flow",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nControl Flow\nIndexing\nComparing Numbers\nif/else Statements\ntry()\nfor Statements"
  },
  {
    "objectID": "lectures/lecture_3.html#control-flow",
    "href": "lectures/lecture_3.html#control-flow",
    "title": "Control Flow",
    "section": "Control Flow",
    "text": "Control Flow\nThe order a computer will complete tasks.\nUsually incorporates statements and loops."
  },
  {
    "objectID": "lectures/lecture_3.html#indexing-1",
    "href": "lectures/lecture_3.html#indexing-1",
    "title": "Control Flow",
    "section": "Indexing",
    "text": "Indexing\nWithin an R object, you can access an element by indexing it.\nIndexing tells R which values to output."
  },
  {
    "objectID": "lectures/lecture_3.html#vectors",
    "href": "lectures/lecture_3.html#vectors",
    "title": "Control Flow",
    "section": "Vectors",
    "text": "Vectors\nA vector can be indexed by adding [] after the object’s name and specifying the number of each element.\n\nletters\nletters[13]"
  },
  {
    "objectID": "lectures/lecture_3.html#matrices",
    "href": "lectures/lecture_3.html#matrices",
    "title": "Control Flow",
    "section": "Matrices",
    "text": "Matrices\nA matrix can be indexed by adding [] after the object’s name and specifying the number of each element. Separate the values by commas for specific indexes.\n\nx &lt;- matrix(1:40, nrow = 4)"
  },
  {
    "objectID": "lectures/lecture_3.html#data-frames",
    "href": "lectures/lecture_3.html#data-frames",
    "title": "Control Flow",
    "section": "Data Frames",
    "text": "Data Frames\nData frames can be indexed using the $ operator and [].\n\nmtcars[,\"mpg\"]"
  },
  {
    "objectID": "lectures/lecture_3.html#lists",
    "href": "lectures/lecture_3.html#lists",
    "title": "Control Flow",
    "section": "Lists",
    "text": "Lists\nLists can be indexed using the [[]] for a specific element of a list.\n\ntoy_list &lt;- list(x = letters,\n                 y = mtcars,\n                 z = list(x = diag(rep(1, 5),\n                          y = matrix(1:40, nrow = 5),\n                          z = band_members)))"
  },
  {
    "objectID": "lectures/lecture_3.html#comparing-numbers-1",
    "href": "lectures/lecture_3.html#comparing-numbers-1",
    "title": "Control Flow",
    "section": "Comparing Numbers",
    "text": "Comparing Numbers\nYou can compare two numbers, or objects, that will result in a logical output."
  },
  {
    "objectID": "lectures/lecture_3.html#comparing-numbers-operators",
    "href": "lectures/lecture_3.html#comparing-numbers-operators",
    "title": "Control Flow",
    "section": "Comparing Numbers Operators",
    "text": "Comparing Numbers Operators\n\n\n\nOperator\nDescription\n\n\n\n\n&gt;\nGreater Than\n\n\n&lt;\nLess Than\n\n\n&gt;=\nGreater than or equal\n\n\n&lt;=\nLess than or equal\n\n\n==\nEquals\n\n\n!=\nNot Equals"
  },
  {
    "objectID": "lectures/lecture_3.html#comparing-vectors",
    "href": "lectures/lecture_3.html#comparing-vectors",
    "title": "Control Flow",
    "section": "Comparing Vectors",
    "text": "Comparing Vectors\nWhen you compare a number to a vector, it will result as a logical vector."
  },
  {
    "objectID": "lectures/lecture_3.html#example",
    "href": "lectures/lecture_3.html#example",
    "title": "Control Flow",
    "section": "Example",
    "text": "Example\nTry the following code and explain what is happening:\n\n4 &gt; 1:8\n\n1:8 &gt; 4"
  },
  {
    "objectID": "lectures/lecture_3.html#ifelse-statements-1",
    "href": "lectures/lecture_3.html#ifelse-statements-1",
    "title": "Control Flow",
    "section": "if/else Statements",
    "text": "if/else Statements\nif/else statements are used to conduct specific tasks depending on the conditions"
  },
  {
    "objectID": "lectures/lecture_3.html#if-statement",
    "href": "lectures/lecture_3.html#if-statement",
    "title": "Control Flow",
    "section": "if Statement",
    "text": "if Statement\nAn if statement is used to if you want R to perform a specific function if a certain condition is met. An if statement will only run a task if a logical is returned. You will need type if, followed by the condition (as a logical) in parentheses, then the task."
  },
  {
    "objectID": "lectures/lecture_3.html#example-1",
    "href": "lectures/lecture_3.html#example-1",
    "title": "Control Flow",
    "section": "Example",
    "text": "Example\n\nx &lt;- sample(-10:10,1)\nif (x &gt; 0){\n  print(\"Positive\")\n}\nprint(x)"
  },
  {
    "objectID": "lectures/lecture_3.html#else-statement",
    "href": "lectures/lecture_3.html#else-statement",
    "title": "Control Flow",
    "section": "else statement",
    "text": "else statement\nAn else statement will conduct a different task if the if statement does not conduct the tasks."
  },
  {
    "objectID": "lectures/lecture_3.html#example-2",
    "href": "lectures/lecture_3.html#example-2",
    "title": "Control Flow",
    "section": "Example",
    "text": "Example\n\nx &lt;- sample(-10:10,1)\nif (x &gt; 0 ){\n  print(\"Positive\")\n} else {\n  print(\"Non-positive\")\n} \nprint(x)"
  },
  {
    "objectID": "lectures/lecture_3.html#chain-ifelse-statement",
    "href": "lectures/lecture_3.html#chain-ifelse-statement",
    "title": "Control Flow",
    "section": "Chain if/else statement",
    "text": "Chain if/else statement\nIf you have more than two options, you can chain if/else statements by adding an if statement immediately after the word else."
  },
  {
    "objectID": "lectures/lecture_3.html#example-3",
    "href": "lectures/lecture_3.html#example-3",
    "title": "Control Flow",
    "section": "Example",
    "text": "Example\n\nx &lt;- sample(-10:10,1)\nif (x &gt; 0 ){\n  print(\"Positive\")\n} else if (x == 0) {\n  print(\"Zero\")\n} else {\n  print(\"Negative\")\n}\nprint(x)"
  },
  {
    "objectID": "lectures/lecture_3.html#try-1",
    "href": "lectures/lecture_3.html#try-1",
    "title": "Control Flow",
    "section": "try()",
    "text": "try()\nThe try() is an extremely powerful function that will prevent a code from stopping if an error occurs."
  },
  {
    "objectID": "lectures/lecture_3.html#example-4",
    "href": "lectures/lecture_3.html#example-4",
    "title": "Control Flow",
    "section": "Example",
    "text": "Example\n\nx &lt;- sample(-10:10,1)\nif (x &gt; 0 ){\n  stop(\"This is an error\")\n  print(\"Positive\")\n} else if (x == 0) {\n  print(\"Zero\")\n} else {\n  stop(\"This is an error\")\n  print(\"Negative\")\n}\nprint(x)\n\n\nx &lt;- sample(-10:10,1)\nif (x &gt; 0 ){\n  try(stop(\"This is an error\"))\n  print(\"Positive\")\n} else if (x == 0) {\n  print(\"Zero\")\n} else {\n  try(stop(\"This is an error\"))\n  print(\"Negative\")\n}\nprint(x)\n\n\nx &lt;- sample(-10:10,1)\nif (x &gt; 0 ){\n  try(stop(\"This is an error\"), silent = T)\n  print(\"Positive\")\n} else if (x == 0) {\n  print(\"Zero\")\n} else {\n  try(stop(\"This is an error\"), silent = T)\n  print(\"Negative\")\n}\nprint(x)"
  },
  {
    "objectID": "lectures/lecture_3.html#for-loops-1",
    "href": "lectures/lecture_3.html#for-loops-1",
    "title": "Control Flow",
    "section": "for Loops",
    "text": "for Loops\nfor loops are used to conduct an iterative task with slight changes to the input. The general format goes as follows:\n\nfor (index in vector){\n  Conduct task\n}\n\nYou will repeat the for loop untie all the elements in the vector have been used."
  },
  {
    "objectID": "lectures/lecture_3.html#example-5",
    "href": "lectures/lecture_3.html#example-5",
    "title": "Control Flow",
    "section": "Example",
    "text": "Example\nCompute the mean:\n\\[\n\\bar x = \\frac{1}{n}\\sum^n_{i=1}x_i\n\\]\n\nx &lt;- rnorm(100)\nmean(x)\n\n#&gt; [1] -0.003684802"
  },
  {
    "objectID": "lectures/lecture_3.html#example-6",
    "href": "lectures/lecture_3.html#example-6",
    "title": "Control Flow",
    "section": "Example",
    "text": "Example\nCompute the variance:\n\\[\ns^2 = \\frac{1}{n-1}\\sum^n_{i-1}(x_i-\\bar x)^2\n\\]\n\nx &lt;- rnorm(100)\nvar(x)\n\n#&gt; [1] 0.9961101"
  },
  {
    "objectID": "lectures/lecture_4.html#learning-objectives",
    "href": "lectures/lecture_4.html#learning-objectives",
    "title": "Control Flow",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nNested for Loops\nnext Statements\nbreak Statements\nwhile Loops"
  },
  {
    "objectID": "lectures/lecture_4.html#nested-for-loops-1",
    "href": "lectures/lecture_4.html#nested-for-loops-1",
    "title": "Control Flow",
    "section": "Nested for Loops",
    "text": "Nested for Loops\nNested for loops are for loops within another for loop. You can stack these loops as much as needed. Just make sure the index is different for each loop. The general format for a loop goes as follow:\n\nfor (i in vector_1){\n  for (ii in vector_2){\n    perform task\n  }\n}"
  },
  {
    "objectID": "lectures/lecture_4.html#example",
    "href": "lectures/lecture_4.html#example",
    "title": "Control Flow",
    "section": "Example",
    "text": "Example\nWithout using the sd() function, compute the standard deviation for each column of the matrix:\n\nx &lt;- matrix(rnorm(1000), nrow = 10)\n\n\\[\ns^2 = \\frac{1}{n-1}\\sum^n_{i=1}(x_i-\\bar x)^2\n\\]"
  },
  {
    "objectID": "lectures/lecture_4.html#next-statements-1",
    "href": "lectures/lecture_4.html#next-statements-1",
    "title": "Control Flow",
    "section": "next Statements",
    "text": "next Statements\nThe next statement is used to skip an iteration of a loop. This is used along an if statement.\n\nfor (i in vector){\n  perform task\n  if (condition){\n    next\n  } else {\n    perform task\n  }\n}"
  },
  {
    "objectID": "lectures/lecture_4.html#example-1",
    "href": "lectures/lecture_4.html#example-1",
    "title": "Control Flow",
    "section": "Example",
    "text": "Example\nCompute the variance for all the possible variables in the leuk data set.\n\nlibrary(MASS)\nleuk\n\nUse is.factor"
  },
  {
    "objectID": "lectures/lecture_4.html#break-statements-1",
    "href": "lectures/lecture_4.html#break-statements-1",
    "title": "Control Flow",
    "section": "break Statements",
    "text": "break Statements\nThe break statement is used to stop a loop if the condition is met. This is used along with an if statement.\n\nfor (i in vector){\n  perform task\n  if (condition){\n    break\n  } else {\n    perform task\n  }\n}"
  },
  {
    "objectID": "lectures/lecture_4.html#example-2",
    "href": "lectures/lecture_4.html#example-2",
    "title": "Control Flow",
    "section": "Example",
    "text": "Example\nSimulate from a \\(N(1,1)\\) distribution until you have 30 positive numbers or you simulate one negative number."
  },
  {
    "objectID": "lectures/lecture_4.html#while-loops-1",
    "href": "lectures/lecture_4.html#while-loops-1",
    "title": "Control Flow",
    "section": "while Loops",
    "text": "while Loops\nA while loop is a combination of a for loop and a break statement. The loop will continue indefinitely until a condition becomes false.\n\nwhile (condition){\n  perform task\n  condition &lt;- update condition\n}"
  },
  {
    "objectID": "lectures/lecture_4.html#example-3",
    "href": "lectures/lecture_4.html#example-3",
    "title": "Control Flow",
    "section": "Example",
    "text": "Example\nSimulate from a \\(N(0,1)\\) distribution until you have 50 positive numbers."
  },
  {
    "objectID": "lectures/lecture_6.html#learning-objectives",
    "href": "lectures/lecture_6.html#learning-objectives",
    "title": "Functions",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nBuilt-in Functions\nUser-built functions\nExtensions"
  },
  {
    "objectID": "lectures/lecture_6.html#built-in-functions-1",
    "href": "lectures/lecture_6.html#built-in-functions-1",
    "title": "Functions",
    "section": "Built-in Functions",
    "text": "Built-in Functions\nThere are several available functions in R to conduct specific statistical methods or tasks"
  },
  {
    "objectID": "lectures/lecture_6.html#help-documentation",
    "href": "lectures/lecture_6.html#help-documentation",
    "title": "Functions",
    "section": "Help Documentation",
    "text": "Help Documentation\n\n\n\n\n\n\n\nSection\nDescription\n\n\n\n\nDescription\nProvides a brief introduction of the function\n\n\nUsage\nProvides potential usage of the function\n\n\nArguments\nArguments that the function can take\n\n\nDetails\nAn in depth description of the function\n\n\nValue\nProvides information of the output produced by the function\n\n\nNotes\nAny need to know information about the function\n\n\nAuthors\nDevelopers of the function\n\n\nReferences\nReferences to the model and function\n\n\nSee Also\nProvide information of supporting functions\n\n\nExamples\nExamples of the function"
  },
  {
    "objectID": "lectures/lecture_6.html#generic-functions",
    "href": "lectures/lecture_6.html#generic-functions",
    "title": "Functions",
    "section": "Generic Functions",
    "text": "Generic Functions\nSeveral R objects have a known class attached to it. A specialized object designed to be read by generic functions, such as summary() and plot().\nFor example, the summary() is a generic for several types of functions: summary.aov(), summary.lm(), summary.glm(), and many more."
  },
  {
    "objectID": "lectures/lecture_6.html#commonly-used-function",
    "href": "lectures/lecture_6.html#commonly-used-function",
    "title": "Functions",
    "section": "Commonly-used Function",
    "text": "Commonly-used Function\n\n\n\nFunctions\nDescription\n\n\n\n\naov()\nFits an ANOVA Model\n\n\nlm()\nFits a linear model\n\n\nglm()\nFits a general linear model\n\n\nt.test()\nConducts a t-test"
  },
  {
    "objectID": "lectures/lecture_6.html#user-built-functions-1",
    "href": "lectures/lecture_6.html#user-built-functions-1",
    "title": "Functions",
    "section": "User-built functions",
    "text": "User-built functions\n\nFunctions created by the user for analysis\nNeeds to be ran once to the R environment\nWill be lost when R session is closed"
  },
  {
    "objectID": "lectures/lecture_6.html#anatomy",
    "href": "lectures/lecture_6.html#anatomy",
    "title": "Functions",
    "section": "Anatomy",
    "text": "Anatomy\n\nname_of_function &lt;- function(data_1, data_2 = NULL, \n                             argument_1, argument_2 = TRUE, argument_3 = NULL,\n                             ...){\n  # Conduct Task\n  # Conduct Task\n  output_object &lt;- Tasks\n  return(output_object)\n}\n\n\n\nfunction: used to construct the function\ndata1: first data argument that needs to supplied\ndata2: second data argument that does not need to be supplied\nargument1: first argument must be supplied to alter function\nargument2: second argument to alter function, set to TRUE\nargument3: third argument that does not need to be supplied\n…: additional arguments supplied to other functions"
  },
  {
    "objectID": "lectures/lecture_6.html#example",
    "href": "lectures/lecture_6.html#example",
    "title": "Functions",
    "section": "Example",
    "text": "Example\nCreate a function for\n\\[\ny = \\ln(x^2)\n\\]"
  },
  {
    "objectID": "lectures/lecture_6.html#example-1",
    "href": "lectures/lecture_6.html#example-1",
    "title": "Functions",
    "section": "Example",
    "text": "Example\nCreate a function for\n\\[\nf(x) = \\left\\{\\begin{array}{cc}\nx^3 & x&lt;0\\\\\nx^2 + 5 & \\mathrm{otherwise}\n\\end{array} \\right.\n\\]"
  },
  {
    "objectID": "lectures/lecture_6.html#example-2",
    "href": "lectures/lecture_6.html#example-2",
    "title": "Functions",
    "section": "Example",
    "text": "Example\nCreate a function for\n\\[\nf(x,y) = \\left\\{\\begin{array}{cc}\nx^3 e^y &  x&lt;0\\ \\\\\nx^2 + 5 + \\ln(y) & \\mathrm{otherwise}\n\\end{array} \\right.\n\\]"
  },
  {
    "objectID": "lectures/lecture_6.html#example-3",
    "href": "lectures/lecture_6.html#example-3",
    "title": "Functions",
    "section": "Example",
    "text": "Example\nCreate the function that allows your to compute the z-score of a specific value x using the sampling distribution from a set of data (y vector):\n\\[\nz =  \\frac{x-\\bar y}{\\sqrt{s^2_{y}/n_y}}\n\\]"
  },
  {
    "objectID": "lectures/lecture_6.html#r-packages",
    "href": "lectures/lecture_6.html#r-packages",
    "title": "Functions",
    "section": "R Packages",
    "text": "R Packages\nR Packages are used to utilize functions created from the community.\nInstallation\n\ninstall.packages(\"tidyverse\")\n\nLoading\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "lectures/lecture_6.html#reticulate",
    "href": "lectures/lecture_6.html#reticulate",
    "title": "Functions",
    "section": "Reticulate",
    "text": "Reticulate\nReticulate is an R package that allows you utilized python within R."
  },
  {
    "objectID": "lectures/lecture_6.html#rcpp",
    "href": "lectures/lecture_6.html#rcpp",
    "title": "Functions",
    "section": "Rcpp",
    "text": "Rcpp\nRcpp is an R package that allows you to call C++ programs in R.\nRcpp code:\n\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n// [[Rcpp::export]]\ndouble var_cpp(NumericVector x){\nint n = x.length();\nNumericVector pre(n);\ndouble mean_x = mean(x);\nfor (int i=0; i&lt;n; ++i){\n   pre[i] = pow(x[i]-mean_x, 2);\n}\nint divisor = n - 1;\ndouble post = sum(pre) / divisor;\nreturn post;\n}\n\nR code:\n\nvar_r &lt;- function(x){\n  sum((x-mean(x))^2) / (length(x)-1)\n}\n\nBenchmark Analysis\n\nx &lt;- rnorm(50)\nbench::mark(\nvar_cpp(x),\nvar_r(x),\nvar(x)\n)\n\n#&gt; # A tibble: 3 × 6\n#&gt;   expression      min   median `itr/sec` mem_alloc `gc/sec`\n#&gt;   &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 var_cpp(x)   1.36µs    3.3µs   266243.    2.93KB      0  \n#&gt; 2 var_r(x)     2.91µs   5.36µs   137830.   23.75KB     27.6\n#&gt; 3 var(x)       5.36µs  11.41µs    76099.   13.73KB     30.5\n\n\n\nvar\n\n#&gt; function (x, y = NULL, na.rm = FALSE, use) \n#&gt; {\n#&gt;     if (missing(use)) \n#&gt;         use &lt;- if (na.rm) \n#&gt;             \"na.or.complete\"\n#&gt;         else \"everything\"\n#&gt;     na.method &lt;- pmatch(use, c(\"all.obs\", \"complete.obs\", \"pairwise.complete.obs\", \n#&gt;         \"everything\", \"na.or.complete\"))\n#&gt;     if (is.na(na.method)) \n#&gt;         stop(\"invalid 'use' argument\")\n#&gt;     if (is.data.frame(x)) \n#&gt;         x &lt;- as.matrix(x)\n#&gt;     else stopifnot(is.atomic(x))\n#&gt;     if (is.data.frame(y)) \n#&gt;         y &lt;- as.matrix(y)\n#&gt;     else stopifnot(is.atomic(y))\n#&gt;     .Call(C_cov, x, y, na.method, FALSE)\n#&gt; }\n#&gt; &lt;bytecode: 0x643bdc514298&gt;\n#&gt; &lt;environment: namespace:stats&gt;\n\n\nThis is an extremely advanced topic. Only do this if you need real speed and efficiency."
  },
  {
    "objectID": "lectures/lecture_8.html#learning-objectives",
    "href": "lectures/lecture_8.html#learning-objectives",
    "title": "Intro to Data Manipulation",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nAnonymous Functions\nPipes\nData Manipulation"
  },
  {
    "objectID": "lectures/lecture_8.html#anonymous-functions-1",
    "href": "lectures/lecture_8.html#anonymous-functions-1",
    "title": "Intro to Data Manipulation",
    "section": "Anonymous Functions",
    "text": "Anonymous Functions\nAn anonymous function is a function that is not stored in an R object for the global environment. It can be thought of as a temporary function to complete a task. A common way to used an anonymous function is with an *apply() function\n\n\nx &lt;- 1:10\nsapply(x, function(x) rnorm(1,x))\n\n#&gt;  [1] 1.543385 1.930780 2.727452 3.800343 5.547326 5.192631 7.241937 8.259342\n#&gt;  [9] 9.958393 9.545454\n\n\n\nx &lt;- 1:10\nsapply(x, \\(x) rnorm(1,1,x))\n\n#&gt;  [1]   1.74605181   0.03303168   2.29893287  -2.40247066   1.78035919\n#&gt;  [6]  -0.04529955   4.08815310  -1.07672310 -10.93652787  -7.59738247"
  },
  {
    "objectID": "lectures/lecture_8.html#example",
    "href": "lectures/lecture_8.html#example",
    "title": "Intro to Data Manipulation",
    "section": "Example",
    "text": "Example\nUse an anonymous function to square all the values in the following vector:\n\n# Use an anonymous function to calculate the square of each element in a vector\nnumbers &lt;- 1:40"
  },
  {
    "objectID": "lectures/lecture_8.html#example-1",
    "href": "lectures/lecture_8.html#example-1",
    "title": "Intro to Data Manipulation",
    "section": "Example",
    "text": "Example\nUse an anonymous function to convert the vector from Fahrenheit to Celsius:\n\n# Create a vector of temperatures in Fahrenheit\ntemperatures_f &lt;- c(32, 68, 104, 50)\n\n\\[\nC = \\frac{5(F-32)}{9}\n\\]"
  },
  {
    "objectID": "lectures/lecture_8.html#pipes-1",
    "href": "lectures/lecture_8.html#pipes-1",
    "title": "Intro to Data Manipulation",
    "section": "Pipes",
    "text": "Pipes\nPipes are used to pass the output from one function and use it as input for another function. The output is piped into the first argument of the next function. There are two main pipes: R’s base pipe and Magrittr’s pipes. You must download and install the magrittr package; and you will need to load it everytime:\n\nlibrary(magrittr)\n\nAdditionally, pipes can be used to chain functions together."
  },
  {
    "objectID": "lectures/lecture_8.html#section",
    "href": "lectures/lecture_8.html#section",
    "title": "Intro to Data Manipulation",
    "section": "|>",
    "text": "|&gt;\nBefore R 4.1, R did not have a pipe in its main program. The base pipe, |&gt;, will pipe the output of the first operation and use it as the input of the first argument of the next function.\n\nx &lt;- 1:40\nx |&gt; mean()\n\n#&gt; [1] 20.5"
  },
  {
    "objectID": "lectures/lecture_8.html#section-1",
    "href": "lectures/lecture_8.html#section-1",
    "title": "Intro to Data Manipulation",
    "section": "%>%",
    "text": "%&gt;%\nThe magrittr pipe, %&gt;%, operates the same way as |&gt;. Below are a couple of examples\n\nx &lt;- 1:10\nx %&gt;%  mean()\n\n#&gt; [1] 5.5\n\nx %&gt;% sd\n\n#&gt; [1] 3.02765\n\nx %&gt;% rnorm(1, .)\n\n#&gt; [1] -1.197894"
  },
  {
    "objectID": "lectures/lecture_8.html#section-2",
    "href": "lectures/lecture_8.html#section-2",
    "title": "Intro to Data Manipulation",
    "section": "%$%",
    "text": "%$%\nThe exposition pipe, %$%, will expose the named elements, from a list or data frame, to the next function.\n\nmtcars %$% plot(mpg, hp)"
  },
  {
    "objectID": "lectures/lecture_8.html#t",
    "href": "lectures/lecture_8.html#t",
    "title": "Intro to Data Manipulation",
    "section": "%T>%",
    "text": "%T&gt;%\nThe Tee pipe, %T&gt;%, forward the output in the\n\nsin_40 &lt;- 1:40 %&gt;% mean %T&gt;% print %&gt;% sin\n\n#&gt; [1] 20.5\n\nprint(sin_40)\n\n#&gt; [1] 0.9968298"
  },
  {
    "objectID": "lectures/lecture_8.html#t-1",
    "href": "lectures/lecture_8.html#t-1",
    "title": "Intro to Data Manipulation",
    "section": "%T>%",
    "text": "%T&gt;%\n\nrnorm(100) %&gt;% \n  matrix(ncol=2) %&gt;% \n  sin() %T&gt;% \n  plot() %&gt;% \n  colSums()"
  },
  {
    "objectID": "lectures/lecture_8.html#examples",
    "href": "lectures/lecture_8.html#examples",
    "title": "Intro to Data Manipulation",
    "section": "Examples",
    "text": "Examples\nUsing the vector below, find the standard deviation using a pipe:\n\nx &lt;- rgamma(100, 1)\nsd(x)"
  },
  {
    "objectID": "lectures/lecture_8.html#examples-1",
    "href": "lectures/lecture_8.html#examples-1",
    "title": "Intro to Data Manipulation",
    "section": "Examples",
    "text": "Examples\nChain pipe the previous results into the \\(sin(x)\\)."
  },
  {
    "objectID": "lectures/lecture_8.html#examples-2",
    "href": "lectures/lecture_8.html#examples-2",
    "title": "Intro to Data Manipulation",
    "section": "Examples",
    "text": "Examples\nChain pipe the previous results into \\(e^x\\)."
  },
  {
    "objectID": "lectures/lecture_8.html#examples-3",
    "href": "lectures/lecture_8.html#examples-3",
    "title": "Intro to Data Manipulation",
    "section": "Examples",
    "text": "Examples\nChain pipe the previous results into \\(x^2+5x+4\\)"
  },
  {
    "objectID": "lectures/lecture_8.html#data-manipulation-1",
    "href": "lectures/lecture_8.html#data-manipulation-1",
    "title": "Intro to Data Manipulation",
    "section": "Data Manipulation",
    "text": "Data Manipulation"
  },
  {
    "objectID": "lectures/lecture_8.html#tidyverse",
    "href": "lectures/lecture_8.html#tidyverse",
    "title": "Intro to Data Manipulation",
    "section": "Tidyverse",
    "text": "Tidyverse\nTidyverse is a collection of R packages used for data manipulation. The dplyr package is known as the grammar of data manipulation with a set"
  },
  {
    "objectID": "lectures/lecture_8.html#verbs",
    "href": "lectures/lecture_8.html#verbs",
    "title": "Intro to Data Manipulation",
    "section": "Verbs",
    "text": "Verbs\n\nmutate() adds new variables\nselect() selects variables\nfilter() filters data\nif_else() conditional function that returns 2 values\ngroup_by() a dataset is grouped by factors\nsummarise() provides summaries of data"
  },
  {
    "objectID": "lectures/lecture_8.html#example-2",
    "href": "lectures/lecture_8.html#example-2",
    "title": "Intro to Data Manipulation",
    "section": "Example",
    "text": "Example\n\nlibrary(palmerpenguins)\nsum_stats &lt;- penguins %&gt;% \n  drop_na %&gt;% \n  filter(year==2007) %&gt;% \n  group_by(island) %&gt;% \n  summarise(mean = mean(bill_length_mm),\n            sd = sd(bill_length_mm),\n            median = median(bill_length_mm),\n            n = length(bill_length_mm)) %&gt;% \n  print\n\n#&gt; # A tibble: 3 × 5\n#&gt;   island     mean    sd median     n\n#&gt;   &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 Biscoe     45.1  4.80   46.1    43\n#&gt; 2 Dream      44.7  5.64   45.4    45\n#&gt; 3 Torgersen  39.0  2.92   39.1    15"
  },
  {
    "objectID": "posts/week_1.html",
    "href": "posts/week_1.html",
    "title": "Week 1",
    "section": "",
    "text": "On Strike Jan 22 to Jan 26"
  },
  {
    "objectID": "posts/week_1.html#learning-outcomes",
    "href": "posts/week_1.html#learning-outcomes",
    "title": "Week 1",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nFirst Lecture\n\nInstalling R and RStudio\nScripts\nR Calculator\nTypes of Data\n\n\n\nSecond Lecture\n\nR Objects\nR Functions\nR Packages"
  },
  {
    "objectID": "posts/week_1.html#important-concepts",
    "href": "posts/week_1.html#important-concepts",
    "title": "Week 1",
    "section": "Important Concepts",
    "text": "Important Concepts\n\nFirst Lecture\n\nAccessing R & RStudio\nIf you are on a tablet or Chromebook, you can access R & RStudio via https://posit.cloud/ for free. However, they have limited computing resources. Be mindful of your experimentation. You may also be able to use Quarto in Rstudio cloud.\nYou can install R via their website: https://www.r-project.org/.\nYou can install RStudio for free from their website: https://posit.co/download/rstudio-desktop/\n\n\nUsing R\nR can be used as a calculator; below are a few examples:\n\n1+2\n\n[1] 3\n\n3/4\n\n[1] 0.75\n\n9*8\n\n[1] 72\n\nexp(4)\n\n[1] 54.59815\n\n\n\n\nTypes of Data\n\nNumeric\nThese types of data are stored as a number in R. They may be whole numbers or contains decimal values known as double.\n\n\nCharacter/String\nThis type of data is stored a string of character values. They are usually surrounded by quotes in the output.\n\n\nLogical\nThis type of data indicates TRUE or FALSE data. It is binary data.\n\n\nMissing\nThis indicates that a value is missing or not computed. Commonly stored as NA or NaN.\n\n\n\n\nSecond Lecture\n\nR Functions\nR has specialized functions that can compute specific values. R functions require inputs, known as arguments, to produce a specific output.\nFor example, the log() function can be used to compute the natural logarithm of a specified input:\n\nlog(34)\n\n[1] 3.526361\n\n\nIf you want to know information about a specific function, you can use the ? operator:\n\n?log\n\nwhich will open the help tab. Notice there are 2 arguments: x and base. This means that the log() function can be extended to other base. To use common log1, specify the arguments:\n\nlog(x=34, base=10)\n\n[1] 1.531479\n\n\nNotice that I specified the arguments. You can also type this:\n\nlog(34, 10)\n\n[1] 1.531479\n\n\nwhich produces the same results. This is because R uses positions in the function to determine argument values; therefore, if the positions are correct, you do not need to specify the argument name.\nGoing back to the First Lecture example, log(34), we did not specify the base. This is because functions have default values for arguments. The help documentation tells us what arguments have defaults and do not need to be specified.\n\n\nR Objects\n\n\nInstall packages\nYou can extend the functionality of R. The tidyverse package includes a popular set of R packages for data wrangling and analysis. To install tidyverse, use the install.packages() function2:\n\ninstall.packages('tidyverse')\n\nOnce you installed the R package, you will need to load with every R session using the library() function:\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/week_1.html#resources",
    "href": "posts/week_1.html#resources",
    "title": "Week 1",
    "section": "Resources",
    "text": "Resources\n\n\n\nLecture\nSlides\nVideos\nFiles\n\n\n\n\n1\nSlides\nNA\nNA\n\n\n2\nSlides\nNA\nExamples General Script"
  }
]